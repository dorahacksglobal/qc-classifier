{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Please Note**: As of now, this Jupyter notebook is under active development. Starting from June 5th, 2024, I intend to initiate a series of refinements and expansions. These updates will include additional changes and improvements to enhance the functionality and usability of the notebook. Your patience and understanding during this development phase are greatly appreciated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import fft, ifft\n",
        "from scipy.special import erfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "the pre-processing part was based on a previous commit of the public repository of sid-chava [QRNGClassifier Repository](https://github.com/sid-chava/QRNGClassifier)\n",
        "\n",
        "\n",
        "**QRNG Classifier Preprocessing Functions Improvements**: Starting from June 5th, 2024, I plan to enhance the preprocessing functions of the QRNG Classifier. This could involve:\n",
        "\n",
        "1. **Refining Feature Extraction**: Improve the methods used to extract features from the raw data. This could involve using more sophisticated techniques or algorithms to better capture the characteristics of the data.\n",
        "\n",
        "\n",
        "2. **Introducing New Data Transformation Techniques**: Implement new techniques for transforming the data into a format that's more suitable for the classifier. This could involve normalization, scaling, or other transformation methods.\n",
        "\n",
        "By implementing these improvements, we aim to enhance the effectiveness of the preprocessing functions, which could lead to better performance of the QRNG Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxSPTrNXsp6",
        "outputId": "0e8f768e-776e-480f-f290-0725176cabe7"
      },
      "outputs": [],
      "source": [
        "# File path in Google Drive\n",
        "file_path = 'AI_2qubits_training_data.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "num_concats = 4\n",
        "\n",
        "new_df = pd.DataFrame({'Concatenated_Data': [''] * (len(df) // num_concats), 'label': [''] * (len(df) // num_concats)})\n",
        "\n",
        "# Loop through each group of 10 rows and concatenate their 'Data' strings\n",
        "for i in range(0, len(df), num_concats):\n",
        "    new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i+num_concats])\n",
        "    new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
        "\n",
        "# Calculate Shannon entropy for each concatenated binary sequence\n",
        "def calculate_2bit_shannon_entropy(binary_string):\n",
        "    # Ensure the string length is a multiple of 2 for exact 2-bit grouping\n",
        "    if len(binary_string) % 4 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
        "    \n",
        "    # Define possible 2-bit combinations\n",
        "    #patterns = ['0000', '1000', '1100', '1110', '1111', '0100', '0110', '0111', '0010', '0011', '0001', '1001', '1101', '0110', '0101', '1010']\n",
        "    patterns = ['00', '10', '11', '01']\n",
        "    frequency = {pattern: 0 for pattern in patterns}\n",
        "    \n",
        "    # Count frequency of each pattern\n",
        "    for i in range(0, len(binary_string), 2):\n",
        "        segment = binary_string[i:i+2]\n",
        "        if segment in patterns:\n",
        "            frequency[segment] += 1\n",
        "    \n",
        "    # Calculate total segments counted\n",
        "    total_segments = sum(frequency.values())\n",
        "    \n",
        "    # Calculate probabilities and entropy\n",
        "    entropy = 0\n",
        "    for count in frequency.values():\n",
        "        if count > 0:\n",
        "            probability = count / total_segments\n",
        "            entropy -= probability * log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "def classic_spectral_test(bit_string):\n",
        "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
        "    dft = fft(bit_array)\n",
        "    n_half = len(bit_string) // 2 + 1\n",
        "    mod_dft = np.abs(dft[:n_half])\n",
        "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
        "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
        "    expected_peaks = 0.95 * n_half\n",
        "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
        "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
        "    return p_value\n",
        "\n",
        "# Apply the entropy calculationnew_df['shannon_entropy'] = new_df['Concatenated_Data'].apply(calculate_2bit_shannon_entropy)\n",
        "\n",
        "new_df['shannon_entropy'] = new_df['Concatenated_Data'].apply(calculate_2bit_shannon_entropy)\n",
        "new_df['spectral_test'] = new_df['Concatenated_Data'].apply(classic_spectral_test)\n",
        "\n",
        "df_features = pd.DataFrame(new_df['Concatenated_Data'].apply(list).tolist())\n",
        "new_df = pd.concat([new_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
        "\n",
        "#print(df)\n",
        "\n",
        "#print(df.head(10))\n",
        "\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = new_df.drop(columns='label').values\n",
        "#print(X)\n",
        "y = new_df['label'].values\n",
        "y=y.astype('int')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in this notebbok, we tried the approach of joining 4 sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "4    2000\n",
              "1     500\n",
              "2     500\n",
              "3     500\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>shannon_entropy</th>\n",
              "      <th>spectral_test</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>...</th>\n",
              "      <th>390</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.953850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.982907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.957665</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.981947</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.957641</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3495</th>\n",
              "      <td>4</td>\n",
              "      <td>1.862790</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3496</th>\n",
              "      <td>4</td>\n",
              "      <td>1.911613</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3497</th>\n",
              "      <td>4</td>\n",
              "      <td>1.940077</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3498</th>\n",
              "      <td>4</td>\n",
              "      <td>1.930763</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3499</th>\n",
              "      <td>4</td>\n",
              "      <td>1.892917</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3500 rows Ã— 403 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label  shannon_entropy  spectral_test  0  1  2  3  4  5  6  ... 390 391  \\\n",
              "0        1         1.953850            0.0  0  1  0  0  1  1  1  ...   1   1   \n",
              "1        1         1.982907            0.0  0  0  0  0  0  0  0  ...   1   1   \n",
              "2        1         1.957665            0.0  1  0  1  0  0  0  0  ...   1   1   \n",
              "3        1         1.981947            0.0  1  0  1  1  0  0  1  ...   1   1   \n",
              "4        1         1.957641            0.0  1  0  0  1  0  0  1  ...   1   0   \n",
              "...    ...              ...            ... .. .. .. .. .. .. ..  ...  ..  ..   \n",
              "3495     4         1.862790            0.0  1  1  1  0  0  1  1  ...   1   0   \n",
              "3496     4         1.911613            0.0  0  0  1  1  1  0  0  ...   0   1   \n",
              "3497     4         1.940077            0.0  1  1  0  0  0  1  0  ...   1   0   \n",
              "3498     4         1.930763            0.0  1  1  1  1  0  0  1  ...   0   0   \n",
              "3499     4         1.892917            0.0  0  1  0  1  1  0  0  ...   0   0   \n",
              "\n",
              "     392 393 394 395 396 397 398 399  \n",
              "0      0   1   1   1   1   1   0   1  \n",
              "1      0   0   0   0   1   1   1   0  \n",
              "2      1   1   0   0   1   0   1   1  \n",
              "3      1   0   0   1   0   0   0   0  \n",
              "4      1   1   0   1   0   1   0   0  \n",
              "...   ..  ..  ..  ..  ..  ..  ..  ..  \n",
              "3495   0   0   1   0   1   1   1   1  \n",
              "3496   0   1   0   1   1   1   0   0  \n",
              "3497   0   1   0   0   0   0   0   1  \n",
              "3498   1   1   0   0   1   1   0   0  \n",
              "3499   0   0   1   1   0   1   1   1  \n",
              "\n",
              "[3500 rows x 403 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 374\n",
            "Value: 2, Count: 395\n",
            "Value: 3, Count: 405\n",
            "Value: 4, Count: 1626\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_train, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDFlld25Xze5"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFkkQCBxXlWh",
        "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.5585714285714286\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_over)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_over_with_entropy = np.column_stack((X_over, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_over_with_entropy, y_over)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature ranking:\n",
            "1. feature 0 (0.08417361516844546)\n",
            "2. feature 402 (0.06276002662013498)\n",
            "3. feature 327 (0.00421214990667557)\n",
            "4. feature 251 (0.004192491751805198)\n",
            "5. feature 165 (0.0038018692064021907)\n",
            "6. feature 35 (0.0034800235293270675)\n",
            "7. feature 111 (0.003391084922748429)\n",
            "8. feature 383 (0.0033394795208258722)\n",
            "9. feature 287 (0.003258439440553982)\n",
            "10. feature 357 (0.00323974001799086)\n",
            "11. feature 393 (0.003211066668065588)\n",
            "12. feature 317 (0.003187722666689175)\n",
            "13. feature 55 (0.002971570905016892)\n",
            "14. feature 325 (0.002897786685324477)\n",
            "15. feature 265 (0.0028422458685201193)\n",
            "16. feature 61 (0.0027955545968050247)\n",
            "17. feature 71 (0.0027911505477893317)\n",
            "18. feature 257 (0.002789803790923472)\n",
            "19. feature 263 (0.0027618253788747873)\n",
            "20. feature 315 (0.002756535973231819)\n",
            "21. feature 229 (0.0027285326177937824)\n",
            "22. feature 11 (0.002728019225822083)\n",
            "23. feature 163 (0.0027243812152641946)\n",
            "24. feature 299 (0.0027187501226598556)\n",
            "25. feature 305 (0.0026883443764413794)\n",
            "26. feature 377 (0.0026658423285058027)\n",
            "27. feature 70 (0.002659276946282171)\n",
            "28. feature 349 (0.002657173734024613)\n",
            "29. feature 45 (0.0026473901542883376)\n",
            "30. feature 343 (0.002627059917702185)\n",
            "31. feature 279 (0.0026246691549529533)\n",
            "32. feature 83 (0.0026178572055318636)\n",
            "33. feature 37 (0.002611698881055238)\n",
            "34. feature 47 (0.0026099119382489767)\n",
            "35. feature 237 (0.0026069475345400395)\n",
            "36. feature 77 (0.002601679417055499)\n",
            "37. feature 29 (0.002597120611961387)\n",
            "38. feature 12 (0.0025706669988168757)\n",
            "39. feature 117 (0.0025664166982651833)\n",
            "40. feature 17 (0.0025652828439277965)\n",
            "41. feature 6 (0.0025623845580161424)\n",
            "42. feature 211 (0.0025567064903477764)\n",
            "43. feature 105 (0.0025551153361856867)\n",
            "44. feature 337 (0.0025535676882588016)\n",
            "45. feature 189 (0.002550163403794321)\n",
            "46. feature 182 (0.002546098093221346)\n",
            "47. feature 392 (0.0025456859895672907)\n",
            "48. feature 32 (0.002545209809552778)\n",
            "49. feature 374 (0.0025282313209357156)\n",
            "50. feature 87 (0.0025167869251237884)\n",
            "51. feature 153 (0.002515441661916215)\n",
            "52. feature 106 (0.0025120507617950394)\n",
            "53. feature 369 (0.002505491461825507)\n",
            "54. feature 185 (0.002474074180324216)\n",
            "55. feature 291 (0.0024686771168700794)\n",
            "56. feature 53 (0.002468604732069142)\n",
            "57. feature 179 (0.0024652031112216725)\n",
            "58. feature 351 (0.0024566642259377133)\n",
            "59. feature 91 (0.002456246325219974)\n",
            "60. feature 107 (0.002455502959367935)\n",
            "61. feature 65 (0.002452005817224886)\n",
            "62. feature 143 (0.002450162904613601)\n",
            "63. feature 307 (0.002445017701985964)\n",
            "64. feature 297 (0.0024429400330098377)\n",
            "65. feature 261 (0.0024413914055142266)\n",
            "66. feature 274 (0.002440525793460187)\n",
            "67. feature 277 (0.0024404922838067167)\n",
            "68. feature 51 (0.0024338853320713593)\n",
            "69. feature 280 (0.002431881873480232)\n",
            "70. feature 373 (0.0024242014224379726)\n",
            "71. feature 109 (0.0024204393468725692)\n",
            "72. feature 7 (0.002407932395519975)\n",
            "73. feature 44 (0.002405277441475723)\n",
            "74. feature 316 (0.0024009649920829603)\n",
            "75. feature 243 (0.002400190072588262)\n",
            "76. feature 323 (0.002399890145817821)\n",
            "77. feature 137 (0.002399442691334077)\n",
            "78. feature 157 (0.0023921893630057043)\n",
            "79. feature 81 (0.0023910071908210865)\n",
            "80. feature 293 (0.002381871435737845)\n",
            "81. feature 255 (0.00237960100616042)\n",
            "82. feature 223 (0.0023795790889704032)\n",
            "83. feature 333 (0.0023739422080797335)\n",
            "84. feature 283 (0.0023697341843970676)\n",
            "85. feature 311 (0.002365978966087788)\n",
            "86. feature 285 (0.0023655442205270673)\n",
            "87. feature 133 (0.002365207869463357)\n",
            "88. feature 3 (0.0023614665373355693)\n",
            "89. feature 31 (0.0023559360791603396)\n",
            "90. feature 345 (0.0023396146516723106)\n",
            "91. feature 140 (0.0023373290049646042)\n",
            "92. feature 379 (0.0023280433430382546)\n",
            "93. feature 290 (0.0023250292555116172)\n",
            "94. feature 295 (0.002324903511431969)\n",
            "95. feature 19 (0.002324746671704483)\n",
            "96. feature 341 (0.002320513446475061)\n",
            "97. feature 361 (0.002312992692880554)\n",
            "98. feature 63 (0.0023096546348609316)\n",
            "99. feature 309 (0.0023080593455307727)\n",
            "100. feature 247 (0.002307963680596422)\n",
            "101. feature 146 (0.002305903009355602)\n",
            "102. feature 43 (0.0023022825444420896)\n",
            "103. feature 121 (0.0023001270433718976)\n",
            "104. feature 84 (0.0022973944779349025)\n",
            "105. feature 366 (0.0022941443693380056)\n",
            "106. feature 303 (0.0022941279818566407)\n",
            "107. feature 123 (0.0022914348461131866)\n",
            "108. feature 382 (0.0022900928242038108)\n",
            "109. feature 2 (0.002289887884383745)\n",
            "110. feature 168 (0.002289396720174326)\n",
            "111. feature 399 (0.0022798608134788237)\n",
            "112. feature 346 (0.002279431716424572)\n",
            "113. feature 353 (0.002273261940385134)\n",
            "114. feature 400 (0.002270832174195535)\n",
            "115. feature 371 (0.002270641016628676)\n",
            "116. feature 203 (0.002268203171645085)\n",
            "117. feature 126 (0.0022577188192226985)\n",
            "118. feature 60 (0.00225455619717813)\n",
            "119. feature 372 (0.0022496414500881826)\n",
            "120. feature 326 (0.0022460937649419594)\n",
            "121. feature 322 (0.002242594477651338)\n",
            "122. feature 79 (0.002233484004284791)\n",
            "123. feature 260 (0.002227976479682961)\n",
            "124. feature 281 (0.002227447262590542)\n",
            "125. feature 238 (0.0022171023744596216)\n",
            "126. feature 34 (0.0022142228892536994)\n",
            "127. feature 177 (0.0022069650192577494)\n",
            "128. feature 216 (0.0022067424291911733)\n",
            "129. feature 367 (0.0022046361811038583)\n",
            "130. feature 289 (0.002203633229760808)\n",
            "131. feature 213 (0.002201120166637865)\n",
            "132. feature 215 (0.0021996085021666932)\n",
            "133. feature 313 (0.002196963674751468)\n",
            "134. feature 258 (0.0021961213706068214)\n",
            "135. feature 102 (0.002195450760963698)\n",
            "136. feature 385 (0.002191029843366036)\n",
            "137. feature 78 (0.002189870052795534)\n",
            "138. feature 86 (0.0021894828503283303)\n",
            "139. feature 110 (0.0021891590108557343)\n",
            "140. feature 390 (0.0021875398554460457)\n",
            "141. feature 396 (0.002186627470733219)\n",
            "142. feature 278 (0.0021801252667676793)\n",
            "143. feature 4 (0.002175891727443448)\n",
            "144. feature 314 (0.002171468669787656)\n",
            "145. feature 282 (0.002166522756715453)\n",
            "146. feature 122 (0.002162755476948603)\n",
            "147. feature 141 (0.0021624480374102884)\n",
            "148. feature 201 (0.0021617969075038357)\n",
            "149. feature 13 (0.002161401881667508)\n",
            "150. feature 59 (0.0021609493440322013)\n",
            "151. feature 232 (0.002159165423492323)\n",
            "152. feature 256 (0.0021579461985639014)\n",
            "153. feature 173 (0.0021574694850623618)\n",
            "154. feature 391 (0.002157344456708383)\n",
            "155. feature 147 (0.0021569671505557216)\n",
            "156. feature 395 (0.0021520260693977953)\n",
            "157. feature 159 (0.0021509166033685336)\n",
            "158. feature 127 (0.0021508869985557744)\n",
            "159. feature 276 (0.0021506932917947927)\n",
            "160. feature 104 (0.00214528774958183)\n",
            "161. feature 144 (0.0021436103693921074)\n",
            "162. feature 135 (0.0021428974559068517)\n",
            "163. feature 220 (0.0021411338841955795)\n",
            "164. feature 225 (0.0021380146835960887)\n",
            "165. feature 194 (0.002137862571829465)\n",
            "166. feature 334 (0.002135094948866053)\n",
            "167. feature 152 (0.0021284882414413465)\n",
            "168. feature 139 (0.002123598420558393)\n",
            "169. feature 200 (0.0021209113660578525)\n",
            "170. feature 272 (0.0021193988869064677)\n",
            "171. feature 33 (0.0021189199329285616)\n",
            "172. feature 72 (0.002118262690422639)\n",
            "173. feature 221 (0.002115351360647305)\n",
            "174. feature 202 (0.002112107368082739)\n",
            "175. feature 344 (0.002111179433103433)\n",
            "176. feature 112 (0.0021075683477185594)\n",
            "177. feature 329 (0.002101850620529855)\n",
            "178. feature 80 (0.0021016862943415084)\n",
            "179. feature 254 (0.0021003127810488324)\n",
            "180. feature 131 (0.0021002722177918126)\n",
            "181. feature 30 (0.002096349874551605)\n",
            "182. feature 292 (0.002094986923167817)\n",
            "183. feature 196 (0.00209490963764511)\n",
            "184. feature 253 (0.0020942640210210536)\n",
            "185. feature 54 (0.0020912695151367425)\n",
            "186. feature 188 (0.002091164556957504)\n",
            "187. feature 138 (0.002090075859938768)\n",
            "188. feature 116 (0.002088124169417437)\n",
            "189. feature 205 (0.0020880432381997563)\n",
            "190. feature 149 (0.0020874812057438)\n",
            "191. feature 241 (0.002084192626385275)\n",
            "192. feature 354 (0.0020805849656574085)\n",
            "193. feature 25 (0.002079191877316975)\n",
            "194. feature 103 (0.002078265708308403)\n",
            "195. feature 245 (0.00207754998409584)\n",
            "196. feature 310 (0.0020768049125737455)\n",
            "197. feature 136 (0.002076733342332313)\n",
            "198. feature 339 (0.002075405598036463)\n",
            "199. feature 170 (0.0020732675010206054)\n",
            "200. feature 330 (0.002073037538958001)\n",
            "201. feature 181 (0.002069867338907915)\n",
            "202. feature 9 (0.002069476201024589)\n",
            "203. feature 360 (0.0020622634611316406)\n",
            "204. feature 151 (0.0020589282541839445)\n",
            "205. feature 197 (0.002058015128077069)\n",
            "206. feature 363 (0.0020569407987843214)\n",
            "207. feature 332 (0.0020564426978164096)\n",
            "208. feature 193 (0.002055541868521242)\n",
            "209. feature 191 (0.0020541143769150427)\n",
            "210. feature 375 (0.002053456300976288)\n",
            "211. feature 39 (0.0020524185344008485)\n",
            "212. feature 101 (0.0020468874682612955)\n",
            "213. feature 340 (0.002046854890726193)\n",
            "214. feature 347 (0.002044695548152344)\n",
            "215. feature 162 (0.002043396181906697)\n",
            "216. feature 298 (0.0020419842355237925)\n",
            "217. feature 68 (0.002040008674949277)\n",
            "218. feature 388 (0.0020394303446267426)\n",
            "219. feature 175 (0.0020367443370315633)\n",
            "220. feature 85 (0.0020360798840553082)\n",
            "221. feature 209 (0.0020303145849266398)\n",
            "222. feature 239 (0.002029172928707221)\n",
            "223. feature 266 (0.0020263430300004113)\n",
            "224. feature 308 (0.002023939729566462)\n",
            "225. feature 386 (0.002023517657798257)\n",
            "226. feature 57 (0.002021491723459456)\n",
            "227. feature 304 (0.0020167809588848215)\n",
            "228. feature 100 (0.0020151611357419907)\n",
            "229. feature 124 (0.00201323511858195)\n",
            "230. feature 14 (0.002013168406985074)\n",
            "231. feature 97 (0.0020125312687707163)\n",
            "232. feature 56 (0.002008619618098403)\n",
            "233. feature 171 (0.002006989802051198)\n",
            "234. feature 286 (0.0020050453800245045)\n",
            "235. feature 267 (0.002004345665263069)\n",
            "236. feature 74 (0.0020035398591122747)\n",
            "237. feature 98 (0.0020019335024791243)\n",
            "238. feature 23 (0.001999892282551917)\n",
            "239. feature 192 (0.001998466734342847)\n",
            "240. feature 249 (0.0019975572042912585)\n",
            "241. feature 145 (0.001995377108142076)\n",
            "242. feature 49 (0.00199376003285531)\n",
            "243. feature 284 (0.0019926502068790823)\n",
            "244. feature 108 (0.001992277030094211)\n",
            "245. feature 271 (0.0019919530785498327)\n",
            "246. feature 18 (0.001990443206179836)\n",
            "247. feature 176 (0.001990269593035968)\n",
            "248. feature 28 (0.0019902224215789317)\n",
            "249. feature 222 (0.001988385379110022)\n",
            "250. feature 380 (0.0019875674650459965)\n",
            "251. feature 401 (0.0019865066875445194)\n",
            "252. feature 73 (0.0019861344758691966)\n",
            "253. feature 384 (0.001984564387087767)\n",
            "254. feature 219 (0.001982080354489771)\n",
            "255. feature 66 (0.001980816020343882)\n",
            "256. feature 335 (0.001980582327873749)\n",
            "257. feature 236 (0.0019750947316793657)\n",
            "258. feature 324 (0.001971810619780308)\n",
            "259. feature 234 (0.0019717648181178313)\n",
            "260. feature 269 (0.00196919791310988)\n",
            "261. feature 186 (0.001968709035523872)\n",
            "262. feature 306 (0.001968390591160348)\n",
            "263. feature 338 (0.001967905525031405)\n",
            "264. feature 10 (0.0019672973864568763)\n",
            "265. feature 231 (0.001964001423229125)\n",
            "266. feature 119 (0.0019630727899217822)\n",
            "267. feature 312 (0.0019606097362051683)\n",
            "268. feature 259 (0.0019605500886327753)\n",
            "269. feature 381 (0.0019586880666263426)\n",
            "270. feature 20 (0.0019572493149952333)\n",
            "271. feature 240 (0.0019571262169543104)\n",
            "272. feature 368 (0.001956753321295484)\n",
            "273. feature 364 (0.001956020700467712)\n",
            "274. feature 50 (0.001955932923514251)\n",
            "275. feature 113 (0.0019550536762563747)\n",
            "276. feature 48 (0.001954282393401072)\n",
            "277. feature 125 (0.001953323455616819)\n",
            "278. feature 252 (0.001949443154435899)\n",
            "279. feature 190 (0.001948771894879731)\n",
            "280. feature 233 (0.001947688213920277)\n",
            "281. feature 207 (0.0019424529662763507)\n",
            "282. feature 22 (0.0019344671865005989)\n",
            "283. feature 82 (0.0019324790794021146)\n",
            "284. feature 187 (0.0019309070919736851)\n",
            "285. feature 217 (0.00192660600650785)\n",
            "286. feature 342 (0.001924383625018238)\n",
            "287. feature 387 (0.001921516313491953)\n",
            "288. feature 398 (0.0019199519426812929)\n",
            "289. feature 262 (0.0019181224455847102)\n",
            "290. feature 16 (0.0019174658312650287)\n",
            "291. feature 208 (0.0019156268604057342)\n",
            "292. feature 328 (0.0019126707441000758)\n",
            "293. feature 358 (0.001912490196629862)\n",
            "294. feature 69 (0.0019114182497941639)\n",
            "295. feature 370 (0.0019079006438119362)\n",
            "296. feature 350 (0.0019070232552803289)\n",
            "297. feature 397 (0.0019067800089748747)\n",
            "298. feature 99 (0.001900498629969715)\n",
            "299. feature 8 (0.0018998856413495553)\n",
            "300. feature 250 (0.0018984013949502946)\n",
            "301. feature 296 (0.0018976207995281007)\n",
            "302. feature 318 (0.0018957069093136153)\n",
            "303. feature 58 (0.0018944657185114096)\n",
            "304. feature 52 (0.0018931235726444321)\n",
            "305. feature 273 (0.001890372790927728)\n",
            "306. feature 242 (0.001886996493713302)\n",
            "307. feature 169 (0.0018844298704595184)\n",
            "308. feature 150 (0.0018788727265536902)\n",
            "309. feature 161 (0.001877198776073941)\n",
            "310. feature 67 (0.0018691101711915974)\n",
            "311. feature 235 (0.0018658036489152608)\n",
            "312. feature 348 (0.001865368111780014)\n",
            "313. feature 378 (0.001865171661144193)\n",
            "314. feature 321 (0.0018647182817556715)\n",
            "315. feature 376 (0.0018643091595921656)\n",
            "316. feature 227 (0.0018626621194945505)\n",
            "317. feature 264 (0.001862270956816155)\n",
            "318. feature 41 (0.0018602146627663416)\n",
            "319. feature 156 (0.0018591108753014051)\n",
            "320. feature 362 (0.0018575157814940807)\n",
            "321. feature 389 (0.001856453611921779)\n",
            "322. feature 132 (0.00185553702636462)\n",
            "323. feature 46 (0.001853794403428807)\n",
            "324. feature 40 (0.001851462144778449)\n",
            "325. feature 174 (0.0018465689754863389)\n",
            "326. feature 204 (0.0018382615280168155)\n",
            "327. feature 76 (0.001838219453403792)\n",
            "328. feature 244 (0.0018381630532221128)\n",
            "329. feature 230 (0.0018379233985390867)\n",
            "330. feature 301 (0.0018349154128936725)\n",
            "331. feature 180 (0.001831543637658067)\n",
            "332. feature 95 (0.001829606470644272)\n",
            "333. feature 142 (0.0018293721345520606)\n",
            "334. feature 27 (0.0018292709419226898)\n",
            "335. feature 115 (0.0018292105025073543)\n",
            "336. feature 15 (0.0018226597964163215)\n",
            "337. feature 134 (0.0018224857329974826)\n",
            "338. feature 148 (0.001820181691764092)\n",
            "339. feature 356 (0.0018196350746553986)\n",
            "340. feature 158 (0.0018191175242475485)\n",
            "341. feature 62 (0.001817063588311826)\n",
            "342. feature 294 (0.0018168858048743387)\n",
            "343. feature 183 (0.001811042252115675)\n",
            "344. feature 160 (0.0018102734278412898)\n",
            "345. feature 128 (0.0018043325248033408)\n",
            "346. feature 38 (0.0018037048332499967)\n",
            "347. feature 199 (0.0018012068132636113)\n",
            "348. feature 288 (0.0017983744280045243)\n",
            "349. feature 275 (0.0017983587571528228)\n",
            "350. feature 319 (0.0017979104533044838)\n",
            "351. feature 228 (0.001797383786189752)\n",
            "352. feature 64 (0.0017969836197164045)\n",
            "353. feature 226 (0.0017960655235967629)\n",
            "354. feature 130 (0.0017945406984821819)\n",
            "355. feature 5 (0.001793080540423478)\n",
            "356. feature 355 (0.001791021480718516)\n",
            "357. feature 195 (0.0017902651215815967)\n",
            "358. feature 184 (0.0017866413326821172)\n",
            "359. feature 248 (0.0017796815656327535)\n",
            "360. feature 331 (0.0017735603799286543)\n",
            "361. feature 21 (0.0017723323317613518)\n",
            "362. feature 172 (0.0017636320538056847)\n",
            "363. feature 166 (0.001759323661698)\n",
            "364. feature 93 (0.0017591936052086267)\n",
            "365. feature 352 (0.0017587673999225853)\n",
            "366. feature 320 (0.001757805389309473)\n",
            "367. feature 92 (0.0017560365050119447)\n",
            "368. feature 212 (0.0017552128232244)\n",
            "369. feature 210 (0.0017503431662365262)\n",
            "370. feature 302 (0.0017498287412986997)\n",
            "371. feature 36 (0.001749702138689561)\n",
            "372. feature 89 (0.0017483783494190103)\n",
            "373. feature 206 (0.00174209528273516)\n",
            "374. feature 154 (0.0017419545082921855)\n",
            "375. feature 118 (0.0017409608940407943)\n",
            "376. feature 94 (0.0017409283458121492)\n",
            "377. feature 88 (0.0017248511454310673)\n",
            "378. feature 129 (0.0017229669903251242)\n",
            "379. feature 270 (0.0017225475561621865)\n",
            "380. feature 167 (0.0017176752727018358)\n",
            "381. feature 268 (0.0017170132395150815)\n",
            "382. feature 394 (0.0017134172015486914)\n",
            "383. feature 24 (0.0017132640081979774)\n",
            "384. feature 26 (0.001708320253940549)\n",
            "385. feature 214 (0.0017035821115117566)\n",
            "386. feature 218 (0.0016937763421664277)\n",
            "387. feature 246 (0.0016830661701725878)\n",
            "388. feature 90 (0.0016807126947231122)\n",
            "389. feature 359 (0.0016670931689894333)\n",
            "390. feature 178 (0.001665218540564537)\n",
            "391. feature 75 (0.0016635072414302853)\n",
            "392. feature 164 (0.0016632955676368156)\n",
            "393. feature 114 (0.0016613522857059021)\n",
            "394. feature 96 (0.0016592931576072305)\n",
            "395. feature 365 (0.0016536394635329494)\n",
            "396. feature 42 (0.0016471596086952396)\n",
            "397. feature 336 (0.0016350540691323597)\n",
            "398. feature 300 (0.0015655147258918582)\n",
            "399. feature 224 (0.0015439132659471728)\n",
            "400. feature 198 (0.0015408505001757607)\n",
            "401. feature 155 (0.0015402022501960714)\n",
            "402. feature 120 (0.0014676860578981443)\n",
            "403. feature 1 (0.0)\n",
            "Random Forest Accuracy with Top 10 Features: 0.7528571428571429\n",
            "Cross-Validation Scores: [0.74285714 0.77857143 0.73571429 0.76428571 0.77857143]\n",
            "Mean Cross-Validation Score: 0.76\n"
          ]
        }
      ],
      "source": [
        "# most important features\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "for f in range(X_over_with_entropy.shape[1]):\n",
        "    print(f\"{f + 1}. feature {indices[f]} ({importances[indices[f]]})\")\n",
        "\n",
        "# Select the top 10 features\n",
        "\n",
        "top_10_features = indices[:10]\n",
        "\n",
        "# Train the model with the top 10 features\n",
        "class_weights = {1: 0.5, 2: 0.2, 3: 0.2, 4: 0.1}\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_over_with_entropy[:, top_10_features], y_over)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "# cross validation score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation on the Random Forest model with the top 10 features\n",
        "\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_test_with_entropy[:, top_10_features], y_test, cv=5)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# Calculate the mean cross-validation score\n",
        "\n",
        "mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "print(\"Mean Cross-Validation Score:\", mean_cv_score)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 9, Accuracy: 0.7757142857142857\n",
            "Number of features: 18, Accuracy: 0.7642857142857142\n",
            "Number of features: 8, Accuracy: 0.7614285714285715\n",
            "Number of features: 10, Accuracy: 0.76\n",
            "Number of features: 16, Accuracy: 0.76\n",
            "Number of features: 20, Accuracy: 0.76\n",
            "Number of features: 43, Accuracy: 0.76\n",
            "Number of features: 14, Accuracy: 0.7585714285714286\n",
            "Number of features: 7, Accuracy: 0.7571428571428571\n",
            "Number of features: 17, Accuracy: 0.7571428571428571\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define your class weights\n",
        "# class_weights = {1: 0.5, 2: 0.2, 3: 0.2, 4: 0.1}\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "\n",
        "# Loop over the desired range of feature counts\n",
        "for num_features in range(5, 51):\n",
        "    # Select the top features\n",
        "    top_features = indices[:num_features]\n",
        "\n",
        "    # Train the model with the top features and class weights\n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train_with_entropy[:, top_features], y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_rf = rf_model.predict(X_test_with_entropy[:, top_features])\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "    # Store the number of features and the accuracy in the results list\n",
        "    results.append((num_features, accuracy_rf))\n",
        "    # print(num_features)\n",
        "# Sort the results by accuracy in descending order\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 10 results\n",
        "for i in range(10):\n",
        "    print(f\"Number of features: {results[i][0]}, Accuracy: {results[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.7514285714285714\n",
            "Cross-Validation Scores: [0.76785714 0.79642857 0.78214286 0.78571429 0.76785714 0.75714286\n",
            " 0.78571429 0.73571429 0.8        0.82857143]\n",
            "Mean Cross-Validation Score: 0.7807142857142857\n",
            "Confusion Matrix:\n",
            "[[ 30  39  20  37]\n",
            " [ 27  56   2  20]\n",
            " [  4   0  83   8]\n",
            " [  5   6   6 357]]\n"
          ]
        }
      ],
      "source": [
        "# class_weights = {1: 20, 2: 20, 3: 20, 4: 1}\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:16]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "# cross validation score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation on the Random Forest model with the top 10 features\n",
        "\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# Calculate the mean cross-validation score\n",
        "\n",
        "mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "print(\"Mean Cross-Validation Score:\", mean_cv_score)\n",
        "\n",
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 1626\n",
            "Value: 2, Count: 395\n",
            "Value: 3, Count: 405\n",
            "Value: 4, Count: 1626\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_over, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.8594327990135635\n",
            "Confusion Matrix:\n",
            "[[312   0   1   2]\n",
            " [ 37  29   3  20]\n",
            " [ 12   0  58   6]\n",
            " [ 17   6  10 298]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:13]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.83384615 0.85493827 0.88888889 0.88888889 0.87037037 0.84567901\n",
            " 0.85802469 0.84876543 0.85185185 0.86728395]\n",
            "Mean cross-validation score: 0.8608537511870844\n",
            "Training accuracy: 1.0\n",
            "Test accuracy: 0.8594327990135635\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
        "\n",
        "# Check accuracy on the training dataset\n",
        "train_accuracy = rf_model_top_10.score(X_train_with_entropy[:, top_10_features], y_train)\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "\n",
        "# Check accuracy on the test dataset\n",
        "test_accuracy = rf_model_top_10.score(X_test_with_entropy[:, top_10_features], y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuracy: 0.7977805178791615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.6831072749691739\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier(random_state=12)\n",
        "gb_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_gb = gb_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n",
        "\n",
        "# AdaBoost\n",
        "ab_model = AdaBoostClassifier(random_state=12)\n",
        "ab_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_ab = ab_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_ab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNF Accuracy: 0.8631319358816276\n"
          ]
        }
      ],
      "source": [
        "ab_model = RandomForestClassifier(random_state=12)\n",
        "ab_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_ab = ab_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
        "print(\"RNF Accuracy:\", accuracy_ab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='auto')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 1311\n",
            "Value: 2, Count: 1311\n",
            "Value: 3, Count: 1311\n",
            "Value: 4, Count: 1311\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_over, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.9618684461391802\n",
            "Confusion Matrix:\n",
            "[[257   2   1   4]\n",
            " [  3 270   0   0]\n",
            " [  2   3 261   0]\n",
            " [  8  11   6 221]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:13]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.9547619  0.9452381  0.94285714 0.95238095 0.95714286 0.93794749\n",
            " 0.94988067 0.96420048 0.95465394 0.95942721]\n",
            "Mean cross-validation score: 0.9518490737583816\n",
            "Training accuracy: 1.0\n",
            "Test accuracy: 0.9618684461391802\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
        "\n",
        "# Check accuracy on the training dataset\n",
        "train_accuracy = rf_model_top_10.score(X_train_with_entropy[:, top_10_features], y_train)\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "\n",
        "# Check accuracy on the test dataset\n",
        "test_accuracy = rf_model_top_10.score(X_test_with_entropy[:, top_10_features], y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test accuracy 95~96%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
