{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Please Note**: As of now, this Jupyter notebook is under active development. Starting from June 5th, 2024, I intend to initiate a series of refinements and expansions. These updates will include additional changes and improvements to enhance the functionality and usability of the notebook. Your patience and understanding during this development phase are greatly appreciated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import fft, ifft\n",
        "from scipy.special import erfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pre processing part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "the pre-processing part was based on a previous commit of the public repository of sid-chava [QRNGClassifier Repository](https://github.com/sid-chava/QRNGClassifier)\n",
        "\n",
        "\n",
        "**QRNG Classifier Preprocessing Functions Improvements**: Starting from June 5th, 2024, I plan to enhance the preprocessing functions of the QRNG Classifier. This could involve:\n",
        "\n",
        "1. **Refining Feature Extraction**: Improve the methods used to extract features from the raw data. This could involve using more sophisticated techniques or algorithms to better capture the characteristics of the data.\n",
        "\n",
        "\n",
        "2. **Introducing New Data Transformation Techniques**: Implement new techniques for transforming the data into a format that's more suitable for the classifier. This could involve normalization, scaling, or other transformation methods.\n",
        "\n",
        "By implementing these improvements, we aim to enhance the effectiveness of the preprocessing functions, which could lead to better performance of the QRNG Classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxSPTrNXsp6",
        "outputId": "0e8f768e-776e-480f-f290-0725176cabe7"
      },
      "outputs": [],
      "source": [
        "# File path in Google Drive\n",
        "file_path = 'AI_2qubits_training_data.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "num_concats = 1\n",
        "\n",
        "new_df = pd.DataFrame({'Concatenated_Data': [''] * (len(df) // num_concats), 'label': [''] * (len(df) // num_concats)})\n",
        "\n",
        "# Loop through each group of 10 rows and concatenate their 'Data' strings\n",
        "for i in range(0, len(df), num_concats):\n",
        "    new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i+num_concats])\n",
        "    new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
        "\n",
        "# Calculate Shannon entropy for each concatenated binary sequence\n",
        "def calculate_2bit_shannon_entropy(binary_string):\n",
        "    # Ensure the string length is a multiple of 2 for exact 2-bit grouping\n",
        "    if len(binary_string) % 4 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
        "    \n",
        "    # Define possible 2-bit combinations\n",
        "    #patterns = ['0000', '1000', '1100', '1110', '1111', '0100', '0110', '0111', '0010', '0011', '0001', '1001', '1101', '0110', '0101', '1010']\n",
        "    patterns = ['00', '10', '11', '01']\n",
        "    frequency = {pattern: 0 for pattern in patterns}\n",
        "    \n",
        "    # Count frequency of each pattern\n",
        "    for i in range(0, len(binary_string), 2):\n",
        "        segment = binary_string[i:i+2]\n",
        "        if segment in patterns:\n",
        "            frequency[segment] += 1\n",
        "    \n",
        "    # Calculate total segments counted\n",
        "    total_segments = sum(frequency.values())\n",
        "    \n",
        "    # Calculate probabilities and entropy\n",
        "    entropy = 0\n",
        "    for count in frequency.values():\n",
        "        if count > 0:\n",
        "            probability = count / total_segments\n",
        "            entropy -= probability * log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "def classic_spectral_test(bit_string):\n",
        "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
        "    dft = fft(bit_array)\n",
        "    n_half = len(bit_string) // 2 + 1\n",
        "    mod_dft = np.abs(dft[:n_half])\n",
        "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
        "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
        "    expected_peaks = 0.95 * n_half\n",
        "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
        "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
        "    return p_value\n",
        "\n",
        "# Apply the entropy calculationnew_df['shannon_entropy'] = new_df['Concatenated_Data'].apply(calculate_2bit_shannon_entropy)\n",
        "\n",
        "new_df['shannon_entropy'] = new_df['Concatenated_Data'].apply(calculate_2bit_shannon_entropy)\n",
        "new_df['spectral_test'] = new_df['Concatenated_Data'].apply(classic_spectral_test)\n",
        "\n",
        "df_features = pd.DataFrame(new_df['Concatenated_Data'].apply(list).tolist())\n",
        "new_df = pd.concat([new_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
        "\n",
        "#print(df)\n",
        "\n",
        "#print(df.head(10))\n",
        "\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = new_df.drop(columns='label').values\n",
        "#print(X)\n",
        "y = new_df['label'].values\n",
        "y=y.astype('int')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## getting to understand the data and modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label\n",
              "4    8000\n",
              "1    2000\n",
              "2    2000\n",
              "3    2000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>shannon_entropy</th>\n",
              "      <th>spectral_test</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.935451</td>\n",
              "      <td>2.158752e-105</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.963615</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.939471</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.872164</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.976281</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13995</th>\n",
              "      <td>4</td>\n",
              "      <td>1.942653</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13996</th>\n",
              "      <td>4</td>\n",
              "      <td>1.919479</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13997</th>\n",
              "      <td>4</td>\n",
              "      <td>1.862236</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13998</th>\n",
              "      <td>4</td>\n",
              "      <td>1.856367</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13999</th>\n",
              "      <td>4</td>\n",
              "      <td>1.717977</td>\n",
              "      <td>8.731597e-110</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 103 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  shannon_entropy  spectral_test  0  1  2  3  4  5  6  ... 90 91  \\\n",
              "0         1         1.935451  2.158752e-105  0  1  0  0  1  1  1  ...  1  1   \n",
              "1         1         1.963615  8.731597e-110  0  1  1  0  0  1  1  ...  0  1   \n",
              "2         1         1.939471  8.731597e-110  1  1  1  0  1  0  0  ...  0  1   \n",
              "3         1         1.872164  8.731597e-110  1  1  0  1  0  0  0  ...  1  1   \n",
              "4         1         1.976281  8.731597e-110  0  0  0  0  0  0  0  ...  0  0   \n",
              "...     ...              ...            ... .. .. .. .. .. .. ..  ... .. ..   \n",
              "13995     4         1.942653  8.731597e-110  1  1  1  1  0  1  0  ...  0  0   \n",
              "13996     4         1.919479  8.731597e-110  0  1  0  1  1  0  0  ...  0  1   \n",
              "13997     4         1.862236  8.731597e-110  1  1  0  0  1  1  1  ...  0  0   \n",
              "13998     4         1.856367  8.731597e-110  1  1  0  1  1  1  0  ...  1  1   \n",
              "13999     4         1.717977  8.731597e-110  0  0  1  1  1  1  0  ...  0  0   \n",
              "\n",
              "      92 93 94 95 96 97 98 99  \n",
              "0      1  1  1  1  0  0  1  0  \n",
              "1      1  0  0  0  1  1  0  1  \n",
              "2      1  0  0  0  0  0  1  1  \n",
              "3      0  1  1  1  1  1  0  1  \n",
              "4      1  1  1  1  0  0  1  1  \n",
              "...   .. .. .. .. .. .. .. ..  \n",
              "13995  1  1  0  0  1  1  0  0  \n",
              "13996  0  0  0  0  1  1  0  0  \n",
              "13997  0  0  1  1  1  0  0  0  \n",
              "13998  0  0  1  1  0  1  0  0  \n",
              "13999  0  0  1  1  0  1  1  1  \n",
              "\n",
              "[14000 rows x 103 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 1593\n",
            "Value: 2, Count: 1602\n",
            "Value: 3, Count: 1598\n",
            "Value: 4, Count: 6407\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_train, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDFlld25Xze5"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFkkQCBxXlWh",
        "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.5807142857142857\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_over)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_over_with_entropy = np.column_stack((X_over, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_over_with_entropy, y_over)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature ranking:\n",
            "1. feature 0 (0.08477059987352682)\n",
            "2. feature 102 (0.06434640975426185)\n",
            "3. feature 17 (0.009360488671683025)\n",
            "4. feature 63 (0.009163701720908513)\n",
            "5. feature 77 (0.009079627343959694)\n",
            "6. feature 15 (0.00900906297108786)\n",
            "7. feature 93 (0.008950080531223542)\n",
            "8. feature 45 (0.008922831238742648)\n",
            "9. feature 101 (0.008918315560353371)\n",
            "10. feature 57 (0.00891624892302456)\n",
            "11. feature 75 (0.008901839749523063)\n",
            "12. feature 71 (0.008867368045092748)\n",
            "13. feature 87 (0.008865473227228818)\n",
            "14. feature 21 (0.008858333071439956)\n",
            "15. feature 40 (0.008851016009282807)\n",
            "16. feature 29 (0.008840669183816813)\n",
            "17. feature 13 (0.008816316806955847)\n",
            "18. feature 23 (0.008815627007443063)\n",
            "19. feature 97 (0.008811754262263737)\n",
            "20. feature 6 (0.008796758562269922)\n",
            "21. feature 43 (0.008787512165949007)\n",
            "22. feature 5 (0.008741807351821028)\n",
            "23. feature 91 (0.008737613662650044)\n",
            "24. feature 83 (0.008735842074073328)\n",
            "25. feature 42 (0.008731787876909754)\n",
            "26. feature 51 (0.008709759322460665)\n",
            "27. feature 65 (0.008682040078831367)\n",
            "28. feature 74 (0.00866287428401124)\n",
            "29. feature 27 (0.008650898173195714)\n",
            "30. feature 64 (0.008648997216660563)\n",
            "31. feature 81 (0.008628844525232555)\n",
            "32. feature 35 (0.008628746452565433)\n",
            "33. feature 47 (0.008622077889133276)\n",
            "34. feature 37 (0.008617087843565666)\n",
            "35. feature 49 (0.00861261463593652)\n",
            "36. feature 82 (0.008601070719761003)\n",
            "37. feature 96 (0.008595168912687131)\n",
            "38. feature 26 (0.008588786245447524)\n",
            "39. feature 61 (0.008585485217447602)\n",
            "40. feature 73 (0.008563705845857939)\n",
            "41. feature 58 (0.008555335646658656)\n",
            "42. feature 62 (0.008538123623963789)\n",
            "43. feature 70 (0.008534090547996847)\n",
            "44. feature 66 (0.008522609720317612)\n",
            "45. feature 14 (0.008508465205148575)\n",
            "46. feature 69 (0.008504825801200894)\n",
            "47. feature 76 (0.008504214939647304)\n",
            "48. feature 85 (0.008502213385531575)\n",
            "49. feature 72 (0.00849964641116556)\n",
            "50. feature 7 (0.008499635020390939)\n",
            "51. feature 39 (0.008480377909656599)\n",
            "52. feature 60 (0.00846889136122462)\n",
            "53. feature 41 (0.00846246138310501)\n",
            "54. feature 22 (0.008460265769459994)\n",
            "55. feature 3 (0.008460164323884483)\n",
            "56. feature 20 (0.008452427066679912)\n",
            "57. feature 4 (0.008447146509662528)\n",
            "58. feature 11 (0.008445975404891573)\n",
            "59. feature 67 (0.00843671543777354)\n",
            "60. feature 9 (0.008428279286605434)\n",
            "61. feature 55 (0.008427967112267285)\n",
            "62. feature 92 (0.008393272111308388)\n",
            "63. feature 34 (0.008388160880941142)\n",
            "64. feature 32 (0.00837983926019589)\n",
            "65. feature 88 (0.008372367887461141)\n",
            "66. feature 84 (0.008371945990441814)\n",
            "67. feature 25 (0.008366067474347557)\n",
            "68. feature 90 (0.008361380089879059)\n",
            "69. feature 33 (0.008359571767956531)\n",
            "70. feature 2 (0.008348082652899862)\n",
            "71. feature 95 (0.008345640603071042)\n",
            "72. feature 100 (0.008344739226376453)\n",
            "73. feature 94 (0.008342113038836856)\n",
            "74. feature 54 (0.008337245343221816)\n",
            "75. feature 56 (0.008331741789608273)\n",
            "76. feature 50 (0.00833117289102997)\n",
            "77. feature 46 (0.008323070731498917)\n",
            "78. feature 89 (0.008317521601752927)\n",
            "79. feature 79 (0.008313518088038839)\n",
            "80. feature 31 (0.008310331426635412)\n",
            "81. feature 59 (0.008305794902042491)\n",
            "82. feature 53 (0.008305628746322232)\n",
            "83. feature 99 (0.008298096345859495)\n",
            "84. feature 36 (0.008295742642842014)\n",
            "85. feature 80 (0.008276278360890127)\n",
            "86. feature 19 (0.008270873329420936)\n",
            "87. feature 98 (0.008248101892082882)\n",
            "88. feature 68 (0.008229119428890123)\n",
            "89. feature 12 (0.008229046002927549)\n",
            "90. feature 28 (0.008222478619415241)\n",
            "91. feature 44 (0.00822225263722624)\n",
            "92. feature 10 (0.008200665791322384)\n",
            "93. feature 38 (0.008186091634026312)\n",
            "94. feature 8 (0.00816787068629612)\n",
            "95. feature 86 (0.00815869997541102)\n",
            "96. feature 78 (0.008148961244063425)\n",
            "97. feature 16 (0.0081464071157088)\n",
            "98. feature 18 (0.008109129215936458)\n",
            "99. feature 30 (0.008102745786569953)\n",
            "100. feature 48 (0.008065828747358476)\n",
            "101. feature 52 (0.008048685965500424)\n",
            "102. feature 24 (0.00798864319886668)\n",
            "103. feature 1 (0.0)\n",
            "Random Forest Accuracy with Top 10 Features: 0.5542857142857143\n",
            "Cross-Validation Scores: [0.55714286 0.5625     0.56964286 0.56071429 0.55714286]\n",
            "Mean Cross-Validation Score: 0.5614285714285714\n"
          ]
        }
      ],
      "source": [
        "# most important features\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Feature ranking:\")\n",
        "for f in range(X_over_with_entropy.shape[1]):\n",
        "    print(f\"{f + 1}. feature {indices[f]} ({importances[indices[f]]})\")\n",
        "\n",
        "# Select the top 10 features\n",
        "\n",
        "top_10_features = indices[:10]\n",
        "\n",
        "# Train the model with the top 10 features\n",
        "class_weights = {1: 0.5, 2: 0.2, 3: 0.2, 4: 0.1}\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_over_with_entropy[:, top_10_features], y_over)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "# cross validation score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation on the Random Forest model with the top 10 features\n",
        "\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_test_with_entropy[:, top_10_features], y_test, cv=5)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# Calculate the mean cross-validation score\n",
        "\n",
        "mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "print(\"Mean Cross-Validation Score:\", mean_cv_score)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 40, Accuracy: 0.6057142857142858\n",
            "Number of features: 29, Accuracy: 0.6053571428571428\n",
            "Number of features: 36, Accuracy: 0.6032142857142857\n",
            "Number of features: 46, Accuracy: 0.6032142857142857\n",
            "Number of features: 44, Accuracy: 0.6028571428571429\n",
            "Number of features: 26, Accuracy: 0.6014285714285714\n",
            "Number of features: 39, Accuracy: 0.6014285714285714\n",
            "Number of features: 28, Accuracy: 0.6003571428571428\n",
            "Number of features: 42, Accuracy: 0.6\n",
            "Number of features: 27, Accuracy: 0.5996428571428571\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define your class weights\n",
        "# class_weights = {1: 0.5, 2: 0.2, 3: 0.2, 4: 0.1}\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "\n",
        "# Loop over the desired range of feature counts\n",
        "for num_features in range(5, 51):\n",
        "    # Select the top features\n",
        "    top_features = indices[:num_features]\n",
        "\n",
        "    # Train the model with the top features and class weights\n",
        "    rf_model = RandomForestClassifier(random_state=42)\n",
        "    rf_model.fit(X_train_with_entropy[:, top_features], y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred_rf = rf_model.predict(X_test_with_entropy[:, top_features])\n",
        "\n",
        "    # Calculate the accuracy of the model\n",
        "    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "    # Store the number of features and the accuracy in the results list\n",
        "    results.append((num_features, accuracy_rf))\n",
        "    # print(num_features)\n",
        "# Sort the results by accuracy in descending order\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the top 10 results\n",
        "for i in range(10):\n",
        "    print(f\"Number of features: {results[i][0]}, Accuracy: {results[i][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.5853571428571429\n",
            "Cross-Validation Scores: [0.59464286 0.60267857 0.59732143 0.59464286 0.58928571 0.59107143\n",
            " 0.58571429 0.5875     0.6        0.59464286]\n",
            "Mean Cross-Validation Score: 0.59375\n",
            "Confusion Matrix:\n",
            "[[  43   56   47  261]\n",
            " [  33   90   34  241]\n",
            " [  26   11  113  252]\n",
            " [  54   56   90 1393]]\n"
          ]
        }
      ],
      "source": [
        "# class_weights = {1: 20, 2: 20, 3: 20, 4: 1}\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:16]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "# cross validation score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation on the Random Forest model with the top 10 features\n",
        "\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "\n",
        "# Calculate the mean cross-validation score\n",
        "\n",
        "mean_cv_score = np.mean(cv_scores)\n",
        "\n",
        "print(\"Mean Cross-Validation Score:\", mean_cv_score)\n",
        "\n",
        "# confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 6407\n",
            "Value: 2, Count: 1602\n",
            "Value: 3, Count: 1598\n",
            "Value: 4, Count: 6407\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_over, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.765220106150484\n",
            "Confusion Matrix:\n",
            "[[1246    2    5   23]\n",
            " [  92   43   18  172]\n",
            " [  60    8   66  181]\n",
            " [ 109   26   56 1096]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:13]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.73478939 0.754879   0.74395004 0.75019516 0.74004684 0.7392662\n",
            " 0.73224044 0.7392662  0.75565964 0.74863388]\n",
            "Mean cross-validation score: 0.7438926784237646\n",
            "Training accuracy: 0.99867301537741\n",
            "Test accuracy: 0.765220106150484\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
        "\n",
        "# Check accuracy on the training dataset\n",
        "train_accuracy = rf_model_top_10.score(X_train_with_entropy[:, top_10_features], y_train)\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "\n",
        "# Check accuracy on the test dataset\n",
        "test_accuracy = rf_model_top_10.score(X_test_with_entropy[:, top_10_features], y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuracy: 0.6063065875741492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost Accuracy: 0.5707149547299407\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_model = GradientBoostingClassifier(random_state=12)\n",
        "gb_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_gb = gb_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n",
        "\n",
        "# AdaBoost\n",
        "ab_model = AdaBoostClassifier(random_state=12)\n",
        "ab_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_ab = ab_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_ab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNF Accuracy: 0.7664689353730877\n"
          ]
        }
      ],
      "source": [
        "ab_model = RandomForestClassifier(random_state=12)\n",
        "ab_model.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "y_pred_ab = ab_model.predict(X_test_with_entropy[:, top_10_features])\n",
        "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
        "print(\"RNF Accuracy:\", accuracy_ab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# define oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='auto')\n",
        "\n",
        "# fit and apply the transform\n",
        "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
        "\n",
        "# Now, you can use X_over and y_over to train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value: 1, Count: 5131\n",
            "Value: 2, Count: 5131\n",
            "Value: 3, Count: 5131\n",
            "Value: 4, Count: 5131\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_train is a numpy array\n",
        "values, counts = np.unique(y_over, return_counts=True)\n",
        "for value, count in zip(values, counts):\n",
        "    print(f\"Value: {value}, Count: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy with Top 10 Features: 0.9003654080389769\n",
            "Confusion Matrix:\n",
            "[[ 972   15   19   16]\n",
            " [  18 1005    5    7]\n",
            " [   6    8  977   17]\n",
            " [ 104   83  111  742]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "from sklearn.utils import compute_class_weight\n",
        "\n",
        "\n",
        "top_10_features = indices[:13]\n",
        "\n",
        "\n",
        "rf_model_top_10 = RandomForestClassifier(random_state=42, class_weight=class_weights)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "rf_model_top_10.fit(X_train_with_entropy[:, top_10_features], y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "\n",
        "y_pred_rf_top_10 = rf_model_top_10.predict(X_test_with_entropy[:, top_10_features])\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model with the top 10 features\n",
        "\n",
        "accuracy_rf_top_10 = accuracy_score(y_test, y_pred_rf_top_10)\n",
        "\n",
        "print(\"Random Forest Accuracy with Top 10 Features:\", accuracy_rf_top_10)\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_rf_top_10)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-validation scores: [0.88124239 0.88611449 0.9043849  0.89159562 0.88976857 0.89159562\n",
            " 0.8818514  0.89220463 0.8909866  0.89335771]\n",
            "Mean cross-validation score: 0.8903101923086915\n",
            "Training accuracy: 0.9987209939704002\n",
            "Test accuracy: 0.9003654080389769\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(rf_model_top_10, X_train_with_entropy[:, top_10_features], y_train, cv=10)\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Mean cross-validation score:\", np.mean(cv_scores))\n",
        "\n",
        "# Check accuracy on the training dataset\n",
        "train_accuracy = rf_model_top_10.score(X_train_with_entropy[:, top_10_features], y_train)\n",
        "print(\"Training accuracy:\", train_accuracy)\n",
        "\n",
        "# Check accuracy on the test dataset\n",
        "test_accuracy = rf_model_top_10.score(X_test_with_entropy[:, top_10_features], y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "test accuracy 89~90% "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
