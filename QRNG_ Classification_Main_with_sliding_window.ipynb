{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# fixes firefox tab completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wgVYr5-TXm5t"
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.special import erfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MxSPTrNXsp6",
    "outputId": "0e8f768e-776e-480f-f290-0725176cabe7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatenate data\n",
    "def concatenateData(df, num_concats):\n",
    "    if num_concats == 0: # do nothing\n",
    "        return df.rename(columns={'binary_number': 'Concatenated_Data'})\n",
    "    new_df = pd.DataFrame({\n",
    "        'Concatenated_Data': [''] * (len(df) // num_concats), \n",
    "        'label': [0] * (len(df) // num_concats)\n",
    "    })\n",
    "\n",
    "    # Loop through each group of num_concats rows and concatenate their 'binary_number' strings\n",
    "    for i in range(0, len(df), num_concats):\n",
    "        new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i + num_concats])\n",
    "        new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Calculate Shannon entropy for each concatenated binary sequence\n",
    "def shannon_entropy(binary_string):\n",
    "    if len(binary_string) % 2 != 0:\n",
    "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
    "    \n",
    "    patterns = ['00', '10', '11', '01']\n",
    "    frequency = {pattern: 0 for pattern in patterns}\n",
    "    \n",
    "    for i in range(0, len(binary_string), 2):\n",
    "        segment = binary_string[i:i+2]\n",
    "        if segment in patterns:\n",
    "            frequency[segment] += 1\n",
    "    \n",
    "    total_segments = sum(frequency.values())\n",
    "    \n",
    "    entropy = 0\n",
    "    for count in frequency.values():\n",
    "        if count > 0:\n",
    "            probability = count / total_segments\n",
    "            entropy -= probability * log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def classic_spectral_test(bit_string):\n",
    "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
    "    dft = fft(bit_array)\n",
    "    n_half = len(bit_string) // 2 + 1\n",
    "    mod_dft = np.abs(dft[:n_half])\n",
    "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
    "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
    "    expected_peaks = 0.95 * n_half\n",
    "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
    "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
    "    return d\n",
    "\n",
    "def frequency_test(bit_string):\n",
    "    n = len(bit_string)\n",
    "    count_ones = bit_string.count('1')\n",
    "    count_zeros = bit_string.count('0')\n",
    "    \n",
    "    # The test statistic\n",
    "    s = (count_ones - count_zeros) / sqrt(n)\n",
    "    \n",
    "    # The p-value\n",
    "    p_value = erfc(abs(s) / sqrt(2))\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def runs_test(bit_string):\n",
    "    n = len(bit_string)\n",
    "    runs = 1  # Start with the first run\n",
    "    for i in range(1, n):\n",
    "        if bit_string[i] != bit_string[i - 1]:\n",
    "            runs += 1\n",
    "    \n",
    "    n0 = bit_string.count('0')\n",
    "    n1 = bit_string.count('1')\n",
    "    \n",
    "    # Expected number of runs\n",
    "    expected_runs = (2 * n0 * n1 / n) + 1\n",
    "    variance_runs = (2 * n0 * n1 * (2 * n0 * n1 - n)) / (n ** 2 * (n - 1))\n",
    "    \n",
    "    # The test statistic\n",
    "    z = (runs - expected_runs) / sqrt(variance_runs)\n",
    "    \n",
    "    # The p-value\n",
    "    p_value = erfc(abs(z) / sqrt(2))\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def linear_complexity(bit_string, M=500):\n",
    "    # Perform linear complexity test with block size M\n",
    "    n = len(bit_string)\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    lc = 0  # Initialize linear complexity\n",
    "    \n",
    "    # Process blocks of size M\n",
    "    for i in range(0, n, M):\n",
    "        block = bit_array[i:i+M]\n",
    "        if len(block) < M:\n",
    "            continue\n",
    "        \n",
    "        lc_block = 0\n",
    "        for j in range(M):\n",
    "            if block[j] == 1:\n",
    "                lc_block = j + 1\n",
    "        \n",
    "        lc += lc_block\n",
    "    \n",
    "    lc = lc / (n / M)\n",
    "    return lc\n",
    "\n",
    "def autocorrelation_test(bit_string, lag=1):\n",
    "    n = len(bit_string)\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    autocorrelation = np.correlate(bit_array, np.roll(bit_array, lag), mode='valid')[0]\n",
    "    return autocorrelation / n\n",
    "\n",
    "def maurer_universal_test(bit_string):\n",
    "    k = 6\n",
    "    l = 5\n",
    "    q = 20\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    max_val = 2**k\n",
    "    init_subseq = bit_array[:q]\n",
    "    rest_subseq = bit_array[q:]\n",
    "    d = {}\n",
    "    for i in range(len(init_subseq) - k + 1):\n",
    "        d[tuple(init_subseq[i:i+k])] = i\n",
    "    t = []\n",
    "    for i in range(len(rest_subseq) - k + 1):\n",
    "        subseq = tuple(rest_subseq[i:i+k])\n",
    "        if subseq in d:\n",
    "            t.append(i - d[subseq])\n",
    "            d[subseq] = i\n",
    "    if not t:\n",
    "        return 0\n",
    "    t = np.array(t)\n",
    "    log_avg = np.mean(np.log2(t))\n",
    "    return log_avg - np.log2(q)\n",
    "\n",
    "def binary_matrix_rank_test(bit_string, M=32, Q=32):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    num_matrices = len(bit_array) // (M * Q)\n",
    "    ranks = []\n",
    "    for i in range(num_matrices):\n",
    "        matrix = bit_array[i*M*Q:(i+1)*M*Q].reshape((M, Q))\n",
    "        rank = np.linalg.matrix_rank(matrix)\n",
    "        ranks.append(rank)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "def cumulative_sums_test(bit_string):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    adjusted = 2 * bit_array - 1\n",
    "    cumulative_sum = np.cumsum(adjusted)\n",
    "    max_excursion = np.max(np.abs(cumulative_sum))\n",
    "    return max_excursion\n",
    "\n",
    "def longest_run_ones_test(bit_string, block_size=100):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    num_blocks = len(bit_array) // block_size\n",
    "    max_runs = []\n",
    "    for i in range(num_blocks):\n",
    "        block = bit_array[i*block_size:(i+1)*block_size]\n",
    "        max_run = max([len(list(g)) for k, g in itertools.groupby(block) if k == 1])\n",
    "        max_runs.append(max_run)\n",
    "    return np.mean(max_runs)\n",
    "\n",
    "def random_excursions_test(bit_string):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    bit_array = 2 * bit_array - 1  # Convert to Â±1\n",
    "\n",
    "    cumulative_sum = np.cumsum(bit_array)\n",
    "    states = np.unique(cumulative_sum)\n",
    "\n",
    "    if 0 not in states:\n",
    "        states = np.append(states, 0)\n",
    "    state_counts = {state: 0 for state in states}\n",
    "    for state in cumulative_sum:\n",
    "        state_counts[state] += 1\n",
    "\n",
    "    state_counts[0] -= 1  # Adjust for zero state\n",
    "    pi = [0.5 * (1 - (1 / (2 * state + 1)**2)) for state in states]\n",
    "    x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unique_subsequences(bit_string, length=4):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    subsequences = set()\n",
    "    \n",
    "    for i in range(n - length + 1):\n",
    "        subseq = tuple(bit_array[i:i+length])\n",
    "        subsequences.add(subseq)\n",
    "    \n",
    "    return len(subsequences)\n",
    "\n",
    "def sample_entropy(bit_string, m=2, r=0.2):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    N = len(bit_array)\n",
    "    \n",
    "    def _phi(m):\n",
    "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.all(np.abs(x[:, None] - x) <= r, axis=2), axis=0) / (N - m + 1.0)\n",
    "        return np.sum(C) / (N - m + 1.0)\n",
    "    \n",
    "    return -np.log(_phi(m + 1) / _phi(m))\n",
    "\n",
    "def permutation_entropy(bit_string, order=3):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    \n",
    "    permutations = np.array(list(itertools.permutations(range(order))))\n",
    "    c = np.zeros(len(permutations))\n",
    "    \n",
    "    for i in range(n - order + 1):\n",
    "        sorted_index_array = tuple(np.argsort(bit_array[i:i+order]))\n",
    "        for j, p in enumerate(permutations):\n",
    "            if np.array_equal(p, sorted_index_array):\n",
    "                c[j] += 1\n",
    "    \n",
    "    c = c / (n - order + 1)\n",
    "    pe = -np.sum(c * np.log2(c + np.finfo(float).eps))\n",
    "    return pe\n",
    "\n",
    "def lyapunov_exponent(bit_string, m=2, t=1):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    N = len(bit_array)\n",
    "    \n",
    "    def _phi(m):\n",
    "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.all(np.abs(x[:, None] - x) <= t, axis=2), axis=0) / (N - m + 1.0)\n",
    "        return np.sum(np.log(C + np.finfo(float).eps)) / (N - m + 1.0)\n",
    "    \n",
    "    return abs(_phi(m) - _phi(m + 1))\n",
    "\n",
    "def entropy_rate(bit_string, k=2):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    prob = {}\n",
    "    \n",
    "    for i in range(n - k + 1):\n",
    "        subseq = tuple(bit_array[i:i + k])\n",
    "        if subseq in prob:\n",
    "            prob[subseq] += 1\n",
    "        else:\n",
    "            prob[subseq] = 1\n",
    "    \n",
    "    for key in prob:\n",
    "        prob[key] /= (n - k + 1)\n",
    "    \n",
    "    entropy_rate = -sum(p * log2(p) for p in prob.values())\n",
    "    return entropy_rate\n",
    "\n",
    "# Apply randomness tests\n",
    "def apply_randomness_tests(df, tests):\n",
    "    if not tests:\n",
    "        raise ValueError(\"No randomness tests specified.\")\n",
    "\n",
    "    test_functions = {\n",
    "        'autocorrelation': autocorrelation_test,\n",
    "        'cumulative_sums': cumulative_sums_test,\n",
    "        'spectral_test': classic_spectral_test,\n",
    "        'frequency_test': frequency_test,\n",
    "        'runs_test': runs_test,\n",
    "        'shannon_entropy': shannon_entropy\n",
    "    }\n",
    "\n",
    "    for test in tests:\n",
    "        if test not in test_functions:\n",
    "            raise ValueError(f\"Invalid randomness test: {test}\")\n",
    "        df[test] = df['Concatenated_Data'].apply(test_functions[test])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df, num_concats, tests):\n",
    "    df = concatenateData(df, num_concats)\n",
    "    processed_df = apply_randomness_tests(df, tests)\n",
    "    \n",
    "    # Convert concatenated binary strings into separate columns\n",
    "    df_features = pd.DataFrame(processed_df['Concatenated_Data'].apply(list).tolist()).astype(int).astype(bool)\n",
    "    processed_df = pd.concat([processed_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "# Calculate min-entropy\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "file_path = 'AI_2qubits_training_data.txt'\n",
    "\n",
    "# Read the data from the file\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            binary_number, label = line.strip().split()\n",
    "            data.append((binary_number, int(label)))\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try generating more training data - any sequence of random bits from the bit string also came from the quantum computer\n",
    "\n",
    "def binary_series_to_string(x):\n",
    "    return ''.join(x['binary_number'].to_list())\n",
    "\n",
    "# work around deprecation warning\n",
    "squashed_string_df = df.groupby('label')[['label','binary_number']].apply(binary_series_to_string)\n",
    "\n",
    "def window(word, size=1, gap=1): return [word[i:i+size] for i in range(0, len(word)-size + 1, gap)]\n",
    "\n",
    "for i in range(len(squashed_string_df)):\n",
    "    squashed_string_df.iloc[i] = window(squashed_string_df.iloc[i], 100, 2)  # sample 2 qubits at a time\n",
    "\n",
    "# TODO: assume bits are processed in a single stream - k-fold cross validation should split into k chunks ahead of time to preserve time dependency information?\n",
    "df = pd.DataFrame(squashed_string_df.explode(), columns=['Concatenated_Data']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_to_apply = ['spectral_test', 'shannon_entropy', 'frequency_test', 'runs_test', 'autocorrelation']\n",
    "\n",
    "# Preprocess data and apply randomness tests\n",
    "preprocessed_df = preprocess_data(df, num_concats=0, tests=tests_to_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[preprocessed_df.select_dtypes(np.float64).columns] = preprocessed_df.select_dtypes(np.float64).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>spectral_test</th>\n",
       "      <th>shannon_entropy</th>\n",
       "      <th>frequency_test</th>\n",
       "      <th>runs_test</th>\n",
       "      <th>autocorrelation</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-21.771553</td>\n",
       "      <td>1.935451</td>\n",
       "      <td>0.423711</td>\n",
       "      <td>0.120217</td>\n",
       "      <td>0.33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-21.771553</td>\n",
       "      <td>1.935451</td>\n",
       "      <td>0.423711</td>\n",
       "      <td>0.079056</td>\n",
       "      <td>0.33</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.945597</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.084331</td>\n",
       "      <td>0.34</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-21.771553</td>\n",
       "      <td>1.963233</td>\n",
       "      <td>0.423711</td>\n",
       "      <td>0.120217</td>\n",
       "      <td>0.33</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.976281</td>\n",
       "      <td>0.548506</td>\n",
       "      <td>0.168839</td>\n",
       "      <td>0.31</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699799</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.702839</td>\n",
       "      <td>0.027807</td>\n",
       "      <td>0.109146</td>\n",
       "      <td>0.19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699800</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.702839</td>\n",
       "      <td>0.027807</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699801</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.702839</td>\n",
       "      <td>0.027807</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.19</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699802</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.702839</td>\n",
       "      <td>0.027807</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.19</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699803</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.717977</td>\n",
       "      <td>0.071861</td>\n",
       "      <td>0.051254</td>\n",
       "      <td>0.21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699804 rows Ã 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
       "0           1     -21.771553         1.935451        0.423711   0.120217   \n",
       "1           1     -21.771553         1.935451        0.423711   0.079056   \n",
       "2           1     -22.230385         1.945597        0.317311   0.084331   \n",
       "3           1     -21.771553         1.963233        0.423711   0.120217   \n",
       "4           1     -22.230385         1.976281        0.548506   0.168839   \n",
       "...       ...            ...              ...             ...        ...   \n",
       "699799      4     -22.230385         1.702839        0.027807   0.109146   \n",
       "699800      4     -22.230385         1.702839        0.027807   0.069771   \n",
       "699801      4     -22.230385         1.702839        0.027807   0.069771   \n",
       "699802      4     -22.230385         1.702839        0.027807   0.069771   \n",
       "699803      4     -22.230385         1.717977        0.071861   0.051254   \n",
       "\n",
       "        autocorrelation      0      1      2      3  ...     90     91     92  \\\n",
       "0                  0.33  False   True  False  False  ...   True   True   True   \n",
       "1                  0.33  False  False   True   True  ...   True   True   True   \n",
       "2                  0.34   True   True   True   True  ...   True   True  False   \n",
       "3                  0.33   True   True   True   True  ...  False  False   True   \n",
       "4                  0.31   True   True   True   True  ...   True  False  False   \n",
       "...                 ...    ...    ...    ...    ...  ...    ...    ...    ...   \n",
       "699799             0.19  False  False   True   True  ...   True  False   True   \n",
       "699800             0.19   True   True  False   True  ...   True   True   True   \n",
       "699801             0.19  False   True  False  False  ...   True   True   True   \n",
       "699802             0.19  False  False  False  False  ...   True   True  False   \n",
       "699803             0.21  False  False   True   True  ...  False  False  False   \n",
       "\n",
       "           93     94     95     96     97     98     99  \n",
       "0        True   True   True  False  False   True  False  \n",
       "1        True  False  False   True  False  False   True  \n",
       "2       False   True  False  False   True   True  False  \n",
       "3       False  False   True   True  False  False   True  \n",
       "4        True   True  False  False   True   True  False  \n",
       "...       ...    ...    ...    ...    ...    ...    ...  \n",
       "699799   True   True   True   True   True  False  False  \n",
       "699800   True   True   True  False  False  False  False  \n",
       "699801   True  False  False  False  False   True   True  \n",
       "699802  False  False  False   True   True  False   True  \n",
       "699803  False   True   True  False   True   True   True  \n",
       "\n",
       "[699804 rows x 106 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test splits should not be shuffled since the order that the bits were sampled matters. However, we need to take splits from each of the quantum computers, so the splits really should be interleaved. \n",
    "\n",
    "Later we'll use k-fold CV to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
      "0           1     -21.771553         1.935451        0.423711   0.120217   \n",
      "1           1     -21.771553         1.935451        0.423711   0.079056   \n",
      "2           1     -22.230385         1.945597        0.317311   0.084331   \n",
      "3           1     -21.771553         1.963233        0.423711   0.120217   \n",
      "4           1     -22.230385         1.976281        0.548506   0.168839   \n",
      "...       ...            ...              ...             ...        ...   \n",
      "699799      4     -22.230385         1.702839        0.027807   0.109146   \n",
      "699800      4     -22.230385         1.702839        0.027807   0.069771   \n",
      "699801      4     -22.230385         1.702839        0.027807   0.069771   \n",
      "699802      4     -22.230385         1.702839        0.027807   0.069771   \n",
      "699803      4     -22.230385         1.717977        0.071861   0.051254   \n",
      "\n",
      "        autocorrelation      0      1      2      3  ...     90     91     92  \\\n",
      "0                  0.33  False   True  False  False  ...   True   True   True   \n",
      "1                  0.33  False  False   True   True  ...   True   True   True   \n",
      "2                  0.34   True   True   True   True  ...   True   True  False   \n",
      "3                  0.33   True   True   True   True  ...  False  False   True   \n",
      "4                  0.31   True   True   True   True  ...   True  False  False   \n",
      "...                 ...    ...    ...    ...    ...  ...    ...    ...    ...   \n",
      "699799             0.19  False  False   True   True  ...   True  False   True   \n",
      "699800             0.19   True   True  False   True  ...   True   True   True   \n",
      "699801             0.19  False   True  False  False  ...   True   True   True   \n",
      "699802             0.19  False  False  False  False  ...   True   True  False   \n",
      "699803             0.21  False  False   True   True  ...  False  False  False   \n",
      "\n",
      "           93     94     95     96     97     98     99  \n",
      "0        True   True   True  False  False   True  False  \n",
      "1        True  False  False   True  False  False   True  \n",
      "2       False   True  False  False   True   True  False  \n",
      "3       False  False   True   True  False  False   True  \n",
      "4        True   True  False  False   True   True  False  \n",
      "...       ...    ...    ...    ...    ...    ...    ...  \n",
      "699799   True   True   True   True   True  False  False  \n",
      "699800   True   True   True  False  False  False  False  \n",
      "699801   True  False  False  False  False   True   True  \n",
      "699802  False  False  False   True   True  False   True  \n",
      "699803  False   True   True  False   True   True   True  \n",
      "\n",
      "[699804 rows x 106 columns]\n",
      "[[-22.230384826660156 1.923143744468689 0.2301393449306488 ... True True\n",
      "  0.6124041659426278]\n",
      " [-22.230384826660156 1.8969428539276123 0.2301393449306488 ... False\n",
      "  False 0.38530401537969583]\n",
      " [-22.230384826660156 1.8613152503967285 0.0051102605648338795 ... True\n",
      "  False 0.7914166079866634]\n",
      " ...\n",
      " [-22.230384826660156 1.8699880838394165 0.016395071521401405 ... True\n",
      "  True 0.7428737306925517]\n",
      " [-22.230384826660156 1.8990670442581177 0.6891565322875977 ... False\n",
      "  False 0.5382643961584794]\n",
      " [-22.230384826660156 1.832305669784546 0.1615133136510849 ... True True\n",
      "  0.6346658251086552]]\n",
      "[[-22.230384826660156 1.8969428539276123 0.2301393449306488 ... True True\n",
      "  0.37910956398155227]\n",
      " [-22.230384826660156 1.9129350185394287 0.07186064124107361 ... True\n",
      "  True 0.686014212066725]\n",
      " [-22.230384826660156 1.80945885181427 0.42371079325675964 ... False\n",
      "  False 0.5725052722462552]\n",
      " ...\n",
      " [-22.230384826660156 1.7845127582550049 0.002699796110391617 ... True\n",
      "  True 0.8249412834540485]\n",
      " [-22.230384826660156 1.9235323667526245 0.841480553150177 ... False\n",
      "  False 0.5254840016895512]\n",
      " [-22.230384826660156 1.9516135454177856 0.31731051206588745 ... True\n",
      "  True 0.4005532641051853]]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = preprocessed_df.drop(columns='label').values\n",
    "y = preprocessed_df['label'].values.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "# min-entropy also takes into account the bitstring! is this ok?\n",
    "min_entropy_train = np.apply_along_axis(calculate_min_entropy, 1, X_train)\n",
    "min_entropy_test = np.apply_along_axis(calculate_min_entropy, 1, X_test)\n",
    "\n",
    "# Add min-entropy as a feature\n",
    "X_train = np.column_stack((X_train, min_entropy_train))\n",
    "X_test = np.column_stack((X_test, min_entropy_test))\n",
    "\n",
    "print(preprocessed_df)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale all input data for fairness. Fit it on the training data only to prevent leakage. Only scale the nonbinary columns. (Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this much data, we need multithreading. Get number of cores now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 4\n",
      "Using 2 jobs\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "N_CORES = joblib.cpu_count(only_physical_cores=True)\n",
    "print(f\"Number of physical cores: {N_CORES}\")\n",
    "\n",
    "jobs = N_CORES // 2\n",
    "print(f\"Using {jobs} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model testing\n",
    "\n",
    "For now, do a single run of a parallelized model. The goal is to beat 63%, and achieve >95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Brik04cEFwK"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SGDClassifier with hinge loss to get an parallelized SVM. Input data should be scaled to avoid divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 24.87, NNZs: 106, Bias: -15.478192, T: 559843, Avg. loss: 7.349713\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.70, NNZs: 106, Bias: -18.909748, T: 559843, Avg. loss: 6.605183\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.16, NNZs: 106, Bias: -16.298848, T: 1119686, Avg. loss: 1.056295\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.51, NNZs: 106, Bias: -19.523071, T: 1119686, Avg. loss: 0.976348\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.89, NNZs: 106, Bias: -16.735968, T: 1679529, Avg. loss: 0.746303\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.01, NNZs: 106, Bias: -19.807212, T: 1679529, Avg. loss: 0.698437\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.87, NNZs: 106, Bias: -17.018356, T: 2239372, Avg. loss: 0.621026\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.44, NNZs: 106, Bias: -19.982463, T: 2239372, Avg. loss: 0.585542\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.44, NNZs: 106, Bias: -17.220800, T: 2799215, Avg. loss: 0.554310\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 8.45, NNZs: 106, Bias: -20.074835, T: 2799215, Avg. loss: 0.524489\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 10.43, NNZs: 106, Bias: -17.355331, T: 3359058, Avg. loss: 0.511392\n",
      "Total training time: 1.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 7.79, NNZs: 106, Bias: -20.131027, T: 3359058, Avg. loss: 0.485536\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 9.56, NNZs: 106, Bias: -17.458316, T: 3918901, Avg. loss: 0.481245\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 7.25, NNZs: 106, Bias: -20.161461, T: 3918901, Avg. loss: 0.458657\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 8.90, NNZs: 106, Bias: -17.523116, T: 4478744, Avg. loss: 0.458043\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 6.79, NNZs: 106, Bias: -20.162080, T: 4478744, Avg. loss: 0.438598\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 8.30, NNZs: 106, Bias: -17.567285, T: 5038587, Avg. loss: 0.439806\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 6.46, NNZs: 106, Bias: -20.170495, T: 5038587, Avg. loss: 0.423801\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 7.81, NNZs: 106, Bias: -17.591437, T: 5598430, Avg. loss: 0.425755\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 6.17, NNZs: 106, Bias: -20.161334, T: 5598430, Avg. loss: 0.411763\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 7.40, NNZs: 106, Bias: -17.600217, T: 6158273, Avg. loss: 0.413695\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 5.87, NNZs: 106, Bias: -20.148088, T: 6158273, Avg. loss: 0.401898\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 7.00, NNZs: 106, Bias: -17.600463, T: 6718116, Avg. loss: 0.403820\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 5.64, NNZs: 106, Bias: -20.131041, T: 6718116, Avg. loss: 0.394170\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 6.66, NNZs: 106, Bias: -17.591974, T: 7277959, Avg. loss: 0.394893\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 5.42, NNZs: 106, Bias: -20.106997, T: 7277959, Avg. loss: 0.387470\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 6.36, NNZs: 106, Bias: -17.577428, T: 7837802, Avg. loss: 0.387583\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 6.09, NNZs: 106, Bias: -17.554027, T: 8397645, Avg. loss: 0.381246\n",
      "Total training time: 2.79 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 5.23, NNZs: 106, Bias: -20.080408, T: 7837802, Avg. loss: 0.381332\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 5.84, NNZs: 106, Bias: -17.521809, T: 8957488, Avg. loss: 0.375598\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 5.06, NNZs: 106, Bias: -20.045829, T: 8397645, Avg. loss: 0.376463\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 5.59, NNZs: 106, Bias: -17.484951, T: 9517331, Avg. loss: 0.370756\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 4.92, NNZs: 106, Bias: -20.008935, T: 8957488, Avg. loss: 0.371450\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 5.37, NNZs: 106, Bias: -17.441038, T: 10077174, Avg. loss: 0.366335\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 4.78, NNZs: 106, Bias: -19.975291, T: 9517331, Avg. loss: 0.367144\n",
      "Total training time: 3.34 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 5.18, NNZs: 106, Bias: -17.390784, T: 10637017, Avg. loss: 0.362295\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 4.64, NNZs: 106, Bias: -19.939557, T: 10077174, Avg. loss: 0.363272\n",
      "Total training time: 3.54 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 5.00, NNZs: 106, Bias: -17.341311, T: 11196860, Avg. loss: 0.358712\n",
      "Total training time: 3.72 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 4.51, NNZs: 106, Bias: -19.902809, T: 10637017, Avg. loss: 0.359934\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 4.81, NNZs: 106, Bias: -17.291644, T: 11756703, Avg. loss: 0.355535\n",
      "Total training time: 3.90 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 4.41, NNZs: 106, Bias: -19.862467, T: 11196860, Avg. loss: 0.356690\n",
      "Total training time: 3.94 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 4.67, NNZs: 106, Bias: -17.235944, T: 12316546, Avg. loss: 0.352391\n",
      "Total training time: 4.09 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 4.28, NNZs: 106, Bias: -19.820687, T: 11756703, Avg. loss: 0.354016\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 4.51, NNZs: 106, Bias: -17.181150, T: 12876389, Avg. loss: 0.349849\n",
      "Total training time: 4.27 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 4.19, NNZs: 106, Bias: -19.779190, T: 12316546, Avg. loss: 0.351078\n",
      "Total training time: 4.32 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 4.37, NNZs: 106, Bias: -17.121088, T: 13436232, Avg. loss: 0.347383\n",
      "Total training time: 4.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 4.07, NNZs: 106, Bias: -19.735591, T: 12876389, Avg. loss: 0.348943\n",
      "Total training time: 4.51 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 4.25, NNZs: 106, Bias: -17.062033, T: 13996075, Avg. loss: 0.345097\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 3.99, NNZs: 106, Bias: -19.694588, T: 13436232, Avg. loss: 0.346083\n",
      "Total training time: 4.69 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 4.13, NNZs: 106, Bias: -17.000356, T: 14555918, Avg. loss: 0.342991\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 3.90, NNZs: 106, Bias: -19.648648, T: 13996075, Avg. loss: 0.344247\n",
      "Total training time: 4.88 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 4.03, NNZs: 106, Bias: -16.936988, T: 15115761, Avg. loss: 0.341032\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 3.81, NNZs: 106, Bias: -19.605212, T: 14555918, Avg. loss: 0.342223\n",
      "Total training time: 5.07 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 3.92, NNZs: 106, Bias: -16.873970, T: 15675604, Avg. loss: 0.339388\n",
      "Total training time: 5.15 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 3.74, NNZs: 106, Bias: -19.558695, T: 15115761, Avg. loss: 0.340182\n",
      "Total training time: 5.26 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 3.82, NNZs: 106, Bias: -16.811322, T: 16235447, Avg. loss: 0.337572\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 3.66, NNZs: 106, Bias: -19.514536, T: 15675604, Avg. loss: 0.338479\n",
      "Total training time: 5.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 3.75, NNZs: 106, Bias: -16.747751, T: 16795290, Avg. loss: 0.335907\n",
      "Total training time: 5.51 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 3.66, NNZs: 106, Bias: -16.686272, T: 17355133, Avg. loss: 0.334551\n",
      "Total training time: 5.69 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 3.60, NNZs: 106, Bias: -19.468764, T: 16235447, Avg. loss: 0.336698\n",
      "Total training time: 5.64 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 3.58, NNZs: 106, Bias: -16.620507, T: 17914976, Avg. loss: 0.333284\n",
      "Total training time: 5.86 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 3.55, NNZs: 106, Bias: -19.421555, T: 16795290, Avg. loss: 0.335277\n",
      "Total training time: 5.83 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 3.50, NNZs: 106, Bias: -16.557316, T: 18474819, Avg. loss: 0.331963\n",
      "Total training time: 6.04 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 3.47, NNZs: 106, Bias: -19.375861, T: 17355133, Avg. loss: 0.333965\n",
      "Total training time: 6.02 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 3.43, NNZs: 106, Bias: -16.493349, T: 19034662, Avg. loss: 0.330777\n",
      "Total training time: 6.22 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 3.42, NNZs: 106, Bias: -19.328782, T: 17914976, Avg. loss: 0.332487\n",
      "Total training time: 6.21 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 3.36, NNZs: 106, Bias: -16.430727, T: 19594505, Avg. loss: 0.329685\n",
      "Total training time: 6.40 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 3.37, NNZs: 106, Bias: -19.281011, T: 18474819, Avg. loss: 0.331328\n",
      "Total training time: 6.40 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 3.29, NNZs: 106, Bias: -16.366839, T: 20154348, Avg. loss: 0.328605\n",
      "Total training time: 6.57 seconds.\n",
      "-- Epoch 37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 3.30, NNZs: 106, Bias: -19.235707, T: 19034662, Avg. loss: 0.330122\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 3.23, NNZs: 106, Bias: -16.304192, T: 20714191, Avg. loss: 0.327618\n",
      "Total training time: 6.75 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 3.26, NNZs: 106, Bias: -19.187580, T: 19594505, Avg. loss: 0.328983\n",
      "Total training time: 6.78 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 3.17, NNZs: 106, Bias: -16.239857, T: 21274034, Avg. loss: 0.326646\n",
      "Total training time: 6.93 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 3.20, NNZs: 106, Bias: -19.143339, T: 20154348, Avg. loss: 0.327817\n",
      "Total training time: 6.97 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 3.12, NNZs: 106, Bias: -16.176292, T: 21833877, Avg. loss: 0.325790\n",
      "Total training time: 7.11 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 3.16, NNZs: 106, Bias: -19.096856, T: 20714191, Avg. loss: 0.326790\n",
      "Total training time: 7.16 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 3.05, NNZs: 106, Bias: -16.113901, T: 22393720, Avg. loss: 0.324960\n",
      "Total training time: 7.28 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 3.11, NNZs: 106, Bias: -19.051611, T: 21274034, Avg. loss: 0.325902\n",
      "Total training time: 7.35 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 3.00, NNZs: 106, Bias: -16.051714, T: 22953563, Avg. loss: 0.324102\n",
      "Total training time: 7.46 seconds.\n",
      "Convergence after 41 epochs took 7.46 seconds\n",
      "-- Epoch 1\n",
      "Norm: 3.08, NNZs: 106, Bias: -19.003828, T: 21833877, Avg. loss: 0.324998\n",
      "Total training time: 7.53 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 29.90, NNZs: 106, Bias: -1.719222, T: 559843, Avg. loss: 6.175123\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.03, NNZs: 106, Bias: -18.959062, T: 22393720, Avg. loss: 0.324255\n",
      "Total training time: 7.71 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 23.60, NNZs: 106, Bias: -2.793272, T: 1119686, Avg. loss: 0.864295\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.00, NNZs: 106, Bias: -18.911856, T: 22953563, Avg. loss: 0.323368\n",
      "Total training time: 7.89 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 20.49, NNZs: 106, Bias: -3.588223, T: 1679529, Avg. loss: 0.619645\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.96, NNZs: 106, Bias: -18.865777, T: 23513406, Avg. loss: 0.322602\n",
      "Total training time: 8.06 seconds.\n",
      "Convergence after 42 epochs took 8.06 seconds\n",
      "-- Epoch 1\n",
      "Norm: 18.50, NNZs: 106, Bias: -4.161154, T: 2239372, Avg. loss: 0.518281\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.57, NNZs: 106, Bias: 21.275405, T: 559843, Avg. loss: 12.901507\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.17, NNZs: 106, Bias: -4.630992, T: 2799215, Avg. loss: 0.463926\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 16.17, NNZs: 106, Bias: -5.019660, T: 3359058, Avg. loss: 0.430368\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 30.82, NNZs: 106, Bias: 23.172873, T: 1119686, Avg. loss: 1.792004\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.40, NNZs: 106, Bias: -5.364200, T: 3918901, Avg. loss: 0.407676\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 26.32, NNZs: 106, Bias: 24.174425, T: 1679529, Avg. loss: 1.285063\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.84, NNZs: 106, Bias: -5.650433, T: 4478744, Avg. loss: 0.391357\n",
      "Total training time: 1.41 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 23.93, NNZs: 106, Bias: 24.818252, T: 2239372, Avg. loss: 1.080461\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.38, NNZs: 106, Bias: -5.905486, T: 5038587, Avg. loss: 0.378147\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 22.46, NNZs: 106, Bias: 25.246296, T: 2799215, Avg. loss: 0.967779\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 14.02, NNZs: 106, Bias: -6.123655, T: 5598430, Avg. loss: 0.367850\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 21.44, NNZs: 106, Bias: 25.558910, T: 3359058, Avg. loss: 0.899377\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 13.71, NNZs: 106, Bias: -6.327765, T: 6158273, Avg. loss: 0.360210\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 13.42, NNZs: 106, Bias: -6.512915, T: 6718116, Avg. loss: 0.353624\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 20.83, NNZs: 106, Bias: 25.796663, T: 3918901, Avg. loss: 0.851279\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 13.16, NNZs: 106, Bias: -6.680230, T: 7277959, Avg. loss: 0.348283\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 20.32, NNZs: 106, Bias: 25.992487, T: 4478744, Avg. loss: 0.817800\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 12.95, NNZs: 106, Bias: -6.833806, T: 7837802, Avg. loss: 0.343600\n",
      "Total training time: 2.47 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 19.92, NNZs: 106, Bias: 26.133857, T: 5038587, Avg. loss: 0.791060\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 12.75, NNZs: 106, Bias: -6.976658, T: 8397645, Avg. loss: 0.339556\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.61, NNZs: 106, Bias: 26.267734, T: 5598430, Avg. loss: 0.771321\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 12.59, NNZs: 106, Bias: -7.102433, T: 8957488, Avg. loss: 0.335796\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 19.37, NNZs: 106, Bias: 26.370043, T: 6158273, Avg. loss: 0.754556\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 12.44, NNZs: 106, Bias: -7.226928, T: 9517331, Avg. loss: 0.332227\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 12.30, NNZs: 106, Bias: -7.339377, T: 10077174, Avg. loss: 0.330321\n",
      "Total training time: 3.19 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 19.16, NNZs: 106, Bias: 26.447805, T: 6718116, Avg. loss: 0.742292\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 12.18, NNZs: 106, Bias: -7.446630, T: 10637017, Avg. loss: 0.327552\n",
      "Total training time: 3.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 18.99, NNZs: 106, Bias: 26.520751, T: 7277959, Avg. loss: 0.730782\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 12.06, NNZs: 106, Bias: -7.550209, T: 11196860, Avg. loss: 0.325577\n",
      "Total training time: 3.55 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 18.82, NNZs: 106, Bias: 26.584427, T: 7837802, Avg. loss: 0.721955\n",
      "Total training time: 3.02 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 11.97, NNZs: 106, Bias: -7.640808, T: 11756703, Avg. loss: 0.323375\n",
      "Total training time: 3.73 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 18.70, NNZs: 106, Bias: 26.633715, T: 8397645, Avg. loss: 0.713125\n",
      "Total training time: 3.24 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 11.87, NNZs: 106, Bias: -7.729634, T: 12316546, Avg. loss: 0.321707\n",
      "Total training time: 3.91 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 11.79, NNZs: 106, Bias: -7.812923, T: 12876389, Avg. loss: 0.319993\n",
      "Total training time: 4.09 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 18.59, NNZs: 106, Bias: 26.678766, T: 8957488, Avg. loss: 0.706054\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 11.70, NNZs: 106, Bias: -7.889684, T: 13436232, Avg. loss: 0.318595\n",
      "Total training time: 4.26 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 18.49, NNZs: 106, Bias: 26.724166, T: 9517331, Avg. loss: 0.699013\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 11.62, NNZs: 106, Bias: -7.962619, T: 13996075, Avg. loss: 0.317116\n",
      "Total training time: 4.44 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.41, NNZs: 106, Bias: 26.750656, T: 10077174, Avg. loss: 0.693589\n",
      "Total training time: 3.91 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 11.55, NNZs: 106, Bias: -8.037652, T: 14555918, Avg. loss: 0.315740\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 18.33, NNZs: 106, Bias: 26.785381, T: 10637017, Avg. loss: 0.689054\n",
      "Total training time: 4.13 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 11.48, NNZs: 106, Bias: -8.105041, T: 15115761, Avg. loss: 0.314755\n",
      "Total training time: 4.80 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 11.40, NNZs: 106, Bias: -8.167405, T: 15675604, Avg. loss: 0.313724\n",
      "Total training time: 4.98 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 18.26, NNZs: 106, Bias: 26.808229, T: 11196860, Avg. loss: 0.684261\n",
      "Total training time: 4.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 11.35, NNZs: 106, Bias: -8.230135, T: 16235447, Avg. loss: 0.312370\n",
      "Total training time: 5.16 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 18.19, NNZs: 106, Bias: 26.830815, T: 11756703, Avg. loss: 0.680611\n",
      "Total training time: 4.58 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 11.29, NNZs: 106, Bias: -8.289468, T: 16795290, Avg. loss: 0.311799\n",
      "Total training time: 5.33 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 18.15, NNZs: 106, Bias: 26.848407, T: 12316546, Avg. loss: 0.676792\n",
      "Total training time: 4.81 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 11.23, NNZs: 106, Bias: -8.345693, T: 17355133, Avg. loss: 0.310914\n",
      "Total training time: 5.51 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 18.09, NNZs: 106, Bias: 26.866658, T: 12876389, Avg. loss: 0.673678\n",
      "Total training time: 5.04 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 11.17, NNZs: 106, Bias: -8.397860, T: 17914976, Avg. loss: 0.309976\n",
      "Total training time: 5.69 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 11.11, NNZs: 106, Bias: -8.452296, T: 18474819, Avg. loss: 0.309174\n",
      "Total training time: 5.87 seconds.\n",
      "-- Epoch 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 18.05, NNZs: 106, Bias: 26.880358, T: 13436232, Avg. loss: 0.670756\n",
      "Total training time: 5.27 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 11.06, NNZs: 106, Bias: -8.503478, T: 19034662, Avg. loss: 0.308395\n",
      "Total training time: 6.05 seconds.\n",
      "Convergence after 34 epochs took 6.05 seconds\n",
      "Norm: 18.01, NNZs: 106, Bias: 26.894263, T: 13996075, Avg. loss: 0.667337\n",
      "Total training time: 5.48 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 17.96, NNZs: 106, Bias: 26.906848, T: 14555918, Avg. loss: 0.665278\n",
      "Total training time: 5.68 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 17.94, NNZs: 106, Bias: 26.915650, T: 15115761, Avg. loss: 0.662719\n",
      "Total training time: 5.89 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.91, NNZs: 106, Bias: 26.923469, T: 15675604, Avg. loss: 0.660087\n",
      "Total training time: 6.09 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 17.88, NNZs: 106, Bias: 26.933494, T: 16235447, Avg. loss: 0.658829\n",
      "Total training time: 6.29 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 17.85, NNZs: 106, Bias: 26.940670, T: 16795290, Avg. loss: 0.656329\n",
      "Total training time: 6.50 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 17.82, NNZs: 106, Bias: 26.948244, T: 17355133, Avg. loss: 0.655183\n",
      "Total training time: 6.69 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 17.78, NNZs: 106, Bias: 26.954491, T: 17914976, Avg. loss: 0.653702\n",
      "Total training time: 6.89 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.76, NNZs: 106, Bias: 26.960530, T: 18474819, Avg. loss: 0.651881\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 17.74, NNZs: 106, Bias: 26.963693, T: 19034662, Avg. loss: 0.650610\n",
      "Total training time: 7.29 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 17.72, NNZs: 106, Bias: 26.967843, T: 19594505, Avg. loss: 0.648792\n",
      "Total training time: 7.49 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 17.69, NNZs: 106, Bias: 26.971330, T: 20154348, Avg. loss: 0.647985\n",
      "Total training time: 7.69 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 17.67, NNZs: 106, Bias: 26.974216, T: 20714191, Avg. loss: 0.646600\n",
      "Total training time: 7.89 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 17.66, NNZs: 106, Bias: 26.973253, T: 21274034, Avg. loss: 0.645899\n",
      "Total training time: 8.09 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 17.65, NNZs: 106, Bias: 26.974661, T: 21833877, Avg. loss: 0.643930\n",
      "Total training time: 8.28 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 17.63, NNZs: 106, Bias: 26.977808, T: 22393720, Avg. loss: 0.643374\n",
      "Total training time: 8.49 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 17.62, NNZs: 106, Bias: 26.975622, T: 22953563, Avg. loss: 0.642301\n",
      "Total training time: 8.69 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 17.60, NNZs: 106, Bias: 26.980393, T: 23513406, Avg. loss: 0.641804\n",
      "Total training time: 8.89 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 17.57, NNZs: 106, Bias: 26.982883, T: 24073249, Avg. loss: 0.640447\n",
      "Total training time: 9.09 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 17.57, NNZs: 106, Bias: 26.980017, T: 24633092, Avg. loss: 0.639769\n",
      "Total training time: 9.29 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 17.55, NNZs: 106, Bias: 26.982809, T: 25192935, Avg. loss: 0.638664\n",
      "Total training time: 9.48 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 17.54, NNZs: 106, Bias: 26.982805, T: 25752778, Avg. loss: 0.638276\n",
      "Total training time: 9.68 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 17.53, NNZs: 106, Bias: 26.982421, T: 26312621, Avg. loss: 0.637621\n",
      "Total training time: 9.88 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 17.52, NNZs: 106, Bias: 26.979042, T: 26872464, Avg. loss: 0.636589\n",
      "Total training time: 10.08 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 17.51, NNZs: 106, Bias: 26.978326, T: 27432307, Avg. loss: 0.635930\n",
      "Total training time: 10.28 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 17.50, NNZs: 106, Bias: 26.977948, T: 27992150, Avg. loss: 0.635150\n",
      "Total training time: 10.48 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 17.50, NNZs: 106, Bias: 26.977207, T: 28551993, Avg. loss: 0.634861\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 17.48, NNZs: 106, Bias: 26.976132, T: 29111836, Avg. loss: 0.634088\n",
      "Total training time: 10.88 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 17.48, NNZs: 106, Bias: 26.974090, T: 29671679, Avg. loss: 0.633926\n",
      "Total training time: 11.07 seconds.\n",
      "Convergence after 53 epochs took 11.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:   19.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6382349368752724\n",
      "CPU times: user 34.5 s, sys: 1.52 s, total: 36 s\n",
      "Wall time: 22.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier(n_jobs=jobs, random_state=42, verbose=1)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparallelized SVC that scales quadratically with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]iter  1 act 2.847e+05 pre 2.847e+05 delta 3.119e-02 f 5.598e+05 |g| 1.826e+07 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 3.585e+03 pre 3.585e+03 delta 1.247e-01 f 2.751e+05 |g| 1.214e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 1.068e+04 pre 1.068e+04 delta 2.847e-01 f 2.715e+05 |g| 1.628e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 8.020e+03 pre 7.753e+03 delta 2.980e-01 f 2.608e+05 |g| 8.419e+04 CG   3\n",
      "iter  5 act 4.262e+02 pre 4.155e+02 delta 2.980e-01 f 2.528e+05 |g| 1.037e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 1.436e+03 pre 1.424e+03 delta 4.560e-01 f 2.524e+05 |g| 1.629e+04 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter  7 act 1.691e+03 pre 1.705e+03 delta 5.279e-01 f 2.510e+05 |g| 1.714e+04 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 1.906e+03 pre 1.819e+03 delta 7.346e-01 f 2.493e+05 |g| 3.919e+04 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter  9 act 1.269e+03 pre 1.237e+03 delta 9.090e-01 f 2.474e+05 |g| 2.163e+04 CG   7\n",
      "iter 10 act 7.534e+01 pre 7.444e+01 delta 9.090e-01 f 2.461e+05 |g| 3.316e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 11 act 5.202e+02 pre 4.927e+02 delta 1.004e+00 f 2.460e+05 |g| 4.161e+03 CG  10\n",
      "iter 12 act 3.050e+01 pre 3.058e+01 delta 1.004e+00 f 2.455e+05 |g| 1.696e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 13 act 2.702e+02 pre 2.782e+02 delta 1.004e+00 f 2.455e+05 |g| 1.504e+03 CG   8\n",
      "iter 14 act 1.949e+01 pre 1.948e+01 delta 1.004e+00 f 2.452e+05 |g| 1.145e+04 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 15 act 3.531e+02 pre 3.480e+02 delta 1.340e+00 f 2.452e+05 |g| 1.293e+03 CG  12\n",
      "iter 16 act 2.579e+01 pre 2.570e+01 delta 1.340e+00 f 2.448e+05 |g| 8.438e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 17 act 2.265e+02 pre 2.289e+02 delta 1.490e+00 f 2.448e+05 |g| 8.922e+02 CG  12\n",
      "iter 18 act 1.209e+02 pre 1.184e+02 delta 1.490e+00 f 2.446e+05 |g| 6.993e+03 CG   8\n",
      "iter 19 act 9.518e-01 pre 9.515e-01 delta 1.490e+00 f 2.445e+05 |g| 5.210e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 20 act 1.925e+02 pre 1.932e+02 delta 1.914e+00 f 2.444e+05 |g| 3.677e+02 CG  11\n",
      "iter 21 act 2.780e+00 pre 2.779e+00 delta 1.914e+00 f 2.443e+05 |g| 6.934e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 22 act 2.086e+02 pre 2.044e+02 delta 2.399e+00 f 2.443e+05 |g| 5.416e+02 CG  12\n",
      "iter 23 act 8.730e+00 pre 8.730e+00 delta 2.399e+00 f 2.440e+05 |g| 7.021e+03 CG   2\n",
      "iter 24 act 6.495e+01 pre 6.431e+01 delta 2.399e+00 f 2.440e+05 |g| 3.686e+02 CG  20\n",
      "iter 25 act 3.401e-03 pre 3.401e-03 delta 2.399e+00 f 2.440e+05 |g| 1.780e+03 CG   1\n",
      "iter  1 act 2.844e+05 pre 2.844e+05 delta 3.117e-02 f 5.598e+05 |g| 1.825e+07 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 5.480e+03 pre 5.480e+03 delta 1.247e-01 f 2.754e+05 |g| 1.831e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 1.810e+04 pre 1.810e+04 delta 4.112e-01 f 2.699e+05 |g| 2.488e+05 CG   2\n",
      "iter  4 act 2.137e+04 pre 2.005e+04 delta 4.112e-01 f 2.519e+05 |g| 1.644e+05 CG   3\n",
      "iter  5 act 1.291e+02 pre 1.283e+02 delta 4.112e-01 f 2.305e+05 |g| 3.595e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 2.100e+03 pre 1.909e+03 delta 5.584e-01 f 2.304e+05 |g| 1.896e+04 CG   5\n",
      "iter  7 act 1.495e+01 pre 1.491e+01 delta 5.584e-01 f 2.283e+05 |g| 1.161e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 1.018e+03 pre 1.013e+03 delta 6.095e-01 f 2.282e+05 |g| 6.427e+03 CG   4\n",
      "iter  9 act 3.499e+02 pre 3.373e+02 delta 6.095e-01 f 2.272e+05 |g| 4.250e+04 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 5.792e+02 pre 5.521e+02 delta 6.915e-01 f 2.269e+05 |g| 1.775e+04 CG   7\n",
      "iter 11 act 3.522e+01 pre 3.517e+01 delta 6.915e-01 f 2.263e+05 |g| 2.522e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 12 act 2.750e+02 pre 2.701e+02 delta 8.112e-01 f 2.263e+05 |g| 2.469e+03 CG   8\n",
      "iter 13 act 2.203e+01 pre 2.213e+01 delta 8.112e-01 f 2.260e+05 |g| 1.160e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 2.122e+02 pre 2.194e+02 delta 8.112e-01 f 2.260e+05 |g| 1.083e+03 CG  10\n",
      "iter 15 act 1.702e+02 pre 1.650e+02 delta 8.112e-01 f 2.257e+05 |g| 5.993e+03 CG   9\n",
      "iter 16 act 6.765e-02 pre 6.763e-02 delta 8.112e-01 f 2.256e+05 |g| 7.494e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 17 act 1.630e+02 pre 1.669e+02 delta 8.112e-01 f 2.256e+05 |g| 5.271e+02 CG   9\n",
      "iter 18 act 1.349e+02 pre 1.310e+02 delta 8.112e-01 f 2.254e+05 |g| 7.255e+03 CG   7\n",
      "iter 19 act 4.757e-02 pre 4.757e-02 delta 8.112e-01 f 2.253e+05 |g| 6.282e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 20 act 1.373e+02 pre 1.395e+02 delta 8.323e-01 f 2.253e+05 |g| 4.728e+02 CG   8\n",
      "iter 21 act 1.280e+02 pre 1.255e+02 delta 8.323e-01 f 2.251e+05 |g| 5.803e+03 CG   9\n",
      "iter 22 act 3.056e-02 pre 3.056e-02 delta 8.323e-01 f 2.250e+05 |g| 5.033e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 23 act 1.248e+02 pre 1.268e+02 delta 8.328e-01 f 2.250e+05 |g| 4.346e+02 CG  11\n",
      "cg reaches trust region boundary\n",
      "iter 24 act 2.419e+02 pre 2.392e+02 delta 1.317e+00 f 2.249e+05 |g| 2.214e+03 CG  12\n",
      "iter 25 act 2.523e+00 pre 2.522e+00 delta 1.317e+00 f 2.246e+05 |g| 5.070e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 26 act 1.872e+02 pre 1.883e+02 delta 1.679e+00 f 2.246e+05 |g| 3.261e+02 CG  12\n",
      "iter 27 act 9.313e+01 pre 9.138e+01 delta 1.679e+00 f 2.245e+05 |g| 4.423e+03 CG   9\n",
      "iter 28 act 1.800e-02 pre 1.800e-02 delta 1.679e+00 f 2.244e+05 |g| 3.874e+03 CG   1\n",
      "iter  1 act 2.863e+05 pre 2.863e+05 delta 3.127e-02 f 5.598e+05 |g| 1.831e+07 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 4.488e+03 pre 4.488e+03 delta 1.251e-01 f 2.736e+05 |g| 1.489e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 1.509e+04 pre 1.509e+04 delta 4.560e-01 f 2.691e+05 |g| 2.031e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 2.690e+04 pre 2.436e+04 delta 5.515e-01 f 2.540e+05 |g| 1.407e+05 CG   3\n",
      "iter  5 act 2.529e+02 pre 2.509e+02 delta 5.515e-01 f 2.271e+05 |g| 5.004e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 7.602e+03 pre 7.237e+03 delta 6.286e-01 f 2.268e+05 |g| 3.035e+04 CG   4\n",
      "iter  7 act 3.262e+01 pre 3.251e+01 delta 6.286e-01 f 2.192e+05 |g| 1.689e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 3.730e+03 pre 3.610e+03 delta 1.124e+00 f 2.192e+05 |g| 1.389e+04 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter  9 act 3.758e+03 pre 3.657e+03 delta 1.274e+00 f 2.155e+05 |g| 7.074e+04 CG   5\n",
      "iter 10 act 1.316e+03 pre 1.325e+03 delta 1.274e+00 f 2.117e+05 |g| 8.780e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 11 act 2.775e+03 pre 2.554e+03 delta 1.582e+00 f 2.104e+05 |g| 3.310e+04 CG   4\n",
      "iter 12 act 7.009e+02 pre 7.065e+02 delta 1.582e+00 f 2.076e+05 |g| 9.759e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 13 act 1.977e+03 pre 1.916e+03 delta 1.773e+00 f 2.069e+05 |g| 1.689e+04 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 2.481e+03 pre 2.448e+03 delta 2.724e+00 f 2.050e+05 |g| 1.304e+04 CG   7\n",
      "iter 15 act 1.598e+02 pre 1.599e+02 delta 2.724e+00 f 2.025e+05 |g| 3.746e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 16 act 1.538e+03 pre 1.571e+03 delta 3.127e+00 f 2.023e+05 |g| 4.240e+03 CG   7\n",
      "iter 17 act 4.398e+02 pre 4.328e+02 delta 3.127e+00 f 2.008e+05 |g| 3.807e+04 CG   3\n",
      "iter 18 act 7.896e+00 pre 7.874e+00 delta 3.127e+00 f 2.003e+05 |g| 1.151e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 19 act 8.249e+02 pre 8.139e+02 delta 3.352e+00 f 2.003e+05 |g| 7.074e+02 CG   7\n",
      "iter 20 act 3.843e+01 pre 3.819e+01 delta 3.352e+00 f 1.995e+05 |g| 2.585e+04 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 21 act 5.290e+02 pre 5.302e+02 delta 3.389e+00 f 1.995e+05 |g| 1.457e+03 CG  13\n",
      "iter 22 act 8.888e+01 pre 8.867e+01 delta 3.389e+00 f 1.989e+05 |g| 1.137e+04 CG   3\n",
      "iter 23 act 2.011e+02 pre 2.002e+02 delta 3.389e+00 f 1.988e+05 |g| 2.671e+03 CG  15\n",
      "iter 24 act 2.522e-02 pre 2.522e-02 delta 3.389e+00 f 1.986e+05 |g| 4.122e+03 CG   1\n",
      "iter  1 act 1.120e+04 pre 1.120e+04 delta 6.195e-03 f 5.598e+05 |g| 3.615e+06 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 1.256e+03 pre 1.256e+03 delta 2.478e-02 f 5.486e+05 |g| 2.099e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 5.038e+03 pre 5.038e+03 delta 9.912e-02 f 5.474e+05 |g| 2.870e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 1.823e+04 pre 1.823e+04 delta 3.965e-01 f 5.424e+05 |g| 2.889e+05 CG   2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg reaches trust region boundary\n",
      "iter  5 act 4.403e+04 pre 4.403e+04 delta 5.796e-01 f 5.241e+05 |g| 2.327e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 3.146e+04 pre 3.145e+04 delta 8.367e-01 f 4.801e+05 |g| 1.001e+05 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  7 act 1.111e+04 pre 1.109e+04 delta 1.008e+00 f 4.486e+05 |g| 4.690e+04 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 9.898e+03 pre 9.906e+03 delta 1.156e+00 f 4.375e+05 |g| 3.583e+04 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter  9 act 1.024e+04 pre 1.006e+04 delta 1.662e+00 f 4.276e+05 |g| 3.130e+04 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 6.485e+03 pre 6.401e+03 delta 2.000e+00 f 4.174e+05 |g| 7.912e+04 CG   7\n",
      "iter 11 act 1.149e+03 pre 1.141e+03 delta 2.000e+00 f 4.109e+05 |g| 5.068e+04 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter 12 act 4.063e+03 pre 4.059e+03 delta 2.758e+00 f 4.098e+05 |g| 8.104e+03 CG   5\n",
      "iter 13 act 1.209e+03 pre 1.205e+03 delta 2.758e+00 f 4.057e+05 |g| 6.625e+04 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 3.578e+03 pre 3.568e+03 delta 2.873e+00 f 4.045e+05 |g| 6.628e+03 CG   6\n",
      "iter 15 act 8.657e+01 pre 8.649e+01 delta 2.873e+00 f 4.009e+05 |g| 3.055e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 16 act 4.671e+03 pre 4.642e+03 delta 3.094e+00 f 4.008e+05 |g| 2.180e+04 CG  12\n",
      "iter 17 act 1.207e+03 pre 1.243e+03 delta 3.094e+00 f 3.962e+05 |g| 1.537e+04 CG   8\n",
      "iter 18 act 3.238e-01 pre 3.237e-01 delta 3.094e+00 f 3.949e+05 |g| 1.872e+04 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 19 act 1.869e+03 pre 1.814e+03 delta 3.513e+00 f 3.949e+05 |g| 1.643e+03 CG  10\n",
      "iter 20 act 2.713e+02 pre 2.717e+02 delta 3.513e+00 f 3.931e+05 |g| 5.376e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 21 act 2.233e+03 pre 2.242e+03 delta 6.284e+00 f 3.928e+05 |g| 7.827e+03 CG  13\n",
      "iter 22 act 3.648e+02 pre 3.584e+02 delta 6.284e+00 f 3.906e+05 |g| 9.566e+03 CG   9\n",
      "cg reaches trust region boundary\n",
      "iter 23 act 2.632e+03 pre 2.632e+03 delta 1.226e+01 f 3.902e+05 |g| 5.887e+03 CG  12\n",
      "iter 24 act 1.253e+02 pre 1.255e+02 delta 1.226e+01 f 3.876e+05 |g| 7.703e+03 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter 25 act 2.355e+03 pre 2.301e+03 delta 1.328e+01 f 3.874e+05 |g| 8.548e+02 CG  13\n",
      "iter 26 act 7.887e+01 pre 7.887e+01 delta 1.328e+01 f 3.851e+05 |g| 7.931e+03 CG   3\n",
      "iter 27 act 2.361e+02 pre 2.351e+02 delta 1.328e+01 f 3.850e+05 |g| 4.261e+02 CG  25\n",
      "iter 28 act 3.877e-02 pre 3.876e-02 delta 1.328e+01 f 3.848e+05 |g| 1.094e+03 CG   2\n",
      "Accuracy: 0.6517458434849708\n",
      "CPU times: user 1min 32s, sys: 2.15 s, total: 1min 34s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import LinearSVC\n",
    "svc_model = LinearSVC(random_state=42, verbose=1)\n",
    "svc_model.fit(X_train, y_train)\n",
    "y_pred = svc_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "937q3D9VAzlM"
   },
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Perform hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMKS-cKqbkL0",
    "outputId": "f1e56f55-3340-423b-8fe5-ac06e95cb71c"
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Perform hyperparameter tuning with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=5, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDFlld25Xze5"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-entropy was previously calculated, no need to recompute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   32.0s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6291609805588699\n",
      "CPU times: user 2min 17s, sys: 3.55 s, total: 2min 20s\n",
      "Wall time: 1min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=jobs, verbose=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFkkQCBxXlWh",
    "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "min_entropy_train = vectorized_entropy(X_train)\n",
    "min_entropy_test = vectorized_entropy(X_test)\n",
    "\n",
    "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
    "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
    "# Create the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_with_entropy, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
    "\n",
    "# Calculate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KULNfXpncJZC",
    "outputId": "f44dab80-2cb1-46c1-ab5f-a26725d94c9c"
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt'],       # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_rf = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DfNGSyaX3kV"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1108           24.04m\n",
      "         2           1.0778           22.78m\n",
      "         3           1.0512           21.91m\n",
      "         4           1.0299           21.41m\n",
      "         5           1.0125           21.07m\n",
      "         6           0.9977           20.75m\n",
      "         7           0.9857           20.53m\n",
      "         8           0.9756           20.37m\n",
      "         9           0.9667           20.15m\n",
      "        10           0.9587           19.91m\n",
      "        20           0.9207           17.41m\n",
      "        30           0.9086           15.15m\n",
      "        40           0.9003           12.95m\n",
      "        50           0.8930           10.74m\n",
      "        60           0.8868            8.56m\n",
      "        70           0.8815            6.40m\n",
      "        80           0.8770            4.26m\n",
      "        90           0.8723            2.13m\n",
      "       100           0.8683            0.00s\n",
      "Accuracy: 0.6461299933552919\n",
      "CPU times: user 21min 18s, sys: 1.02 s, total: 21min 19s\n",
      "Wall time: 21min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42, verbose=1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parallelized version is available as Histogram Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.427 GB of training data: 0.978 s\n",
      "Binning 0.047 GB of validation data: 0.038 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 1.08670, val loss: 1.08687, in 0.420s\n",
      "[2/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 1.04099, val loss: 1.04134, in 0.420s\n",
      "[3/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 1.00740, val loss: 1.00793, in 0.421s\n",
      "[4/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 0.98205, val loss: 0.98271, in 0.427s\n",
      "[5/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.96280, val loss: 0.96365, in 0.423s\n",
      "[6/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.94763, val loss: 0.94871, in 0.426s\n",
      "[7/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.93574, val loss: 0.93690, in 0.460s\n",
      "[8/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.92626, val loss: 0.92751, in 0.429s\n",
      "[9/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.91870, val loss: 0.92010, in 0.444s\n",
      "[10/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.91239, val loss: 0.91394, in 0.433s\n",
      "[11/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.90729, val loss: 0.90894, in 0.436s\n",
      "[12/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.90299, val loss: 0.90485, in 0.449s\n",
      "[13/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.89917, val loss: 0.90124, in 0.450s\n",
      "[14/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.89580, val loss: 0.89811, in 0.457s\n",
      "[15/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.89289, val loss: 0.89542, in 0.465s\n",
      "[16/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.89017, val loss: 0.89287, in 0.460s\n",
      "[17/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.88764, val loss: 0.89063, in 0.464s\n",
      "[18/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.88531, val loss: 0.88854, in 0.470s\n",
      "[19/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.88331, val loss: 0.88673, in 0.448s\n",
      "[20/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.88128, val loss: 0.88495, in 0.455s\n",
      "[21/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.87946, val loss: 0.88340, in 0.468s\n",
      "[22/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.87772, val loss: 0.88182, in 0.462s\n",
      "[23/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.87609, val loss: 0.88041, in 0.473s\n",
      "[24/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.87445, val loss: 0.87894, in 0.488s\n",
      "[25/100] 4 trees, 124 leaves (31 on avg), max depth = 15, train loss: 0.87291, val loss: 0.87758, in 0.490s\n",
      "[26/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.87133, val loss: 0.87616, in 0.501s\n",
      "[27/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.86981, val loss: 0.87490, in 0.501s\n",
      "[28/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.86845, val loss: 0.87363, in 0.487s\n",
      "[29/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.86713, val loss: 0.87246, in 0.480s\n",
      "[30/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.86575, val loss: 0.87129, in 0.504s\n",
      "[31/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.86439, val loss: 0.87016, in 0.508s\n",
      "[32/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.86313, val loss: 0.86913, in 0.482s\n",
      "[33/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.86187, val loss: 0.86799, in 0.520s\n",
      "[34/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.86066, val loss: 0.86687, in 0.487s\n",
      "[35/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.85941, val loss: 0.86583, in 0.519s\n",
      "[36/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.85830, val loss: 0.86477, in 0.505s\n",
      "[37/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.85715, val loss: 0.86382, in 0.514s\n",
      "[38/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.85610, val loss: 0.86299, in 0.496s\n",
      "[39/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.85500, val loss: 0.86211, in 0.512s\n",
      "[40/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.85390, val loss: 0.86127, in 0.498s\n",
      "[41/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.85289, val loss: 0.86043, in 0.484s\n",
      "[42/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.85179, val loss: 0.85954, in 0.540s\n",
      "[43/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.85069, val loss: 0.85857, in 0.539s\n",
      "[44/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.84964, val loss: 0.85773, in 0.541s\n",
      "[45/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.84871, val loss: 0.85696, in 0.495s\n",
      "[46/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 0.84768, val loss: 0.85609, in 0.547s\n",
      "[47/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.84664, val loss: 0.85531, in 0.542s\n",
      "[48/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.84576, val loss: 0.85456, in 0.469s\n",
      "[49/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.84471, val loss: 0.85367, in 0.524s\n",
      "[50/100] 4 trees, 124 leaves (31 on avg), max depth = 7, train loss: 0.84373, val loss: 0.85287, in 0.540s\n",
      "[51/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.84277, val loss: 0.85202, in 0.530s\n",
      "[52/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.84184, val loss: 0.85131, in 0.538s\n",
      "[53/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.84091, val loss: 0.85053, in 0.545s\n",
      "[54/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.83999, val loss: 0.84979, in 0.533s\n",
      "[55/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.83910, val loss: 0.84908, in 0.533s\n",
      "[56/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.83820, val loss: 0.84839, in 0.549s\n",
      "[57/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.83730, val loss: 0.84770, in 0.656s\n",
      "[58/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.83635, val loss: 0.84686, in 0.772s\n",
      "[59/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.83551, val loss: 0.84623, in 0.576s\n",
      "[60/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.83476, val loss: 0.84563, in 0.535s\n",
      "[61/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.83402, val loss: 0.84516, in 0.477s\n",
      "[62/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.83322, val loss: 0.84449, in 0.541s\n",
      "[63/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.83253, val loss: 0.84397, in 0.500s\n",
      "[64/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.83168, val loss: 0.84332, in 0.555s\n",
      "[65/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.83088, val loss: 0.84271, in 0.569s\n",
      "[66/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.83002, val loss: 0.84204, in 0.528s\n",
      "[67/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.82921, val loss: 0.84134, in 0.549s\n",
      "[68/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.82858, val loss: 0.84092, in 0.476s\n",
      "[69/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.82782, val loss: 0.84037, in 0.544s\n",
      "[70/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.82712, val loss: 0.83979, in 0.551s\n",
      "[71/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.82644, val loss: 0.83932, in 0.493s\n",
      "[72/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.82583, val loss: 0.83893, in 0.479s\n",
      "[73/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.82517, val loss: 0.83847, in 0.525s\n",
      "[74/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.82442, val loss: 0.83790, in 0.526s\n",
      "[75/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.82384, val loss: 0.83752, in 0.467s\n",
      "[76/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.82323, val loss: 0.83713, in 0.494s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.82255, val loss: 0.83665, in 0.543s\n",
      "[78/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.82194, val loss: 0.83622, in 0.498s\n",
      "[79/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.82129, val loss: 0.83574, in 0.543s\n",
      "[80/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.82068, val loss: 0.83532, in 0.464s\n",
      "[81/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.82014, val loss: 0.83496, in 0.455s\n",
      "[82/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.81950, val loss: 0.83451, in 0.528s\n",
      "[83/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.81895, val loss: 0.83413, in 0.498s\n",
      "[84/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.81837, val loss: 0.83377, in 0.475s\n",
      "[85/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.81774, val loss: 0.83328, in 0.539s\n",
      "[86/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.81718, val loss: 0.83291, in 0.471s\n",
      "[87/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.81664, val loss: 0.83255, in 0.493s\n",
      "[88/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.81602, val loss: 0.83214, in 0.525s\n",
      "[89/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.81547, val loss: 0.83181, in 0.494s\n",
      "[90/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.81497, val loss: 0.83150, in 0.458s\n",
      "[91/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.81438, val loss: 0.83112, in 0.501s\n",
      "[92/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.81377, val loss: 0.83064, in 0.543s\n",
      "[93/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.81315, val loss: 0.83016, in 0.505s\n",
      "[94/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.81266, val loss: 0.82989, in 0.467s\n",
      "[95/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.81218, val loss: 0.82962, in 0.465s\n",
      "[96/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.81166, val loss: 0.82924, in 0.531s\n",
      "[97/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.81112, val loss: 0.82890, in 0.486s\n",
      "[98/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.81065, val loss: 0.82863, in 0.490s\n",
      "[99/100] 4 trees, 124 leaves (31 on avg), max depth = 8, train loss: 0.81011, val loss: 0.82828, in 0.652s\n",
      "[100/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.80967, val loss: 0.82796, in 0.809s\n",
      "Fit 400 trees in 54.074 s, (12400 total leaves)\n",
      "Time spent computing histograms: 42.030s\n",
      "Time spent finding best splits:  0.319s\n",
      "Time spent applying splits:      1.779s\n",
      "Time spent predicting:           0.467s\n",
      "Accuracy: 0.6661641457263094\n",
      "CPU times: user 3min 31s, sys: 1.39 s, total: 3min 32s\n",
      "Wall time: 58.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb_model = HistGradientBoostingClassifier(random_state=42, verbose=1)\n",
    "hgb_model.fit(X_train, y_train)\n",
    "y_pred = hgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBUNIc9wAzvF",
    "outputId": "91db7722-33bb-4ea7-d39d-e0d6f484d698"
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "# Train the model with the new feature included\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set with the new feature included\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model with the new feature\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy with Min-Entropy Feature:\", accuracy_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1L945YBW8hx"
   },
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iD-fcbeW6NG",
    "outputId": "f25bbfd6-eb0e-4516-fbd3-ee1355c603f6"
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "min_entropy_train = vectorized_entropy(X_train)\n",
    "min_entropy_test = vectorized_entropy(X_test)\n",
    "\n",
    "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
    "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
    "\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_with_entropy, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_gb = best_model.predict(X_test_with_entropy)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model with the best hyperparameters\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other gradient boosted tree methods may have different runtime/performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6rTZWuztjNr"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6704867784597138\n",
      "CPU times: user 13min 22s, sys: 464 ms, total: 13min 22s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=42, verbosity=1, n_jobs=jobs, tree_method='exact')\n",
    "y_train_mapped = y_train-1\n",
    "y_test_mapped = y_test-1\n",
    "xgb_model.fit(X_train, y_train_mapped)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6709726280892534\n",
      "CPU times: user 10min 8s, sys: 7.74 s, total: 10min 15s\n",
      "Wall time: 5min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=42, verbosity=1, n_jobs=jobs, tree_method='approx')\n",
    "y_train_mapped = y_train-1\n",
    "y_test_mapped = y_test-1\n",
    "xgb_model.fit(X_train, y_train_mapped)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6712226977515164\n",
      "CPU times: user 1min 11s, sys: 152 ms, total: 1min 11s\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=42, verbosity=1, n_jobs=jobs, tree_method='hist')\n",
    "y_train_mapped = y_train-1\n",
    "y_test_mapped = y_test-1\n",
    "xgb_model.fit(X_train, y_train_mapped)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_mapped, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4qdLReQtmZ1",
    "outputId": "85c7db2a-2634-4203-b910-d940e2277d82"
   },
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Map classes to [0, 1, 2]\n",
    "y_train_mapped = y_train - 1  # This will change classes [1, 2, 3] to [0, 1, 2]\n",
    "\n",
    "# Continue with the Grid Search\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train_mapped)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best XGBoost model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the XGBoost model with the best hyperparameters\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVKGPCOT3x9q"
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080542 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1013\n",
      "[LightGBM] [Info] Number of data points in the train set: 559843, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score -1.945730\n",
      "[LightGBM] [Info] Start training from score -1.946455\n",
      "[LightGBM] [Info] Start training from score -1.945855\n",
      "[LightGBM] [Info] Start training from score -0.559539\n",
      "Accuracy: 0.6617486299754932\n",
      "CPU times: user 1min 5s, sys: 620 ms, total: 1min 6s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_jobs=jobs, boosting_type='gbdt')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066279 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1013\n",
      "[LightGBM] [Info] Number of data points in the train set: 559843, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score -1.945730\n",
      "[LightGBM] [Info] Start training from score -1.946455\n",
      "[LightGBM] [Info] Start training from score -1.945855\n",
      "[LightGBM] [Info] Start training from score -0.559539\n",
      "Accuracy: 0.6453154807410636\n",
      "CPU times: user 1min 59s, sys: 636 ms, total: 2min\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_jobs=jobs, boosting_type='dart')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fails\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_jobs=jobs, boosting_type='rf')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXVAMJrx3xLr",
    "outputId": "c38cd918-64d1-4d74-ede9-90354ac3b778"
   },
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Create the LightGBM classifier\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
    "grid_search = GridSearchCV(lgb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best LightGBM model\n",
    "y_pred_lgb = best_lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the LightGBM model with the best hyperparameters\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"Best Hyperparameters for LightGBM:\", best_params)\n",
    "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3RxycX8X8wX"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-01 00:19:53.839222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-01 00:19:53.840532: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-01 00:19:53.897827: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-01 00:19:54.121602: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-01 00:19:55.054118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 651us/step - accuracy: 0.5955 - loss: 1.0133 - val_accuracy: 0.6251 - val_loss: 0.9119\n",
      "Epoch 2/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 641us/step - accuracy: 0.6243 - loss: 0.9103 - val_accuracy: 0.6213 - val_loss: 0.9240\n",
      "Epoch 3/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 653us/step - accuracy: 0.6372 - loss: 0.8796 - val_accuracy: 0.6212 - val_loss: 0.9333\n",
      "Epoch 4/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 652us/step - accuracy: 0.6432 - loss: 0.8672 - val_accuracy: 0.6235 - val_loss: 0.9291\n",
      "Epoch 5/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 667us/step - accuracy: 0.6461 - loss: 0.8618 - val_accuracy: 0.6295 - val_loss: 0.9107\n",
      "Epoch 6/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m24563s\u001b[0m 4s/step - accuracy: 0.6482 - loss: 0.8574 - val_accuracy: 0.6433 - val_loss: 0.8685\n",
      "Epoch 7/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - accuracy: 0.6499 - loss: 0.8540 - val_accuracy: 0.6485 - val_loss: 0.8573\n",
      "Epoch 8/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 606us/step - accuracy: 0.6505 - loss: 0.8521 - val_accuracy: 0.6484 - val_loss: 0.8576\n",
      "Epoch 9/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 569us/step - accuracy: 0.6518 - loss: 0.8502 - val_accuracy: 0.6514 - val_loss: 0.8511\n",
      "Epoch 10/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 553us/step - accuracy: 0.6527 - loss: 0.8489 - val_accuracy: 0.6510 - val_loss: 0.8537\n",
      "Epoch 11/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 556us/step - accuracy: 0.6530 - loss: 0.8476 - val_accuracy: 0.6529 - val_loss: 0.8490\n",
      "Epoch 12/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 551us/step - accuracy: 0.6538 - loss: 0.8461 - val_accuracy: 0.6529 - val_loss: 0.8502\n",
      "Epoch 13/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 554us/step - accuracy: 0.6542 - loss: 0.8450 - val_accuracy: 0.6538 - val_loss: 0.8469\n",
      "Epoch 14/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 555us/step - accuracy: 0.6547 - loss: 0.8438 - val_accuracy: 0.6547 - val_loss: 0.8446\n",
      "Epoch 15/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 556us/step - accuracy: 0.6550 - loss: 0.8427 - val_accuracy: 0.6551 - val_loss: 0.8447\n",
      "Epoch 16/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 562us/step - accuracy: 0.6557 - loss: 0.8418 - val_accuracy: 0.6559 - val_loss: 0.8430\n",
      "Epoch 17/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 557us/step - accuracy: 0.6557 - loss: 0.8409 - val_accuracy: 0.6532 - val_loss: 0.8492\n",
      "Epoch 18/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 559us/step - accuracy: 0.6563 - loss: 0.8403 - val_accuracy: 0.6527 - val_loss: 0.8509\n",
      "Epoch 19/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 566us/step - accuracy: 0.6564 - loss: 0.8397 - val_accuracy: 0.6539 - val_loss: 0.8480\n",
      "Epoch 20/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 567us/step - accuracy: 0.6566 - loss: 0.8388 - val_accuracy: 0.6592 - val_loss: 0.8339\n",
      "Epoch 21/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 572us/step - accuracy: 0.6573 - loss: 0.8377 - val_accuracy: 0.6593 - val_loss: 0.8323\n",
      "Epoch 22/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 576us/step - accuracy: 0.6572 - loss: 0.8370 - val_accuracy: 0.6598 - val_loss: 0.8305\n",
      "Epoch 23/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 576us/step - accuracy: 0.6575 - loss: 0.8364 - val_accuracy: 0.6600 - val_loss: 0.8298\n",
      "Epoch 24/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 583us/step - accuracy: 0.6580 - loss: 0.8357 - val_accuracy: 0.6600 - val_loss: 0.8295\n",
      "Epoch 25/25\n",
      "\u001b[1m6999/6999\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 584us/step - accuracy: 0.6579 - loss: 0.8351 - val_accuracy: 0.6601 - val_loss: 0.8294\n",
      "\u001b[1m4374/4374\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step\n",
      "Neural Network Accuracy: 0.6596051757275241\n",
      "CPU times: user 3min 2s, sys: 22 s, total: 3min 24s\n",
      "Wall time: 6h 51min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "y_train_integer = y_train-1 #.astype('float32')\n",
    "y_test_integer = y_test-1 #.astype('float32')\n",
    "\n",
    "X_train_small, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
    "X_train_small = X_train_small.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "# Create the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_train_small.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train_small, y_train_integer, epochs=25, batch_size=64, validation_data=(X_val, y_val_integer)) #, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = nn_model.predict(X_test.astype('float32'))\n",
    "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the Neural Network model\n",
    "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htyjktKSX-cN",
    "outputId": "04255ea4-c379-4f5f-9b09-8ff8dd70ef1e"
   },
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming you have already defined X_train, X_test, y_train, and y_test\n",
    "\n",
    "# Convert binary numbers to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_integer = label_encoder.transform(y_train)\n",
    "y_test_integer = label_encoder.transform(y_test)\n",
    "\n",
    "# Check unique values in y_train_integer and y_test_integer\n",
    "print(\"Unique values in y_train:\", np.unique(y_train_integer))\n",
    "print(\"Unique values in y_test:\", np.unique(y_test_integer))\n",
    "\n",
    "print(\"Shape of y_train_integer:\", y_train_integer.shape)\n",
    "print(\"Shape of y_test_integer:\", y_test_integer.shape)\n",
    "\n",
    "# Manually split the data into training and validation sets\n",
    "X_train, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train, y_train_integer, epochs=2, batch_size=64, validation_data=(X_val, y_val_integer), verbose=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = nn_model.predict(X_test)\n",
    "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the Neural Network model\n",
    "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGXQrE8d9d0x"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 746us/step - accuracy: 0.6123 - loss: 0.9561\n",
      "Epoch 2/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 758us/step - accuracy: 0.6431 - loss: 0.8707\n",
      "Epoch 3/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 770us/step - accuracy: 0.6541 - loss: 0.8472\n",
      "Epoch 4/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 775us/step - accuracy: 0.6590 - loss: 0.8374\n",
      "Epoch 5/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 775us/step - accuracy: 0.6616 - loss: 0.8317\n",
      "Epoch 6/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 781us/step - accuracy: 0.6635 - loss: 0.8279\n",
      "Epoch 7/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 784us/step - accuracy: 0.6646 - loss: 0.8253\n",
      "Epoch 8/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 785us/step - accuracy: 0.6657 - loss: 0.8232\n",
      "Epoch 9/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 791us/step - accuracy: 0.6664 - loss: 0.8216\n",
      "Epoch 10/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 792us/step - accuracy: 0.6668 - loss: 0.8202\n",
      "Epoch 11/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 797us/step - accuracy: 0.6675 - loss: 0.8190\n",
      "Epoch 12/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 794us/step - accuracy: 0.6681 - loss: 0.8179\n",
      "Epoch 13/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 800us/step - accuracy: 0.6685 - loss: 0.8169\n",
      "Epoch 14/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 801us/step - accuracy: 0.6689 - loss: 0.8159\n",
      "Epoch 15/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 804us/step - accuracy: 0.6694 - loss: 0.8151\n",
      "Epoch 16/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 805us/step - accuracy: 0.6699 - loss: 0.8143\n",
      "Epoch 17/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 810us/step - accuracy: 0.6703 - loss: 0.8136\n",
      "Epoch 18/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 810us/step - accuracy: 0.6707 - loss: 0.8129\n",
      "Epoch 19/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 816us/step - accuracy: 0.6712 - loss: 0.8122\n",
      "Epoch 20/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 813us/step - accuracy: 0.6715 - loss: 0.8116\n",
      "Epoch 21/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 815us/step - accuracy: 0.6718 - loss: 0.8110\n",
      "Epoch 22/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 816us/step - accuracy: 0.6720 - loss: 0.8105\n",
      "Epoch 23/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 819us/step - accuracy: 0.6720 - loss: 0.8100\n",
      "Epoch 24/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 818us/step - accuracy: 0.6724 - loss: 0.8096\n",
      "Epoch 25/25\n",
      "\u001b[1m8748/8748\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 821us/step - accuracy: 0.6724 - loss: 0.8092\n",
      "\u001b[1m4374/4374\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 534us/step\n",
      "LSTM Accuracy: 0.6539035874279263\n",
      "CPU times: user 6min 6s, sys: 31.4 s, total: 6min 38s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "y_train_integer = y_train-1 #.astype('float32')\n",
    "y_test_integer = y_test-1 #.astype('float32')\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "# TODO: LSTM with a single time step is not a sequence!\n",
    "time_steps = 1  # Each sample is treated as a single time step\n",
    "X_train_lstm = X_train.astype('float32').reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
    "X_test_lstm = X_test.astype('float32').reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
    "# y_train_lstm = y_train_integer.reshape(y_train_integer.shape[0], time_steps, y_train_integer.shape[1])\n",
    "\n",
    "# Create the Neural Network model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(time_steps, X_train_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_lstm, y_train_integer, epochs=25, batch_size=64)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the LSTM model\n",
    "accuracy_lstm = accuracy_score(y_test_integer, y_pred_lstm)\n",
    "print(\"LSTM Accuracy:\", accuracy_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2sE0n_hlbMAr",
    "outputId": "3cf01564-e8f0-431b-e490-48ab0299a741"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ... (Previous code for reading and preprocessing the data)\n",
    "\n",
    "# Convert the data into numerical format\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "time_steps = 1  # Each sample is treated as a single time step\n",
    "X_train_lstm = X_train.reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
    "X_test_lstm = X_test.reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
    "\n",
    "# Create the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(32, input_shape=(time_steps, X_train.shape[1])))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dense(3, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm = np.round(y_pred_lstm).astype(int).flatten()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate the accuracy of the LSTM model\n",
    "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "print(\"LSTM Accuracy:\", accuracy_lstm)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "unitaryhack",
   "language": "python",
   "name": "unitaryhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
