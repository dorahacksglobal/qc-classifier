{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Please Note**: As of now, this Jupyter notebook is under active development. Since June 5th, 2024, I have initiated a series of refinements and expansions. These updates include additional preprocessing functions, changes, and improvements to enhance the functionality and usability of the notebook. Your patience and understanding during this development phase are greatly appreciated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import fft, ifft\n",
        "from scipy.special import erfc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pre processing part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pre-processing part of this project was initially based on a previous commit from the public repository of Sid Chava, available at [QRNGClassifier Repository](https://github.com/sid-chava/QRNGClassifier).\n",
        "\n",
        "**QRNG Classifier Preprocessing Functions Enhancements**: As of June 8th, 2024, this Jupyter notebook is under active development. I am working on enhancing the preprocessing functions of the QRNG Classifier, which includes:\n",
        "\n",
        "1. **Refining Feature Extraction**: I am improving the methods used to extract features from the raw data. This involves using more sophisticated techniques or algorithms to better capture the characteristics of the data.\n",
        "\n",
        "2. **Introducing New Data Transformation Techniques**: I am implementing new techniques for transforming the data into a format that's more suitable for the classifier. This includes normalization, scaling, or other transformation methods.\n",
        "\n",
        "These improvements are aimed at enhancing the effectiveness of the preprocessing functions, which could potentially lead to better performance of the QRNG Classifier. Your patience and understanding during this development phase are greatly appreciated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the good results that we had previously was due to data leakage, after removing it, we see that by oversampling, the model is overfitting and not preforming as well as we thought it will be "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_5148\\989039983.py:138: RuntimeWarning: divide by zero encountered in log2\n",
            "  log_avg = np.mean(np.log2(t))\n",
            "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_5148\\989039983.py:138: RuntimeWarning: invalid value encountered in log2\n",
            "  log_avg = np.mean(np.log2(t))\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_5148\\989039983.py:183: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
            "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_5148\\989039983.py:183: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       label  shannon_entropy  classic_spectral_test  frequency_test  \\\n",
            "0          1         1.935451             -21.771553        0.423711   \n",
            "1          1         1.963615             -22.230385        0.841481   \n",
            "2          1         1.939471             -22.230385        0.109599   \n",
            "3          1         1.872164             -22.230385        0.071861   \n",
            "4          1         1.976281             -22.230385        0.230139   \n",
            "...      ...              ...                    ...             ...   \n",
            "13995      4         1.942653             -22.230385        0.689157   \n",
            "13996      4         1.919479             -22.230385        0.689157   \n",
            "13997      4         1.862236             -22.230385        0.317311   \n",
            "13998      4         1.856367             -22.230385        0.841481   \n",
            "13999      4         1.717977             -22.230385        0.071861   \n",
            "\n",
            "       runs_test  linear_complexity  autocorrelation_test  \\\n",
            "0       0.120217                0.0                  0.33   \n",
            "1       0.027240                0.0                  0.31   \n",
            "2       0.498506                0.0                  0.32   \n",
            "3       0.620874                0.0                  0.36   \n",
            "4       0.725698                0.0                  0.18   \n",
            "...          ...                ...                   ...   \n",
            "13995   0.556584                0.0                  0.24   \n",
            "13996   0.987149                0.0                  0.23   \n",
            "13997   0.011137                0.0                  0.36   \n",
            "13998   0.548989                0.0                  0.25   \n",
            "13999   0.051254                0.0                  0.21   \n",
            "\n",
            "       maurer_universal_test  binary_matrix_rank_test  cumulative_sums_test  \\\n",
            "0                   0.028726                      NaN                    10   \n",
            "1                        NaN                      NaN                     8   \n",
            "2                       -inf                      NaN                    21   \n",
            "3                        NaN                      NaN                    18   \n",
            "4                        NaN                      NaN                    18   \n",
            "...                      ...                      ...                   ...   \n",
            "13995                    NaN                      NaN                    10   \n",
            "13996              -0.014534                      NaN                     6   \n",
            "13997              -0.127187                      NaN                    15   \n",
            "13998                    NaN                      NaN                     8   \n",
            "13999                    NaN                      NaN                    24   \n",
            "\n",
            "       ...   90   91   92   93   94   95   96   97   98   99  \n",
            "0      ...  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
            "1      ...  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  \n",
            "2      ...  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
            "3      ...  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  \n",
            "4      ...  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  \n",
            "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
            "13995  ...  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  \n",
            "13996  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  \n",
            "13997  ...  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  \n",
            "13998  ...  1.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  \n",
            "13999  ...  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  \n",
            "\n",
            "[14000 rows x 118 columns]\n"
          ]
        }
      ],
      "source": [
        "# Concatenate data\n",
        "import itertools\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "def concatenateData(df, num_concats):\n",
        "    new_df = pd.DataFrame({\n",
        "        'Concatenated_Data': [''] * (len(df) // num_concats), \n",
        "        'label': [0] * (len(df) // num_concats)\n",
        "    })\n",
        "\n",
        "    # Loop through each group of num_concats rows and concatenate their 'binary_number' strings\n",
        "    for i in range(0, len(df), num_concats):\n",
        "        new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i + num_concats])\n",
        "        new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
        "\n",
        "    return new_df\n",
        "\n",
        "# Calculate Shannon entropy for each concatenated binary sequence\n",
        "def shannon_entropy(binary_string):\n",
        "    if len(binary_string) % 2 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
        "    \n",
        "    patterns = ['00', '10', '11', '01']\n",
        "    frequency = {pattern: 0 for pattern in patterns}\n",
        "    \n",
        "    for i in range(0, len(binary_string), 2):\n",
        "        segment = binary_string[i:i+2]\n",
        "        if segment in patterns:\n",
        "            frequency[segment] += 1\n",
        "    \n",
        "    total_segments = sum(frequency.values())\n",
        "    \n",
        "    entropy = 0\n",
        "    for count in frequency.values():\n",
        "        if count > 0:\n",
        "            probability = count / total_segments\n",
        "            entropy -= probability * log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "\n",
        "def classic_spectral_test(bit_string):\n",
        "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
        "    dft = fft(bit_array)\n",
        "    n_half = len(bit_string) // 2 + 1\n",
        "    mod_dft = np.abs(dft[:n_half])\n",
        "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
        "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
        "    expected_peaks = 0.95 * n_half\n",
        "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
        "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
        "    return d\n",
        "\n",
        "def frequency_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    count_ones = bit_string.count('1')\n",
        "    count_zeros = bit_string.count('0')\n",
        "    \n",
        "    # The test statistic\n",
        "    s = (count_ones - count_zeros) / sqrt(n)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(s) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def runs_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    runs = 1  # Start with the first run\n",
        "    for i in range(1, n):\n",
        "        if bit_string[i] != bit_string[i - 1]:\n",
        "            runs += 1\n",
        "    \n",
        "    n0 = bit_string.count('0')\n",
        "    n1 = bit_string.count('1')\n",
        "    \n",
        "    # Expected number of runs\n",
        "    expected_runs = (2 * n0 * n1 / n) + 1\n",
        "    variance_runs = (2 * n0 * n1 * (2 * n0 * n1 - n)) / (n ** 2 * (n - 1))\n",
        "    \n",
        "    # The test statistic\n",
        "    z = (runs - expected_runs) / sqrt(variance_runs)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(z) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def linear_complexity(bit_string, M=500):\n",
        "    # Perform linear complexity test with block size M\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    lc = 0  # Initialize linear complexity\n",
        "    \n",
        "    # Process blocks of size M\n",
        "    for i in range(0, n, M):\n",
        "        block = bit_array[i:i+M]\n",
        "        if len(block) < M:\n",
        "            continue\n",
        "        \n",
        "        lc_block = 0\n",
        "        for j in range(M):\n",
        "            if block[j] == 1:\n",
        "                lc_block = j + 1\n",
        "        \n",
        "        lc += lc_block\n",
        "    \n",
        "    lc = lc / (n / M)\n",
        "    return lc\n",
        "\n",
        "def autocorrelation_test(bit_string, lag=1):\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    autocorrelation = np.correlate(bit_array, np.roll(bit_array, lag), mode='valid')[0]\n",
        "    return autocorrelation / n\n",
        "\n",
        "def maurer_universal_test(bit_string):\n",
        "    k = 6\n",
        "    l = 5\n",
        "    q = 20\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    max_val = 2**k\n",
        "    init_subseq = bit_array[:q]\n",
        "    rest_subseq = bit_array[q:]\n",
        "    d = {}\n",
        "    for i in range(len(init_subseq) - k + 1):\n",
        "        d[tuple(init_subseq[i:i+k])] = i\n",
        "    t = []\n",
        "    for i in range(len(rest_subseq) - k + 1):\n",
        "        subseq = tuple(rest_subseq[i:i+k])\n",
        "        if subseq in d:\n",
        "            t.append(i - d[subseq])\n",
        "            d[subseq] = i\n",
        "    if not t:\n",
        "        return 0\n",
        "    t = np.array(t)\n",
        "    log_avg = np.mean(np.log2(t))\n",
        "    return log_avg - np.log2(q)\n",
        "\n",
        "def binary_matrix_rank_test(bit_string, M=32, Q=32):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_matrices = len(bit_array) // (M * Q)\n",
        "    ranks = []\n",
        "    for i in range(num_matrices):\n",
        "        matrix = bit_array[i*M*Q:(i+1)*M*Q].reshape((M, Q))\n",
        "        rank = np.linalg.matrix_rank(matrix)\n",
        "        ranks.append(rank)\n",
        "    return np.mean(ranks)\n",
        "\n",
        "def cumulative_sums_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    adjusted = 2 * bit_array - 1\n",
        "    cumulative_sum = np.cumsum(adjusted)\n",
        "    max_excursion = np.max(np.abs(cumulative_sum))\n",
        "    return max_excursion\n",
        "\n",
        "def longest_run_ones_test(bit_string, block_size=100):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_blocks = len(bit_array) // block_size\n",
        "    max_runs = []\n",
        "    for i in range(num_blocks):\n",
        "        block = bit_array[i*block_size:(i+1)*block_size]\n",
        "        max_run = max([len(list(g)) for k, g in itertools.groupby(block) if k == 1])\n",
        "        max_runs.append(max_run)\n",
        "    return np.mean(max_runs)\n",
        "\n",
        "def random_excursions_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    bit_array = 2 * bit_array - 1  # Convert to ±1\n",
        "\n",
        "    cumulative_sum = np.cumsum(bit_array)\n",
        "    states = np.unique(cumulative_sum)\n",
        "\n",
        "    if 0 not in states:\n",
        "        states = np.append(states, 0)\n",
        "    state_counts = {state: 0 for state in states}\n",
        "    for state in cumulative_sum:\n",
        "        state_counts[state] += 1\n",
        "\n",
        "    state_counts[0] -= 1  # Adjust for zero state\n",
        "    pi = [0.5 * (1 - (1 / (2 * state + 1)**2)) for state in states]\n",
        "    x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def unique_subsequences(bit_string, length=4):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    subsequences = set()\n",
        "    \n",
        "    for i in range(n - length + 1):\n",
        "        subseq = tuple(bit_array[i:i+length])\n",
        "        subsequences.add(subseq)\n",
        "    \n",
        "    return len(subsequences)\n",
        "\n",
        "def sample_entropy(bit_string, m=2, r=0.2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= r, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(C) / (N - m + 1.0)\n",
        "    \n",
        "    return -np.log(_phi(m + 1) / _phi(m))\n",
        "\n",
        "def permutation_entropy(bit_string, order=3):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    \n",
        "    permutations = np.array(list(itertools.permutations(range(order))))\n",
        "    c = np.zeros(len(permutations))\n",
        "    \n",
        "    for i in range(n - order + 1):\n",
        "        sorted_index_array = tuple(np.argsort(bit_array[i:i+order]))\n",
        "        for j, p in enumerate(permutations):\n",
        "            if np.array_equal(p, sorted_index_array):\n",
        "                c[j] += 1\n",
        "    \n",
        "    c = c / (n - order + 1)\n",
        "    pe = -np.sum(c * np.log2(c + np.finfo(float).eps))\n",
        "    return pe\n",
        "\n",
        "def lyapunov_exponent(bit_string, m=2, t=1):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= t, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(np.log(C + np.finfo(float).eps)) / (N - m + 1.0)\n",
        "    \n",
        "    return abs(_phi(m) - _phi(m + 1))\n",
        "\n",
        "def entropy_rate(bit_string, k=2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    prob = {}\n",
        "    \n",
        "    for i in range(n - k + 1):\n",
        "        subseq = tuple(bit_array[i:i + k])\n",
        "        if subseq in prob:\n",
        "            prob[subseq] += 1\n",
        "        else:\n",
        "            prob[subseq] = 1\n",
        "    \n",
        "    for key in prob:\n",
        "        prob[key] /= (n - k + 1)\n",
        "    \n",
        "    entropy_rate = -sum(p * log2(p) for p in prob.values())\n",
        "    return entropy_rate\n",
        "\n",
        "# Apply randomness tests\n",
        "def apply_randomness_tests(df, tests):\n",
        "    if not tests:\n",
        "        raise ValueError(\"No randomness tests specified.\")\n",
        "\n",
        "    test_functions = {\n",
        "        'shannon_entropy': shannon_entropy,\n",
        "        'classic_spectral_test': classic_spectral_test,\n",
        "        'frequency_test': frequency_test,\n",
        "        'runs_test': runs_test,\n",
        "        'linear_complexity': linear_complexity,\n",
        "        'autocorrelation_test': autocorrelation_test,\n",
        "        'maurer_universal_test': maurer_universal_test,\n",
        "        'binary_matrix_rank_test': binary_matrix_rank_test,\n",
        "        'cumulative_sums_test': cumulative_sums_test,\n",
        "        'longest_run_ones_test': longest_run_ones_test,\n",
        "        'random_excursions_test': random_excursions_test,\n",
        "        'unique_subsequences': unique_subsequences,\n",
        "        'sample_entropy': sample_entropy,\n",
        "        'permutation_entropy': permutation_entropy,\n",
        "        'lyapunov_exponent': lyapunov_exponent,\n",
        "        'entropy_rate': entropy_rate,\n",
        "        'min_entropy': calculate_min_entropy\n",
        "    }\n",
        "\n",
        "    for test in tests:\n",
        "        if test not in test_functions:\n",
        "            raise ValueError(f\"Invalid randomness test: {test}\")\n",
        "        df[test] = df['Concatenated_Data'].apply(test_functions[test])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(df, num_concats, tests):\n",
        "    df = concatenateData(df, num_concats)\n",
        "    processed_df = apply_randomness_tests(df, tests)\n",
        "    \n",
        "    # Convert concatenated binary strings into separate columns\n",
        "    df_features = pd.DataFrame(processed_df['Concatenated_Data'].apply(list).tolist(), dtype=float)\n",
        "    processed_df = pd.concat([processed_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Calculate min-entropy\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "# Main\n",
        "file_path = 'AI_2qubits_training_data.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "tests_to_apply = [\n",
        "    'shannon_entropy', 'classic_spectral_test', 'frequency_test', 'runs_test',\n",
        "    'linear_complexity', 'autocorrelation_test', 'maurer_universal_test', \n",
        "    'binary_matrix_rank_test', 'cumulative_sums_test', 'longest_run_ones_test', \n",
        "    'random_excursions_test', 'unique_subsequences', 'sample_entropy', \n",
        "    'permutation_entropy', 'lyapunov_exponent', 'entropy_rate', 'min_entropy'\n",
        "]\n",
        "\n",
        "# Preprocess data and apply randomness tests\n",
        "preprocessed_df = preprocess_data(df, num_concats=1, tests=tests_to_apply)\n",
        "# Split the data into features (X) and labels (y)\n",
        "\n",
        "print(preprocessed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>shannon_entropy</th>\n",
              "      <th>classic_spectral_test</th>\n",
              "      <th>frequency_test</th>\n",
              "      <th>runs_test</th>\n",
              "      <th>linear_complexity</th>\n",
              "      <th>autocorrelation_test</th>\n",
              "      <th>maurer_universal_test</th>\n",
              "      <th>binary_matrix_rank_test</th>\n",
              "      <th>cumulative_sums_test</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.935451</td>\n",
              "      <td>-21.771553</td>\n",
              "      <td>0.423711</td>\n",
              "      <td>0.120217</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.028726</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.963615</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.841481</td>\n",
              "      <td>0.027240</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.939471</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.109599</td>\n",
              "      <td>0.498506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>-inf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.872164</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.071861</td>\n",
              "      <td>0.620874</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.976281</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.230139</td>\n",
              "      <td>0.725698</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13995</th>\n",
              "      <td>4</td>\n",
              "      <td>1.942653</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.689157</td>\n",
              "      <td>0.556584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13996</th>\n",
              "      <td>4</td>\n",
              "      <td>1.919479</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.689157</td>\n",
              "      <td>0.987149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.014534</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13997</th>\n",
              "      <td>4</td>\n",
              "      <td>1.862236</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.317311</td>\n",
              "      <td>0.011137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.36</td>\n",
              "      <td>-0.127187</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13998</th>\n",
              "      <td>4</td>\n",
              "      <td>1.856367</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.841481</td>\n",
              "      <td>0.548989</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13999</th>\n",
              "      <td>4</td>\n",
              "      <td>1.717977</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.071861</td>\n",
              "      <td>0.051254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14000 rows × 118 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  shannon_entropy  classic_spectral_test  frequency_test  \\\n",
              "0          1         1.935451             -21.771553        0.423711   \n",
              "1          1         1.963615             -22.230385        0.841481   \n",
              "2          1         1.939471             -22.230385        0.109599   \n",
              "3          1         1.872164             -22.230385        0.071861   \n",
              "4          1         1.976281             -22.230385        0.230139   \n",
              "...      ...              ...                    ...             ...   \n",
              "13995      4         1.942653             -22.230385        0.689157   \n",
              "13996      4         1.919479             -22.230385        0.689157   \n",
              "13997      4         1.862236             -22.230385        0.317311   \n",
              "13998      4         1.856367             -22.230385        0.841481   \n",
              "13999      4         1.717977             -22.230385        0.071861   \n",
              "\n",
              "       runs_test  linear_complexity  autocorrelation_test  \\\n",
              "0       0.120217                0.0                  0.33   \n",
              "1       0.027240                0.0                  0.31   \n",
              "2       0.498506                0.0                  0.32   \n",
              "3       0.620874                0.0                  0.36   \n",
              "4       0.725698                0.0                  0.18   \n",
              "...          ...                ...                   ...   \n",
              "13995   0.556584                0.0                  0.24   \n",
              "13996   0.987149                0.0                  0.23   \n",
              "13997   0.011137                0.0                  0.36   \n",
              "13998   0.548989                0.0                  0.25   \n",
              "13999   0.051254                0.0                  0.21   \n",
              "\n",
              "       maurer_universal_test  binary_matrix_rank_test  cumulative_sums_test  \\\n",
              "0                   0.028726                      NaN                    10   \n",
              "1                        NaN                      NaN                     8   \n",
              "2                       -inf                      NaN                    21   \n",
              "3                        NaN                      NaN                    18   \n",
              "4                        NaN                      NaN                    18   \n",
              "...                      ...                      ...                   ...   \n",
              "13995                    NaN                      NaN                    10   \n",
              "13996              -0.014534                      NaN                     6   \n",
              "13997              -0.127187                      NaN                    15   \n",
              "13998                    NaN                      NaN                     8   \n",
              "13999                    NaN                      NaN                    24   \n",
              "\n",
              "       ...   90   91   92   93   94   95   96   97   98   99  \n",
              "0      ...  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
              "1      ...  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  \n",
              "2      ...  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
              "3      ...  1.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  \n",
              "4      ...  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  \n",
              "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
              "13995  ...  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0  \n",
              "13996  ...  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  \n",
              "13997  ...  0.0  0.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  \n",
              "13998  ...  1.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  \n",
              "13999  ...  0.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  \n",
              "\n",
              "[14000 rows x 118 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.fftpack import fft\n",
        "from collections import Counter\n",
        "from itertools import groupby\n",
        "\n",
        "# Count the number of 0s and 1s\n",
        "def count_bits(sequence):\n",
        "    return Counter(sequence)\n",
        "\n",
        "# Count the number of transitions from 0 to 1 and from 1 to 0\n",
        "def count_transitions(sequence):\n",
        "    return sum(sequence[i-1] != sequence[i] for i in range(1, len(sequence)))\n",
        "\n",
        "# Calculate the lengths of runs of consecutive 0s or 1s\n",
        "def run_lengths(sequence):\n",
        "    return [len(list(group)) for key, group in groupby(sequence)]\n",
        "\n",
        "# Measure the entropy of the bit sequence\n",
        "def entropy(sequence):\n",
        "    value,counts = np.unique(list(sequence), return_counts=True)\n",
        "    return -np.sum((counts/len(sequence)) * np.log2(counts/len(sequence)))\n",
        "\n",
        "# Perform a Fourier transform and use the power spectrum as features\n",
        "def spectral_analysis(sequence):\n",
        "    transform = fft([int(bit) for bit in sequence])\n",
        "    power_spectrum = np.abs(transform)**2\n",
        "    return power_spectrum\n",
        "\n",
        "# Compute the autocorrelation of the bit sequence\n",
        "def autocorrelation(sequence):\n",
        "    sequence = np.array([int(bit) for bit in sequence])\n",
        "    result = np.correlate(sequence, sequence, mode='full')\n",
        "    return result[result.size // 2:]\n",
        "\n",
        "# Count the occurrences of each possible n-gram\n",
        "def ngrams(sequence, n=2):\n",
        "    return Counter(sequence[i:i+n] for i in range(len(sequence) - n + 1))\n",
        "\n",
        "# Look for cyclic patterns in the bit sequence\n",
        "def cyclic_patterns(sequence):\n",
        "    # This is a complex task that may require specific domain knowledge\n",
        "    # Placeholder function\n",
        "    pass\n",
        "\n",
        "# Find the longest run of 0s and 1s\n",
        "def longest_run(sequence):\n",
        "    return max(len(list(group)) for key, group in groupby(sequence))\n",
        "\n",
        "# Calculate the rate at which bits flip from 0 to 1 or vice versa\n",
        "def bit_flipping_rate(sequence):\n",
        "    return count_transitions(sequence) / len(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.special import gammaincc\n",
        "\n",
        "def serial_test(bit_string, m=2):\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    counts = np.zeros(2**m)\n",
        "    for i in range(n):\n",
        "        counts[int(bit_string[i:i+m], 2)] += 1\n",
        "    counts /= n\n",
        "    psim = sum(counts**2) * 2**m - 1\n",
        "    del1 = psim - (2**(m-1))\n",
        "    del2 = psim - (2**(m-2)) if m > 1 else 0\n",
        "    p_value1 = gammaincc(2**(m-2), del1 / 2)\n",
        "    p_value2 = gammaincc(2**(m-3), del2 / 2) if m > 1 else 0\n",
        "    return p_value1, p_value2\n",
        "\n",
        "def poker_test(bit_string, m=4):\n",
        "    n = len(bit_string)\n",
        "    k = n // m\n",
        "    counts = np.zeros(2**m)\n",
        "    for i in range(k):\n",
        "        counts[int(bit_string[i*m:(i+1)*m], 2)] += 1\n",
        "    counts /= k\n",
        "    x = 2**m / k * sum(counts**2) - k\n",
        "    p_value = gammaincc(2**(m-2), x / 2)\n",
        "    return p_value\n",
        "\n",
        "def runs_above_below_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    pi = bit_string.count('1') / n\n",
        "    tau = 2 / sqrt(n)\n",
        "    if abs(pi - 0.5) >= tau:\n",
        "        return 0.0\n",
        "    else:\n",
        "        bit_array = np.array([int(bit) for bit in bit_string])\n",
        "        mean = np.mean(bit_array)\n",
        "        diff = bit_array - mean\n",
        "        runs = 1\n",
        "        for i in range(1, n):\n",
        "            if diff[i] * diff[i-1] < 0:\n",
        "                runs += 1\n",
        "        p_value = erfc(abs(runs - 2*n*pi*(1-pi)) / (2*sqrt(2*n)*pi*(1-pi)))\n",
        "        return p_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_df=preprocessed_df.drop(\"binary_matrix_rank_test\",axis=1)\n",
        "preprocessed_df=preprocessed_df.drop(\"linear_complexity\",axis=1)\n",
        "\n",
        "# Apply the feature extraction methods to the binary_number column\n",
        "# Apply the serial_test function to the binary_number column\n",
        "df[['serial_test_p1', 'serial_test_p2']] = df['binary_number'].apply(lambda x: pd.Series(serial_test(x)))\n",
        "df['poker_test'] = df['binary_number'].apply(lambda x: poker_test(x))\n",
        "df['runs_above_below_test'] = df['binary_number'].apply(lambda x: runs_above_below_test(x))\n",
        "df['count_0'] = df['binary_number'].apply(lambda x: count_bits(x)['0'])\n",
        "df['count_1'] = df['binary_number'].apply(lambda x: count_bits(x)['1'])\n",
        "df['transitions'] = df['binary_number'].apply(count_transitions)\n",
        "df['run_lengths_0'] = df['binary_number'].apply(lambda x: run_lengths(x.replace('1', ' ')))\n",
        "df['run_lengths_1'] = df['binary_number'].apply(lambda x: run_lengths(x.replace('0', ' ')))\n",
        "df['entropy'] = df['binary_number'].apply(entropy)\n",
        "df['spectral_analysis'] = df['binary_number'].apply(lambda x: np.mean(spectral_analysis(x)))\n",
        "df['autocorrelation'] = df['binary_number'].apply(lambda x: np.mean(autocorrelation(x)))\n",
        "df['ngrams'] = df['binary_number'].apply(lambda x: ngrams(x, 2)[x[:2]])  # Using 2-grams as an example\n",
        "df['longest_run_0'] = df['binary_number'].apply(lambda x: max(run_lengths(x.replace('1', ' '))))\n",
        "df['longest_run_1'] = df['binary_number'].apply(lambda x: max(run_lengths(x.replace('0', ' '))))\n",
        "df['bit_flipping_rate'] = df['binary_number'].apply(bit_flipping_rate)\n",
        "df\n",
        "# Get the current column names\n",
        "current_columns = df.columns\n",
        "\n",
        "# Identify the columns to drop\n",
        "columns_to_drop = [col for col in current_columns if col.startswith('run_length_0_') or col.startswith('run_length_1_')]\n",
        "\n",
        "# Drop the identified columns\n",
        "df = df.drop(columns=columns_to_drop)\n",
        "# Add columns for the lengths of the first 5 runs of 0 and 1\n",
        "for i in range(10):\n",
        "    df[f'run_length_0_{i+1}'] = df['run_lengths_0'].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
        "    df[f'run_length_1_{i+1}'] = df['run_lengths_1'].apply(lambda x: x[i] if i < len(x) else np.nan)\n",
        "df['mean_run_length_0'] = df['run_lengths_0'].apply(np.mean)\n",
        "df['max_run_length_0'] = df['run_lengths_0'].apply(max)\n",
        "df['min_run_length_0'] = df['run_lengths_0'].apply(min)\n",
        "df['std_run_length_0'] = df['run_lengths_0'].apply(np.std)\n",
        "df['mean_run_length_1'] = df['run_lengths_1'].apply(np.mean)\n",
        "df['max_run_length_1'] = df['run_lengths_1'].apply(max)\n",
        "df['min_run_length_1'] = df['run_lengths_1'].apply(min)\n",
        "df['std_run_length_1'] = df['run_lengths_1'].apply(np.std)\n",
        "df = df[df['label'] != 1]\n",
        "\n",
        "df=df.drop(\"run_lengths_0\",axis=1)\n",
        "df=df.drop(\"run_lengths_1\",axis=1)\n",
        "df=df.drop(\"binary_number\",axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(\"label\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>serial_test_p1</th>\n",
              "      <th>serial_test_p2</th>\n",
              "      <th>poker_test</th>\n",
              "      <th>runs_above_below_test</th>\n",
              "      <th>count_0</th>\n",
              "      <th>count_1</th>\n",
              "      <th>transitions</th>\n",
              "      <th>entropy</th>\n",
              "      <th>spectral_analysis</th>\n",
              "      <th>autocorrelation</th>\n",
              "      <th>...</th>\n",
              "      <th>run_length_0_10</th>\n",
              "      <th>run_length_1_10</th>\n",
              "      <th>mean_run_length_0</th>\n",
              "      <th>max_run_length_0</th>\n",
              "      <th>min_run_length_0</th>\n",
              "      <th>std_run_length_0</th>\n",
              "      <th>mean_run_length_1</th>\n",
              "      <th>max_run_length_1</th>\n",
              "      <th>min_run_length_1</th>\n",
              "      <th>std_run_length_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.226743</td>\n",
              "      <td>35</td>\n",
              "      <td>65</td>\n",
              "      <td>39</td>\n",
              "      <td>0.934068</td>\n",
              "      <td>65.0</td>\n",
              "      <td>21.45</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1.949359</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1.949359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.505677</td>\n",
              "      <td>44</td>\n",
              "      <td>56</td>\n",
              "      <td>45</td>\n",
              "      <td>0.989588</td>\n",
              "      <td>56.0</td>\n",
              "      <td>15.96</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.166644</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1.166644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.339922</td>\n",
              "      <td>33</td>\n",
              "      <td>67</td>\n",
              "      <td>39</td>\n",
              "      <td>0.914926</td>\n",
              "      <td>67.0</td>\n",
              "      <td>22.78</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>2.291288</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>2.291288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.739835</td>\n",
              "      <td>39</td>\n",
              "      <td>61</td>\n",
              "      <td>45</td>\n",
              "      <td>0.964800</td>\n",
              "      <td>61.0</td>\n",
              "      <td>18.91</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.632607</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.632607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.297566</td>\n",
              "      <td>60</td>\n",
              "      <td>40</td>\n",
              "      <td>42</td>\n",
              "      <td>0.970951</td>\n",
              "      <td>40.0</td>\n",
              "      <td>8.20</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.325581</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2.248730</td>\n",
              "      <td>2.325581</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2.248730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13995</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.700522</td>\n",
              "      <td>52</td>\n",
              "      <td>48</td>\n",
              "      <td>47</td>\n",
              "      <td>0.998846</td>\n",
              "      <td>48.0</td>\n",
              "      <td>11.76</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.238839</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.238839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13996</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.828718</td>\n",
              "      <td>52</td>\n",
              "      <td>48</td>\n",
              "      <td>50</td>\n",
              "      <td>0.998846</td>\n",
              "      <td>48.0</td>\n",
              "      <td>11.76</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.960784</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1.468102</td>\n",
              "      <td>1.960784</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1.468102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13997</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.020167</td>\n",
              "      <td>45</td>\n",
              "      <td>55</td>\n",
              "      <td>37</td>\n",
              "      <td>0.992774</td>\n",
              "      <td>55.0</td>\n",
              "      <td>15.40</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2.631579</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1.494218</td>\n",
              "      <td>2.631579</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1.494218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13998</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.691988</td>\n",
              "      <td>51</td>\n",
              "      <td>49</td>\n",
              "      <td>47</td>\n",
              "      <td>0.999711</td>\n",
              "      <td>49.0</td>\n",
              "      <td>12.25</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.204736</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.204736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13999</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.083252</td>\n",
              "      <td>59</td>\n",
              "      <td>41</td>\n",
              "      <td>39</td>\n",
              "      <td>0.976500</td>\n",
              "      <td>41.0</td>\n",
              "      <td>8.61</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.843909</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1.843909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12000 rows × 42 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       serial_test_p1  serial_test_p2  poker_test  runs_above_below_test  \\\n",
              "2000              NaN             NaN         NaN               0.226743   \n",
              "2001              NaN             NaN         NaN               0.505677   \n",
              "2002              NaN             NaN         NaN               0.339922   \n",
              "2003              NaN             NaN         NaN               0.739835   \n",
              "2004              NaN             NaN         NaN               0.297566   \n",
              "...               ...             ...         ...                    ...   \n",
              "13995             NaN             NaN         NaN               0.700522   \n",
              "13996             NaN             NaN         NaN               0.828718   \n",
              "13997             NaN             NaN         NaN               0.020167   \n",
              "13998             NaN             NaN         NaN               0.691988   \n",
              "13999             NaN             NaN         NaN               0.083252   \n",
              "\n",
              "       count_0  count_1  transitions   entropy  spectral_analysis  \\\n",
              "2000        35       65           39  0.934068               65.0   \n",
              "2001        44       56           45  0.989588               56.0   \n",
              "2002        33       67           39  0.914926               67.0   \n",
              "2003        39       61           45  0.964800               61.0   \n",
              "2004        60       40           42  0.970951               40.0   \n",
              "...        ...      ...          ...       ...                ...   \n",
              "13995       52       48           47  0.998846               48.0   \n",
              "13996       52       48           50  0.998846               48.0   \n",
              "13997       45       55           37  0.992774               55.0   \n",
              "13998       51       49           47  0.999711               49.0   \n",
              "13999       59       41           39  0.976500               41.0   \n",
              "\n",
              "       autocorrelation  ...  run_length_0_10  run_length_1_10  \\\n",
              "2000             21.45  ...                1                1   \n",
              "2001             15.96  ...                2                2   \n",
              "2002             22.78  ...                1                1   \n",
              "2003             18.91  ...                3                3   \n",
              "2004              8.20  ...                1                1   \n",
              "...                ...  ...              ...              ...   \n",
              "13995            11.76  ...                1                1   \n",
              "13996            11.76  ...                2                2   \n",
              "13997            15.40  ...                3                3   \n",
              "13998            12.25  ...                2                2   \n",
              "13999             8.61  ...                2                2   \n",
              "\n",
              "       mean_run_length_0  max_run_length_0  min_run_length_0  \\\n",
              "2000            2.500000                11                 1   \n",
              "2001            2.173913                 5                 1   \n",
              "2002            2.500000                12                 1   \n",
              "2003            2.173913                 8                 1   \n",
              "2004            2.325581                11                 1   \n",
              "...                  ...               ...               ...   \n",
              "13995           2.083333                 6                 1   \n",
              "13996           1.960784                 7                 1   \n",
              "13997           2.631579                 7                 1   \n",
              "13998           2.083333                 6                 1   \n",
              "13999           2.500000                 8                 1   \n",
              "\n",
              "       std_run_length_0  mean_run_length_1  max_run_length_1  \\\n",
              "2000           1.949359           2.500000                11   \n",
              "2001           1.166644           2.173913                 5   \n",
              "2002           2.291288           2.500000                12   \n",
              "2003           1.632607           2.173913                 8   \n",
              "2004           2.248730           2.325581                11   \n",
              "...                 ...                ...               ...   \n",
              "13995          1.238839           2.083333                 6   \n",
              "13996          1.468102           1.960784                 7   \n",
              "13997          1.494218           2.631579                 7   \n",
              "13998          1.204736           2.083333                 6   \n",
              "13999          1.843909           2.500000                 8   \n",
              "\n",
              "       min_run_length_1  std_run_length_1  \n",
              "2000                  1          1.949359  \n",
              "2001                  1          1.166644  \n",
              "2002                  1          2.291288  \n",
              "2003                  1          1.632607  \n",
              "2004                  1          2.248730  \n",
              "...                 ...               ...  \n",
              "13995                 1          1.238839  \n",
              "13996                 1          1.468102  \n",
              "13997                 1          1.494218  \n",
              "13998                 1          1.204736  \n",
              "13999                 1          1.843909  \n",
              "\n",
              "[12000 rows x 42 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_df=preprocessed_df.join(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>shannon_entropy</th>\n",
              "      <th>classic_spectral_test</th>\n",
              "      <th>frequency_test</th>\n",
              "      <th>runs_test</th>\n",
              "      <th>autocorrelation_test</th>\n",
              "      <th>cumulative_sums_test</th>\n",
              "      <th>longest_run_ones_test</th>\n",
              "      <th>random_excursions_test</th>\n",
              "      <th>unique_subsequences</th>\n",
              "      <th>...</th>\n",
              "      <th>run_length_0_10</th>\n",
              "      <th>run_length_1_10</th>\n",
              "      <th>mean_run_length_0</th>\n",
              "      <th>max_run_length_0</th>\n",
              "      <th>min_run_length_0</th>\n",
              "      <th>std_run_length_0</th>\n",
              "      <th>mean_run_length_1</th>\n",
              "      <th>max_run_length_1</th>\n",
              "      <th>min_run_length_1</th>\n",
              "      <th>std_run_length_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>2</td>\n",
              "      <td>1.857699</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.150635</td>\n",
              "      <td>0.45</td>\n",
              "      <td>30</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.949359</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.949359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2001</th>\n",
              "      <td>2</td>\n",
              "      <td>1.953521</td>\n",
              "      <td>-21.771553</td>\n",
              "      <td>0.230139</td>\n",
              "      <td>0.382632</td>\n",
              "      <td>0.33</td>\n",
              "      <td>15</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.166644</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.166644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2002</th>\n",
              "      <td>2</td>\n",
              "      <td>1.791342</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.234812</td>\n",
              "      <td>0.47</td>\n",
              "      <td>36</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.291288</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.291288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003</th>\n",
              "      <td>2</td>\n",
              "      <td>1.881277</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.027807</td>\n",
              "      <td>0.585556</td>\n",
              "      <td>0.38</td>\n",
              "      <td>23</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.632607</td>\n",
              "      <td>2.173913</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.632607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2004</th>\n",
              "      <td>2</td>\n",
              "      <td>1.837127</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.045500</td>\n",
              "      <td>0.208791</td>\n",
              "      <td>0.19</td>\n",
              "      <td>23</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.325581</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.248730</td>\n",
              "      <td>2.325581</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.248730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13995</th>\n",
              "      <td>4</td>\n",
              "      <td>1.942653</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.689157</td>\n",
              "      <td>0.556584</td>\n",
              "      <td>0.24</td>\n",
              "      <td>10</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.238839</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.238839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13996</th>\n",
              "      <td>4</td>\n",
              "      <td>1.919479</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.689157</td>\n",
              "      <td>0.987149</td>\n",
              "      <td>0.23</td>\n",
              "      <td>6</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.960784</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.468102</td>\n",
              "      <td>1.960784</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.468102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13997</th>\n",
              "      <td>4</td>\n",
              "      <td>1.862236</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.317311</td>\n",
              "      <td>0.011137</td>\n",
              "      <td>0.36</td>\n",
              "      <td>15</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.631579</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.494218</td>\n",
              "      <td>2.631579</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.494218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13998</th>\n",
              "      <td>4</td>\n",
              "      <td>1.856367</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.841481</td>\n",
              "      <td>0.548989</td>\n",
              "      <td>0.25</td>\n",
              "      <td>8</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.204736</td>\n",
              "      <td>2.083333</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.204736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13999</th>\n",
              "      <td>4</td>\n",
              "      <td>1.717977</td>\n",
              "      <td>-22.230385</td>\n",
              "      <td>0.071861</td>\n",
              "      <td>0.051254</td>\n",
              "      <td>0.21</td>\n",
              "      <td>24</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.843909</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.843909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12000 rows × 157 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  shannon_entropy  classic_spectral_test  frequency_test  \\\n",
              "2000       2         1.857699             -22.230385        0.002700   \n",
              "2001       2         1.953521             -21.771553        0.230139   \n",
              "2002       2         1.791342             -22.230385        0.000674   \n",
              "2003       2         1.881277             -22.230385        0.027807   \n",
              "2004       2         1.837127             -22.230385        0.045500   \n",
              "...      ...              ...                    ...             ...   \n",
              "13995      4         1.942653             -22.230385        0.689157   \n",
              "13996      4         1.919479             -22.230385        0.689157   \n",
              "13997      4         1.862236             -22.230385        0.317311   \n",
              "13998      4         1.856367             -22.230385        0.841481   \n",
              "13999      4         1.717977             -22.230385        0.071861   \n",
              "\n",
              "       runs_test  autocorrelation_test  cumulative_sums_test  \\\n",
              "2000    0.150635                  0.45                    30   \n",
              "2001    0.382632                  0.33                    15   \n",
              "2002    0.234812                  0.47                    36   \n",
              "2003    0.585556                  0.38                    23   \n",
              "2004    0.208791                  0.19                    23   \n",
              "...          ...                   ...                   ...   \n",
              "13995   0.556584                  0.24                    10   \n",
              "13996   0.987149                  0.23                     6   \n",
              "13997   0.011137                  0.36                    15   \n",
              "13998   0.548989                  0.25                     8   \n",
              "13999   0.051254                  0.21                    24   \n",
              "\n",
              "       longest_run_ones_test  random_excursions_test  unique_subsequences  \\\n",
              "2000                    11.0                     NaN                   16   \n",
              "2001                     5.0                     NaN                   15   \n",
              "2002                    12.0                     NaN                   16   \n",
              "2003                     8.0                     NaN                   16   \n",
              "2004                     5.0                     NaN                   16   \n",
              "...                      ...                     ...                  ...   \n",
              "13995                    4.0                     NaN                   16   \n",
              "13996                    7.0                     NaN                   16   \n",
              "13997                    7.0                     NaN                   15   \n",
              "13998                    5.0                     NaN                   16   \n",
              "13999                    6.0                     NaN                   16   \n",
              "\n",
              "       ...  run_length_0_10  run_length_1_10  mean_run_length_0  \\\n",
              "2000   ...              1.0              1.0           2.500000   \n",
              "2001   ...              2.0              2.0           2.173913   \n",
              "2002   ...              1.0              1.0           2.500000   \n",
              "2003   ...              3.0              3.0           2.173913   \n",
              "2004   ...              1.0              1.0           2.325581   \n",
              "...    ...              ...              ...                ...   \n",
              "13995  ...              1.0              1.0           2.083333   \n",
              "13996  ...              2.0              2.0           1.960784   \n",
              "13997  ...              3.0              3.0           2.631579   \n",
              "13998  ...              2.0              2.0           2.083333   \n",
              "13999  ...              2.0              2.0           2.500000   \n",
              "\n",
              "       max_run_length_0  min_run_length_0  std_run_length_0  \\\n",
              "2000               11.0               1.0          1.949359   \n",
              "2001                5.0               1.0          1.166644   \n",
              "2002               12.0               1.0          2.291288   \n",
              "2003                8.0               1.0          1.632607   \n",
              "2004               11.0               1.0          2.248730   \n",
              "...                 ...               ...               ...   \n",
              "13995               6.0               1.0          1.238839   \n",
              "13996               7.0               1.0          1.468102   \n",
              "13997               7.0               1.0          1.494218   \n",
              "13998               6.0               1.0          1.204736   \n",
              "13999               8.0               1.0          1.843909   \n",
              "\n",
              "       mean_run_length_1  max_run_length_1  min_run_length_1  std_run_length_1  \n",
              "2000            2.500000              11.0               1.0          1.949359  \n",
              "2001            2.173913               5.0               1.0          1.166644  \n",
              "2002            2.500000              12.0               1.0          2.291288  \n",
              "2003            2.173913               8.0               1.0          1.632607  \n",
              "2004            2.325581              11.0               1.0          2.248730  \n",
              "...                  ...               ...               ...               ...  \n",
              "13995           2.083333               6.0               1.0          1.238839  \n",
              "13996           1.960784               7.0               1.0          1.468102  \n",
              "13997           2.631579               7.0               1.0          1.494218  \n",
              "13998           2.083333               6.0               1.0          1.204736  \n",
              "13999           2.500000               8.0               1.0          1.843909  \n",
              "\n",
              "[12000 rows x 157 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_df = preprocessed_df[preprocessed_df['label'] != 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['label', 'shannon_entropy', 'classic_spectral_test', 'frequency_test', 'runs_test', 'autocorrelation_test', 'maurer_universal_test', 'cumulative_sums_test', 'longest_run_ones_test', 'random_excursions_test', 'unique_subsequences', 'sample_entropy', 'permutation_entropy', 'lyapunov_exponent', 'entropy_rate', 'min_entropy', 'serial_test_p1', 'serial_test_p2', 'poker_test', 'runs_above_below_test', 'count_0', 'count_1', 'transitions', 'entropy', 'spectral_analysis', 'autocorrelation', 'ngrams', 'longest_run_0', 'longest_run_1', 'bit_flipping_rate', 'run_length_0_1', 'run_length_1_1', 'run_length_0_2', 'run_length_1_2', 'run_length_0_3', 'run_length_1_3', 'run_length_0_4', 'run_length_1_4', 'run_length_0_5', 'run_length_1_5', 'run_length_0_6', 'run_length_1_6', 'run_length_0_7', 'run_length_1_7', 'run_length_0_8', 'run_length_1_8', 'run_length_0_9', 'run_length_1_9', 'run_length_0_10', 'run_length_1_10', 'mean_run_length_0', 'max_run_length_0', 'min_run_length_0', 'std_run_length_0', 'mean_run_length_1', 'max_run_length_1', 'min_run_length_1', 'std_run_length_1']\n"
          ]
        }
      ],
      "source": [
        "non_numeric_column_names = [col for col in preprocessed_df.columns if not str(col).isdigit()]\n",
        "print(non_numeric_column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of columns with null values: 5\n"
          ]
        }
      ],
      "source": [
        "null_columns = preprocessed_df.columns[preprocessed_df.isnull().any()]\n",
        "num_null_columns = len(null_columns)\n",
        "print(f\"Number of columns with null values: {num_null_columns}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['maurer_universal_test'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m null_rows \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnon_numeric_column_names\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(null_rows)\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['maurer_universal_test'] not in index\""
          ]
        }
      ],
      "source": [
        "null_rows = preprocessed_df[non_numeric_column_names].isnull().sum()\n",
        "print(null_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop column binary_matrix_rank_test\n",
        "\n",
        "# preprocessed_df = preprocessed_df.drop(columns=['binary_matrix_rank_test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop maurer_universal_test\n",
        "\n",
        "preprocessed_df = preprocessed_df.drop(columns=['maurer_universal_test'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random_excursions_test avrage value for null values\n",
        "\n",
        "preprocessed_df['random_excursions_test'] = preprocessed_df['random_excursions_test'].fillna(preprocessed_df['random_excursions_test'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Replace infinities with NaN\n",
        "# preprocessed_df = np.where(np.isinf(preprocessed_df), np.nan, preprocessed_df)\n",
        "\n",
        "# # Calculate the mean of each column, ignoring NaN values\n",
        "# col_means = np.nanmean(preprocessed_df, apreprocessed_dfis=0)\n",
        "\n",
        "# # Find indices in preprocessed_df where NaN values are present\n",
        "# inds = np.where(np.isnan(preprocessed_df))\n",
        "\n",
        "# # Replace NaNs with corresponding column mean\n",
        "# preprocessed_df[inds] = np.take(col_means, inds[1])\n",
        "\n",
        "preprocessed_df = preprocessed_df.replace([np.inf, -np.inf], np.nan)\n",
        "preprocessed_df = preprocessed_df.fillna(preprocessed_df.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "X=preprocessed_df.drop(columns='label').values\n",
        "y=preprocessed_df['label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12000, 156)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.725\n",
            "Precision: 0.7036748879017508\n",
            "F1 Score: 0.7016699516855223\n",
            "Confusion Matrix:\n",
            "[[ 162   33  207]\n",
            " [  36  150  240]\n",
            " [  69   75 1428]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.72875\n",
            "Precision: 0.711240331182139\n",
            "F1 Score: 0.7143627575379586\n",
            "Confusion Matrix:\n",
            "[[ 170   43  189]\n",
            " [  38  189  199]\n",
            " [  84   98 1390]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# APPLY WEIGHTS\n",
        "class_weights = { 2: 1, 3: 1, 4: 20}\n",
        "clf = RandomForestClassifier(class_weight=class_weights)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 features:  Index(['run_length_1_10', 'count_1', 'longest_run_1', 'spectral_analysis',\n",
            "       'entropy', 'runs_above_below_test', 'std_run_length_0', 'count_0',\n",
            "       'runs_test', 'autocorrelation', 'lyapunov_exponent',\n",
            "       'autocorrelation_test', 'poker_test', 'frequency_test',\n",
            "       'min_run_length_0', 'min_run_length_1', 'entropy_rate',\n",
            "       'unique_subsequences', 'sample_entropy', 'label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# # top 10 features \n",
        "\n",
        "# # Get feature importances\n",
        "# importances = clf.feature_importances_\n",
        "\n",
        "# # Get the indices of the top 10 features\n",
        "# indices = np.argsort(importances)[-20:]\n",
        "\n",
        "# # Get the names of the top 10 features\n",
        "# top_features = X_train.columns[indices]\n",
        "\n",
        "# # Print the top 10 features\n",
        "# print(\"Top 10 features: \", top_features)\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Get the indices of the top 10 features\n",
        "indices = np.argsort(importances)[-20:]\n",
        "\n",
        "# Get the names of the top 10 features\n",
        "top_features = preprocessed_df.columns[indices]\n",
        "\n",
        "# Print the top 10 features\n",
        "print(\"Top 10 features: \", top_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "try to do the approach of one vs all to try to identify the best model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0 appears 4000 times in y_2\n",
            "Value 4 appears 8000 times in y_2\n"
          ]
        }
      ],
      "source": [
        "X_2=preprocessed_df.drop(\"label\",axis=1)\n",
        "y_2=preprocessed_df[\"label\"]\n",
        "\n",
        "\n",
        "y_2 = np.where(y_2 != 4, 0, y_2)\n",
        "unique_values, counts = np.unique(y_2, return_counts=True)\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value {value} appears {count} times in y_2\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X is your feature set and y is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "# Replace infinities with NaN\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_train.fillna(X_train.mean(), inplace=True)\n",
        "\n",
        "# Replace infinities with NaN\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_test.fillna(X_test.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7608333333333334\n",
            "Precision: 0.7544003555787422\n",
            "F1 Score: 0.7542625490601116\n",
            "Confusion Matrix:\n",
            "[[ 465  363]\n",
            " [ 211 1361]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# APPLY WEIGHTS\n",
        "# class_weights = {1: 1, 2, 4: 20}\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 features:  Index(['run_length_0_2', 'run_length_1_6', 'run_length_0_9', 'run_length_0_3',\n",
            "       'run_length_0_7', 'max_run_length_0', 'max_run_length_1',\n",
            "       'run_length_0_4', 'run_length_0_8', 'run_length_1_9', 'run_length_1_8',\n",
            "       'longest_run_0', 'longest_run_ones_test', 'entropy', 'frequency_test',\n",
            "       'ngrams', 'autocorrelation_test', 'cumulative_sums_test', 'count_1',\n",
            "       'autocorrelation', 'min_entropy', 'count_0', 'entropy_rate',\n",
            "       'spectral_analysis', 'sample_entropy', 'std_run_length_1',\n",
            "       'std_run_length_0', 'bit_flipping_rate', 'mean_run_length_1',\n",
            "       'runs_above_below_test', 'transitions', 'runs_test',\n",
            "       'mean_run_length_0', 'permutation_entropy', 'shannon_entropy'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# top 10 features \n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Get the indices of the top 10 features\n",
        "indices = np.argsort(importances)[-35:]\n",
        "\n",
        "# Get the names of the top 10 features\n",
        "top_features = X_train.columns[indices]\n",
        "\n",
        "# Print the top 10 features\n",
        "print(\"Top 10 features: \", top_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7629166666666667\n",
            "Precision: 0.7569843284405694\n",
            "F1 Score: 0.7574754098360656\n",
            "Confusion Matrix:\n",
            "[[ 478  350]\n",
            " [ 219 1353]]\n"
          ]
        }
      ],
      "source": [
        "# Select only the top 10 features\n",
        "X_train_top = X_train[top_features]\n",
        "X_test_top = X_test[top_features]\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train_top, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_top = clf.predict(X_test_top)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_top = accuracy_score(y_test, y_pred_top)\n",
        "precision_top = precision_score(y_test, y_pred_top, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1_top = f1_score(y_test, y_pred_top, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix_top = confusion_matrix(y_test, y_pred_top)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy_top}\")\n",
        "print(f\"Precision: {precision_top}\")\n",
        "print(f\"F1 Score: {f1_top}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_top}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['random_excursions_test' 'serial_test_p1' 'serial_test_p2' 'poker_test']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['random_excursions_test' 'serial_test_p1' 'serial_test_p2' 'poker_test']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7741666666666667\n",
            "Precision: 0.7687035333087965\n",
            "F1 Score: 0.768357953887151\n",
            "Confusion Matrix:\n",
            "[[ 485  343]\n",
            " [ 199 1373]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Create a Gradient Boosting classifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create an imputer object that replaces NaN values with the mean value of the column\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Fit the imputer on the training data and transform it\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data with the same imputer\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Create a Gradient Boosting classifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100)\n",
        "\n",
        "# Train the model with the imputed data\n",
        "clf.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions with the imputed test data\n",
        "y_pred = clf.predict(X_test_imputed)\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['random_excursions_test' 'serial_test_p1' 'serial_test_p2' 'poker_test']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['random_excursions_test' 'serial_test_p1' 'serial_test_p2' 'poker_test']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7741666666666667\n",
            "Precision: 0.7687035333087965\n",
            "F1 Score: 0.768357953887151\n",
            "Confusion Matrix:\n",
            "[[ 485  343]\n",
            " [ 199 1373]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create a SVC classifier\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Fit the imputer on the training data and transform it\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data with the same imputer\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Create a Gradient Boosting classifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100)\n",
        "\n",
        "# Train the model with the imputed data\n",
        "clf.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Make predictions with the imputed test data\n",
        "y_pred = clf.predict(X_test_imputed)\n",
        "# Make predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# try another 2vs all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0 appears 10000 times in y_2\n",
            "Value 2 appears 2000 times in y_2\n"
          ]
        }
      ],
      "source": [
        "X_2=preprocessed_df.drop(\"label\",axis=1)\n",
        "y_2=preprocessed_df[\"label\"]\n",
        "\n",
        "\n",
        "y_2 = np.where(y_2 != 2, 0, y_2)\n",
        "unique_values, counts = np.unique(y_2, return_counts=True)\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value {value} appears {count} times in y_2\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X is your feature set and y is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "# Replace infinities with NaN\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_train.fillna(X_train.mean(), inplace=True)\n",
        "\n",
        "# Replace infinities with NaN\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_test.fillna(X_test.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8541666666666666\n",
            "Precision: 0.8332262399937216\n",
            "F1 Score: 0.8308128196115337\n",
            "Confusion Matrix:\n",
            "[[1932   66]\n",
            " [ 284  118]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# APPLY WEIGHTS\n",
        "# class_weights = {1: 1, 2, 4: 20}\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with three "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0 appears 10000 times in y_2\n",
            "Value 3 appears 2000 times in y_2\n"
          ]
        }
      ],
      "source": [
        "X_2=preprocessed_df.drop(\"label\",axis=1)\n",
        "y_2=preprocessed_df[\"label\"]\n",
        "\n",
        "\n",
        "y_2 = np.where(y_2 != 3, 0, y_2)\n",
        "unique_values, counts = np.unique(y_2, return_counts=True)\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value {value} appears {count} times in y_2\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_2=X_2.drop(\"random_excursions_test\",axis=1)\n",
        "\n",
        "# Assuming X is your feature set and y is your target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "# Replace infinities with NaN\n",
        "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_train.fillna(X_train.mean(), inplace=True)\n",
        "\n",
        "# Replace infinities with NaN\n",
        "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Replace NaN values with the mean of the column\n",
        "X_test.fillna(X_test.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8420833333333333\n",
            "Precision: 0.8195182160747122\n",
            "F1 Score: 0.8088492389364309\n",
            "Confusion Matrix:\n",
            "[[1924   50]\n",
            " [ 329   97]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "# APPLY WEIGHTS\n",
        "# class_weights = {1: 1, 2, 4: 20}\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['serial_test_p1' 'serial_test_p2' 'poker_test']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "found 0 physical cores < 1\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\moham\\anaconda32\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
            "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "X has 155 features, but RandomForestClassifier is expecting 152 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train_res, y_train_res)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m     26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:905\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    885\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:947\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    945\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    946\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    950\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: X has 155 features, but RandomForestClassifier is expecting 152 features as input."
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "# Initialize a SimpleImputer model\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit the imputer model on the training data\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "\n",
        "# Now you can apply SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8425\n",
            "Precision: 0.8218908345752608\n",
            "F1 Score: 0.8245785479382748\n",
            "Confusion Matrix:\n",
            "[[1876   98]\n",
            " [ 280  146]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\moham\\anaconda32\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Initialize RandomOverSampler\n",
        "ros = RandomOverSampler(sampling_strategy=1.0)  # 100% oversampling\n",
        "\n",
        "# Fit RandomOverSampler and resample the data\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Continue with your model training as before\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_res, y_train_res)\n",
        "# Train the model with the best parameters\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multi-class problems\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "2022.10.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
