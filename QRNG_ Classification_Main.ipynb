{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "# fixes firefox tab completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wgVYr5-TXm5t"
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2, sqrt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.special import erfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-MxSPTrNXsp6",
    "outputId": "0e8f768e-776e-480f-f290-0725176cabe7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate data\n",
    "def concatenateData(df, num_concats):\n",
    "    if num_concats == 0: # do nothing\n",
    "        return df.rename(columns={'binary_number': 'Concatenated_Data'})\n",
    "    new_df = pd.DataFrame({\n",
    "        'Concatenated_Data': [''] * (len(df) // num_concats), \n",
    "        'label': [0] * (len(df) // num_concats)\n",
    "    })\n",
    "\n",
    "    # Loop through each group of num_concats rows and concatenate their 'binary_number' strings\n",
    "    for i in range(0, len(df), num_concats):\n",
    "        new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i + num_concats])\n",
    "        new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Calculate Shannon entropy for each concatenated binary sequence\n",
    "def shannon_entropy(binary_string):\n",
    "    if len(binary_string) % 2 != 0:\n",
    "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
    "    \n",
    "    patterns = ['00', '10', '11', '01']\n",
    "    frequency = {pattern: 0 for pattern in patterns}\n",
    "    \n",
    "    for i in range(0, len(binary_string), 2):\n",
    "        segment = binary_string[i:i+2]\n",
    "        if segment in patterns:\n",
    "            frequency[segment] += 1\n",
    "    \n",
    "    total_segments = sum(frequency.values())\n",
    "    \n",
    "    entropy = 0\n",
    "    for count in frequency.values():\n",
    "        if count > 0:\n",
    "            probability = count / total_segments\n",
    "            entropy -= probability * log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def classic_spectral_test(bit_string):\n",
    "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
    "    dft = fft(bit_array)\n",
    "    n_half = len(bit_string) // 2 + 1\n",
    "    mod_dft = np.abs(dft[:n_half])\n",
    "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
    "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
    "    expected_peaks = 0.95 * n_half\n",
    "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
    "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
    "    return d\n",
    "\n",
    "def frequency_test(bit_string):\n",
    "    n = len(bit_string)\n",
    "    count_ones = bit_string.count('1')\n",
    "    count_zeros = bit_string.count('0')\n",
    "    \n",
    "    # The test statistic\n",
    "    s = (count_ones - count_zeros) / sqrt(n)\n",
    "    \n",
    "    # The p-value\n",
    "    p_value = erfc(abs(s) / sqrt(2))\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def runs_test(bit_string):\n",
    "    n = len(bit_string)\n",
    "    runs = 1  # Start with the first run\n",
    "    for i in range(1, n):\n",
    "        if bit_string[i] != bit_string[i - 1]:\n",
    "            runs += 1\n",
    "    \n",
    "    n0 = bit_string.count('0')\n",
    "    n1 = bit_string.count('1')\n",
    "    \n",
    "    # Expected number of runs\n",
    "    expected_runs = (2 * n0 * n1 / n) + 1\n",
    "    variance_runs = (2 * n0 * n1 * (2 * n0 * n1 - n)) / (n ** 2 * (n - 1))\n",
    "    \n",
    "    # The test statistic\n",
    "    z = (runs - expected_runs) / sqrt(variance_runs)\n",
    "    \n",
    "    # The p-value\n",
    "    p_value = erfc(abs(z) / sqrt(2))\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "def linear_complexity(bit_string, M=500):\n",
    "    # Perform linear complexity test with block size M\n",
    "    n = len(bit_string)\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    lc = 0  # Initialize linear complexity\n",
    "    \n",
    "    # Process blocks of size M\n",
    "    for i in range(0, n, M):\n",
    "        block = bit_array[i:i+M]\n",
    "        if len(block) < M:\n",
    "            continue\n",
    "        \n",
    "        lc_block = 0\n",
    "        for j in range(M):\n",
    "            if block[j] == 1:\n",
    "                lc_block = j + 1\n",
    "        \n",
    "        lc += lc_block\n",
    "    \n",
    "    lc = lc / (n / M)\n",
    "    return lc\n",
    "\n",
    "def autocorrelation_test(bit_string, lag=1):\n",
    "    n = len(bit_string)\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    autocorrelation = np.correlate(bit_array, np.roll(bit_array, lag), mode='valid')[0]\n",
    "    return autocorrelation / n\n",
    "\n",
    "def maurer_universal_test(bit_string):\n",
    "    k = 6\n",
    "    l = 5\n",
    "    q = 20\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    max_val = 2**k\n",
    "    init_subseq = bit_array[:q]\n",
    "    rest_subseq = bit_array[q:]\n",
    "    d = {}\n",
    "    for i in range(len(init_subseq) - k + 1):\n",
    "        d[tuple(init_subseq[i:i+k])] = i\n",
    "    t = []\n",
    "    for i in range(len(rest_subseq) - k + 1):\n",
    "        subseq = tuple(rest_subseq[i:i+k])\n",
    "        if subseq in d:\n",
    "            t.append(i - d[subseq])\n",
    "            d[subseq] = i\n",
    "    if not t:\n",
    "        return 0\n",
    "    t = np.array(t)\n",
    "    log_avg = np.mean(np.log2(t))\n",
    "    return log_avg - np.log2(q)\n",
    "\n",
    "def binary_matrix_rank_test(bit_string, M=32, Q=32):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    num_matrices = len(bit_array) // (M * Q)\n",
    "    ranks = []\n",
    "    for i in range(num_matrices):\n",
    "        matrix = bit_array[i*M*Q:(i+1)*M*Q].reshape((M, Q))\n",
    "        rank = np.linalg.matrix_rank(matrix)\n",
    "        ranks.append(rank)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "def cumulative_sums_test(bit_string):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    adjusted = 2 * bit_array - 1\n",
    "    cumulative_sum = np.cumsum(adjusted)\n",
    "    max_excursion = np.max(np.abs(cumulative_sum))\n",
    "    return max_excursion\n",
    "\n",
    "def longest_run_ones_test(bit_string, block_size=100):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    num_blocks = len(bit_array) // block_size\n",
    "    max_runs = []\n",
    "    for i in range(num_blocks):\n",
    "        block = bit_array[i*block_size:(i+1)*block_size]\n",
    "        max_run = max([len(list(g)) for k, g in itertools.groupby(block) if k == 1])\n",
    "        max_runs.append(max_run)\n",
    "    return np.mean(max_runs)\n",
    "\n",
    "def random_excursions_test(bit_string):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    bit_array = 2 * bit_array - 1  # Convert to ±1\n",
    "\n",
    "    cumulative_sum = np.cumsum(bit_array)\n",
    "    states = np.unique(cumulative_sum)\n",
    "\n",
    "    if 0 not in states:\n",
    "        states = np.append(states, 0)\n",
    "    state_counts = {state: 0 for state in states}\n",
    "    for state in cumulative_sum:\n",
    "        state_counts[state] += 1\n",
    "\n",
    "    state_counts[0] -= 1  # Adjust for zero state\n",
    "    pi = [0.5 * (1 - (1 / (2 * state + 1)**2)) for state in states]\n",
    "    x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unique_subsequences(bit_string, length=4):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    subsequences = set()\n",
    "    \n",
    "    for i in range(n - length + 1):\n",
    "        subseq = tuple(bit_array[i:i+length])\n",
    "        subsequences.add(subseq)\n",
    "    \n",
    "    return len(subsequences)\n",
    "\n",
    "def sample_entropy(bit_string, m=2, r=0.2):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    N = len(bit_array)\n",
    "    \n",
    "    def _phi(m):\n",
    "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.all(np.abs(x[:, None] - x) <= r, axis=2), axis=0) / (N - m + 1.0)\n",
    "        return np.sum(C) / (N - m + 1.0)\n",
    "    \n",
    "    return -np.log(_phi(m + 1) / _phi(m))\n",
    "\n",
    "def permutation_entropy(bit_string, order=3):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    \n",
    "    permutations = np.array(list(itertools.permutations(range(order))))\n",
    "    c = np.zeros(len(permutations))\n",
    "    \n",
    "    for i in range(n - order + 1):\n",
    "        sorted_index_array = tuple(np.argsort(bit_array[i:i+order]))\n",
    "        for j, p in enumerate(permutations):\n",
    "            if np.array_equal(p, sorted_index_array):\n",
    "                c[j] += 1\n",
    "    \n",
    "    c = c / (n - order + 1)\n",
    "    pe = -np.sum(c * np.log2(c + np.finfo(float).eps))\n",
    "    return pe\n",
    "\n",
    "def lyapunov_exponent(bit_string, m=2, t=1):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    N = len(bit_array)\n",
    "    \n",
    "    def _phi(m):\n",
    "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
    "        C = np.sum(np.all(np.abs(x[:, None] - x) <= t, axis=2), axis=0) / (N - m + 1.0)\n",
    "        return np.sum(np.log(C + np.finfo(float).eps)) / (N - m + 1.0)\n",
    "    \n",
    "    return abs(_phi(m) - _phi(m + 1))\n",
    "\n",
    "def entropy_rate(bit_string, k=2):\n",
    "    bit_array = np.array([int(bit) for bit in bit_string])\n",
    "    n = len(bit_array)\n",
    "    prob = {}\n",
    "    \n",
    "    for i in range(n - k + 1):\n",
    "        subseq = tuple(bit_array[i:i + k])\n",
    "        if subseq in prob:\n",
    "            prob[subseq] += 1\n",
    "        else:\n",
    "            prob[subseq] = 1\n",
    "    \n",
    "    for key in prob:\n",
    "        prob[key] /= (n - k + 1)\n",
    "    \n",
    "    entropy_rate = -sum(p * log2(p) for p in prob.values())\n",
    "    return entropy_rate\n",
    "\n",
    "# Apply randomness tests\n",
    "def apply_randomness_tests(df, tests):\n",
    "    if not tests:\n",
    "        raise ValueError(\"No randomness tests specified.\")\n",
    "\n",
    "    test_functions = {\n",
    "        'autocorrelation': autocorrelation_test,\n",
    "        'cumulative_sums': cumulative_sums_test,\n",
    "        'spectral_test': classic_spectral_test,\n",
    "        'frequency_test': frequency_test,\n",
    "        'runs_test': runs_test,\n",
    "        'shannon_entropy': shannon_entropy\n",
    "    }\n",
    "\n",
    "    for test in tests:\n",
    "        if test not in test_functions:\n",
    "            raise ValueError(f\"Invalid randomness test: {test}\")\n",
    "        df[test] = df['Concatenated_Data'].apply(test_functions[test])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df, num_concats, tests):\n",
    "    df = concatenateData(df, num_concats)\n",
    "    processed_df = apply_randomness_tests(df, tests)\n",
    "    \n",
    "    # Convert concatenated binary strings into separate columns\n",
    "    df_features = pd.DataFrame(processed_df['Concatenated_Data'].apply(list).tolist()).astype(int).astype(bool)\n",
    "    processed_df = pd.concat([processed_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "# Calculate min-entropy\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "file_path = 'AI_2qubits_training_data.txt'\n",
    "\n",
    "# Read the data from the file\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            binary_number, label = line.strip().split()\n",
    "            data.append((binary_number, int(label)))\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
    "\n",
    "tests_to_apply = ['spectral_test', 'shannon_entropy', 'frequency_test', 'runs_test', 'autocorrelation']\n",
    "\n",
    "# Preprocess data and apply randomness tests\n",
    "preprocessed_df = preprocess_data(df, num_concats=0, tests=tests_to_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[preprocessed_df.select_dtypes(np.float64).columns] = preprocessed_df.select_dtypes(np.float64).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>spectral_test</th>\n",
       "      <th>shannon_entropy</th>\n",
       "      <th>frequency_test</th>\n",
       "      <th>runs_test</th>\n",
       "      <th>autocorrelation</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-21.771553</td>\n",
       "      <td>1.935451</td>\n",
       "      <td>0.423711</td>\n",
       "      <td>0.120217</td>\n",
       "      <td>0.33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.963615</td>\n",
       "      <td>0.841481</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.31</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.939471</td>\n",
       "      <td>0.109599</td>\n",
       "      <td>0.498506</td>\n",
       "      <td>0.32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.872164</td>\n",
       "      <td>0.071861</td>\n",
       "      <td>0.620874</td>\n",
       "      <td>0.36</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.976281</td>\n",
       "      <td>0.230139</td>\n",
       "      <td>0.725698</td>\n",
       "      <td>0.18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13995</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.942653</td>\n",
       "      <td>0.689157</td>\n",
       "      <td>0.556584</td>\n",
       "      <td>0.24</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13996</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.919479</td>\n",
       "      <td>0.689157</td>\n",
       "      <td>0.987149</td>\n",
       "      <td>0.23</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13997</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.862236</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.36</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13998</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.856367</td>\n",
       "      <td>0.841481</td>\n",
       "      <td>0.548989</td>\n",
       "      <td>0.25</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13999</th>\n",
       "      <td>4</td>\n",
       "      <td>-22.230385</td>\n",
       "      <td>1.717977</td>\n",
       "      <td>0.071861</td>\n",
       "      <td>0.051254</td>\n",
       "      <td>0.21</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14000 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
       "0          1     -21.771553         1.935451        0.423711   0.120217   \n",
       "1          1     -22.230385         1.963615        0.841481   0.027240   \n",
       "2          1     -22.230385         1.939471        0.109599   0.498506   \n",
       "3          1     -22.230385         1.872164        0.071861   0.620874   \n",
       "4          1     -22.230385         1.976281        0.230139   0.725698   \n",
       "...      ...            ...              ...             ...        ...   \n",
       "13995      4     -22.230385         1.942653        0.689157   0.556584   \n",
       "13996      4     -22.230385         1.919479        0.689157   0.987149   \n",
       "13997      4     -22.230385         1.862236        0.317311   0.011137   \n",
       "13998      4     -22.230385         1.856367        0.841481   0.548989   \n",
       "13999      4     -22.230385         1.717977        0.071861   0.051254   \n",
       "\n",
       "       autocorrelation      0      1      2      3  ...     90     91     92  \\\n",
       "0                 0.33  False   True  False  False  ...   True   True   True   \n",
       "1                 0.31  False   True   True  False  ...  False   True   True   \n",
       "2                 0.32   True   True   True  False  ...  False   True   True   \n",
       "3                 0.36   True   True  False   True  ...   True   True  False   \n",
       "4                 0.18  False  False  False  False  ...  False  False   True   \n",
       "...                ...    ...    ...    ...    ...  ...    ...    ...    ...   \n",
       "13995             0.24   True   True   True   True  ...  False  False   True   \n",
       "13996             0.23  False   True  False   True  ...  False   True  False   \n",
       "13997             0.36   True   True  False  False  ...  False  False  False   \n",
       "13998             0.25   True   True  False   True  ...   True   True  False   \n",
       "13999             0.21  False  False   True   True  ...  False  False  False   \n",
       "\n",
       "          93     94     95     96     97     98     99  \n",
       "0       True   True   True  False  False   True  False  \n",
       "1      False  False  False   True   True  False   True  \n",
       "2      False  False  False  False  False   True   True  \n",
       "3       True   True   True   True   True  False   True  \n",
       "4       True   True   True  False  False   True   True  \n",
       "...      ...    ...    ...    ...    ...    ...    ...  \n",
       "13995   True  False  False   True   True  False  False  \n",
       "13996  False  False  False   True   True  False  False  \n",
       "13997  False   True   True   True  False  False  False  \n",
       "13998  False   True   True  False   True  False  False  \n",
       "13999  False   True   True  False   True   True   True  \n",
       "\n",
       "[14000 rows x 106 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test splits should not be shuffled since the order that the bits were sampled matters. However, we need to take splits from each of the quantum computers, so the splits really should be interleaved. \n",
    "\n",
    "Later we'll use k-fold CV to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
      "0          1     -21.771553         1.935451        0.423711   0.120217   \n",
      "1          1     -22.230385         1.963615        0.841481   0.027240   \n",
      "2          1     -22.230385         1.939471        0.109599   0.498506   \n",
      "3          1     -22.230385         1.872164        0.071861   0.620874   \n",
      "4          1     -22.230385         1.976281        0.230139   0.725698   \n",
      "...      ...            ...              ...             ...        ...   \n",
      "13995      4     -22.230385         1.942653        0.689157   0.556584   \n",
      "13996      4     -22.230385         1.919479        0.689157   0.987149   \n",
      "13997      4     -22.230385         1.862236        0.317311   0.011137   \n",
      "13998      4     -22.230385         1.856367        0.841481   0.548989   \n",
      "13999      4     -22.230385         1.717977        0.071861   0.051254   \n",
      "\n",
      "       autocorrelation      0      1      2      3  ...     90     91     92  \\\n",
      "0                 0.33  False   True  False  False  ...   True   True   True   \n",
      "1                 0.31  False   True   True  False  ...  False   True   True   \n",
      "2                 0.32   True   True   True  False  ...  False   True   True   \n",
      "3                 0.36   True   True  False   True  ...   True   True  False   \n",
      "4                 0.18  False  False  False  False  ...  False  False   True   \n",
      "...                ...    ...    ...    ...    ...  ...    ...    ...    ...   \n",
      "13995             0.24   True   True   True   True  ...  False  False   True   \n",
      "13996             0.23  False   True  False   True  ...  False   True  False   \n",
      "13997             0.36   True   True  False  False  ...  False  False  False   \n",
      "13998             0.25   True   True  False   True  ...   True   True  False   \n",
      "13999             0.21  False  False   True   True  ...  False  False  False   \n",
      "\n",
      "          93     94     95     96     97     98     99  \n",
      "0       True   True   True  False  False   True  False  \n",
      "1      False  False  False   True   True  False   True  \n",
      "2      False  False  False  False  False   True   True  \n",
      "3       True   True   True   True   True  False   True  \n",
      "4       True   True   True  False  False   True   True  \n",
      "...      ...    ...    ...    ...    ...    ...    ...  \n",
      "13995   True  False  False   True   True  False  False  \n",
      "13996  False  False  False   True   True  False  False  \n",
      "13997  False   True   True   True  False  False  False  \n",
      "13998  False   True   True  False   True  False  False  \n",
      "13999  False   True   True  False   True   True   True  \n",
      "\n",
      "[14000 rows x 106 columns]\n",
      "[[-22.230384826660156 1.9279470443725586 0.6891565322875977 ... True\n",
      "  False 0.465532241688863]\n",
      " [-22.230384826660156 1.8497108221054077 0.027806894853711128 ... True\n",
      "  False 0.7220967743155884]\n",
      " [-22.230384826660156 1.9516135454177856 0.31731051206588745 ... False\n",
      "  False 0.6111185726573288]\n",
      " ...\n",
      " [-22.230384826660156 1.9813152551651 0.31731051206588745 ... False False\n",
      "  0.4138905233967463]\n",
      " [-22.230384826660156 1.970764398574829 0.841480553150177 ... False True\n",
      "  0.49208818179156244]\n",
      " [-22.230384826660156 1.9279470443725586 0.6891565322875977 ... False\n",
      "  False 0.5413230122306167]]\n",
      "[[-22.230384826660156 1.7937331199645996 0.0006738585070706904 ... True\n",
      "  True 0.8658388393859421]\n",
      " [-22.230384826660156 1.9297515153884888 0.31731051206588745 ... False\n",
      "  False 0.40452218535373685]\n",
      " [-22.230384826660156 1.950714111328125 0.841480553150177 ... True True\n",
      "  0.4880916906443335]\n",
      " ...\n",
      " [-22.230384826660156 1.942683219909668 0.1615133136510849 ... False True\n",
      "  0.3652255874459839]\n",
      " [-21.77155303955078 1.8729350566864014 0.016395071521401405 ... False\n",
      "  False 0.2772736230898516]\n",
      " [-22.230384826660156 1.9802690744400024 0.841480553150177 ... True True\n",
      "  0.5236761572745886]]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = preprocessed_df.drop(columns='label').values\n",
    "y = preprocessed_df['label'].values.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "# min-entropy also takes into account the bitstring! is this ok?\n",
    "min_entropy_train = np.apply_along_axis(calculate_min_entropy, 1, X_train)\n",
    "min_entropy_test = np.apply_along_axis(calculate_min_entropy, 1, X_test)\n",
    "\n",
    "# Add min-entropy as a feature\n",
    "X_train = np.column_stack((X_train, min_entropy_train))\n",
    "X_test = np.column_stack((X_test, min_entropy_test))\n",
    "\n",
    "print(preprocessed_df)\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale all input data for fairness. Fit it on the training data only to prevent leakage. Only scale the nonbinary columns. (Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this much data, we need multithreading. Get number of cores now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 2\n",
      "Using 1 jobs\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "N_CORES = joblib.cpu_count(only_physical_cores=True)\n",
    "print(f\"Number of physical cores: {N_CORES}\")\n",
    "\n",
    "jobs = N_CORES // 2\n",
    "print(f\"Using {jobs} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model testing\n",
    "\n",
    "For now, do a single run of a parallelized model. The goal is to beat 63%, and achieve >95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Brik04cEFwK"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SGDClassifier with hinge loss to get an parallelized SVM. Input data should be scaled to avoid divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 167.56, NNZs: 106, Bias: -12.791554, T: 11200, Avg. loss: 142.852770\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 115.68, NNZs: 106, Bias: -12.617004, T: 22400, Avg. loss: 35.906596\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 87.24, NNZs: 106, Bias: -13.269544, T: 33600, Avg. loss: 21.796820\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 76.89, NNZs: 106, Bias: -13.244764, T: 44800, Avg. loss: 15.583747\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 61.37, NNZs: 106, Bias: -13.258531, T: 56000, Avg. loss: 12.281627\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 61.78, NNZs: 106, Bias: -13.597720, T: 67200, Avg. loss: 9.814250\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 49.72, NNZs: 106, Bias: -13.732396, T: 78400, Avg. loss: 8.673080\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 44.51, NNZs: 106, Bias: -13.730732, T: 89600, Avg. loss: 7.555530\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 43.65, NNZs: 106, Bias: -13.940495, T: 100800, Avg. loss: 6.572499\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 44.55, NNZs: 106, Bias: -14.123476, T: 112000, Avg. loss: 5.852925\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 42.58, NNZs: 106, Bias: -14.200692, T: 123200, Avg. loss: 5.452961\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 42.20, NNZs: 106, Bias: -14.197368, T: 134400, Avg. loss: 4.948633\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 38.86, NNZs: 106, Bias: -14.199009, T: 145600, Avg. loss: 4.675781\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 38.59, NNZs: 106, Bias: -14.396999, T: 156800, Avg. loss: 4.156193\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 37.82, NNZs: 106, Bias: -14.456047, T: 168000, Avg. loss: 3.911202\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 36.93, NNZs: 106, Bias: -14.512829, T: 179200, Avg. loss: 3.671244\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 35.78, NNZs: 106, Bias: -14.567368, T: 190400, Avg. loss: 3.563061\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 34.85, NNZs: 106, Bias: -14.668564, T: 201600, Avg. loss: 3.412182\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 32.39, NNZs: 106, Bias: -14.667155, T: 212800, Avg. loss: 3.274439\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 33.41, NNZs: 106, Bias: -14.756455, T: 224000, Avg. loss: 2.973168\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 32.86, NNZs: 106, Bias: -14.755394, T: 235200, Avg. loss: 2.936550\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 31.86, NNZs: 106, Bias: -14.796760, T: 246400, Avg. loss: 2.848341\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 31.97, NNZs: 106, Bias: -14.836903, T: 257600, Avg. loss: 2.647132\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 31.07, NNZs: 106, Bias: -14.837590, T: 268800, Avg. loss: 2.609961\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 30.50, NNZs: 106, Bias: -14.910983, T: 280000, Avg. loss: 2.497186\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 30.50, NNZs: 106, Bias: -14.980716, T: 291200, Avg. loss: 2.358470\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 30.15, NNZs: 106, Bias: -15.014080, T: 302400, Avg. loss: 2.319831\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 30.13, NNZs: 106, Bias: -15.046744, T: 313600, Avg. loss: 2.209964\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 29.24, NNZs: 106, Bias: -15.047166, T: 324800, Avg. loss: 2.238469\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 29.23, NNZs: 106, Bias: -15.137575, T: 336000, Avg. loss: 2.092166\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 28.74, NNZs: 106, Bias: -15.137400, T: 347200, Avg. loss: 2.059896\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 28.28, NNZs: 106, Bias: -15.138117, T: 358400, Avg. loss: 2.014003\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 28.20, NNZs: 106, Bias: -15.193852, T: 369600, Avg. loss: 1.930858\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 27.64, NNZs: 106, Bias: -15.247544, T: 380800, Avg. loss: 1.910919\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 27.35, NNZs: 106, Bias: -15.273787, T: 392000, Avg. loss: 1.891224\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 27.38, NNZs: 106, Bias: -15.299527, T: 403200, Avg. loss: 1.780801\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 27.04, NNZs: 106, Bias: -15.372779, T: 414400, Avg. loss: 1.747880\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 26.50, NNZs: 106, Bias: -15.372671, T: 425600, Avg. loss: 1.735579\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 26.33, NNZs: 106, Bias: -15.396156, T: 436800, Avg. loss: 1.686946\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 26.49, NNZs: 106, Bias: -15.441447, T: 448000, Avg. loss: 1.612666\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 26.12, NNZs: 106, Bias: -15.463670, T: 459200, Avg. loss: 1.628366\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 25.94, NNZs: 106, Bias: -15.485190, T: 470400, Avg. loss: 1.596450\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 25.66, NNZs: 106, Bias: -15.527329, T: 481600, Avg. loss: 1.561766\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 25.41, NNZs: 106, Bias: -15.547792, T: 492800, Avg. loss: 1.552931\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 25.12, NNZs: 106, Bias: -15.567815, T: 504000, Avg. loss: 1.520181\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 25.05, NNZs: 106, Bias: -15.606975, T: 515200, Avg. loss: 1.474174\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 24.67, NNZs: 106, Bias: -15.625892, T: 526400, Avg. loss: 1.499523\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 24.62, NNZs: 106, Bias: -15.644591, T: 537600, Avg. loss: 1.412052\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 24.44, NNZs: 106, Bias: -15.681297, T: 548800, Avg. loss: 1.397774\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 24.22, NNZs: 106, Bias: -15.699363, T: 560000, Avg. loss: 1.397128\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 23.97, NNZs: 106, Bias: -15.699690, T: 571200, Avg. loss: 1.376616\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 23.64, NNZs: 106, Bias: -15.751767, T: 582400, Avg. loss: 1.339456\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 23.67, NNZs: 106, Bias: -15.768740, T: 593600, Avg. loss: 1.304193\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 23.46, NNZs: 106, Bias: -15.785435, T: 604800, Avg. loss: 1.311982\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 23.20, NNZs: 106, Bias: -15.801931, T: 616000, Avg. loss: 1.287145\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 23.15, NNZs: 106, Bias: -15.818084, T: 627200, Avg. loss: 1.296181\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 22.99, NNZs: 106, Bias: -15.849748, T: 638400, Avg. loss: 1.250505\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 22.84, NNZs: 106, Bias: -15.880765, T: 649600, Avg. loss: 1.204078\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 22.78, NNZs: 106, Bias: -15.895983, T: 660800, Avg. loss: 1.218193\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 22.53, NNZs: 106, Bias: -15.911040, T: 672000, Avg. loss: 1.204965\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 22.23, NNZs: 106, Bias: -15.925935, T: 683200, Avg. loss: 1.202584\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 22.14, NNZs: 106, Bias: -15.969360, T: 694400, Avg. loss: 1.156233\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 21.89, NNZs: 106, Bias: -15.983524, T: 705600, Avg. loss: 1.186019\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 21.77, NNZs: 106, Bias: -15.997509, T: 716800, Avg. loss: 1.151515\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 21.65, NNZs: 106, Bias: -16.011272, T: 728000, Avg. loss: 1.140800\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 21.56, NNZs: 106, Bias: -16.024837, T: 739200, Avg. loss: 1.107127\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 21.50, NNZs: 106, Bias: -16.051608, T: 750400, Avg. loss: 1.102709\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 21.43, NNZs: 106, Bias: -16.038512, T: 761600, Avg. loss: 1.066522\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 21.25, NNZs: 106, Bias: -16.077674, T: 772800, Avg. loss: 1.085967\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 21.18, NNZs: 106, Bias: -16.077766, T: 784000, Avg. loss: 1.065911\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 21.17, NNZs: 106, Bias: -16.103109, T: 795200, Avg. loss: 1.051925\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 21.04, NNZs: 106, Bias: -16.127985, T: 806400, Avg. loss: 1.073532\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 20.85, NNZs: 106, Bias: -16.140218, T: 817600, Avg. loss: 1.041465\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 20.71, NNZs: 106, Bias: -16.152356, T: 828800, Avg. loss: 1.048098\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 20.53, NNZs: 106, Bias: -16.176248, T: 840000, Avg. loss: 1.021900\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 20.41, NNZs: 106, Bias: -16.164502, T: 851200, Avg. loss: 1.016378\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 20.25, NNZs: 106, Bias: -16.199592, T: 862400, Avg. loss: 1.022078\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 20.14, NNZs: 106, Bias: -16.211172, T: 873600, Avg. loss: 0.980097\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 20.05, NNZs: 106, Bias: -16.245348, T: 884800, Avg. loss: 0.961934\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 19.90, NNZs: 106, Bias: -16.256607, T: 896000, Avg. loss: 0.988751\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 19.85, NNZs: 106, Bias: -16.267717, T: 907200, Avg. loss: 0.970289\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 19.74, NNZs: 106, Bias: -16.300532, T: 918400, Avg. loss: 0.953010\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 19.71, NNZs: 106, Bias: -16.311235, T: 929600, Avg. loss: 0.936839\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 19.50, NNZs: 106, Bias: -16.321808, T: 940800, Avg. loss: 0.947373\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 19.36, NNZs: 106, Bias: -16.321827, T: 952000, Avg. loss: 0.949015\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 19.30, NNZs: 106, Bias: -16.353097, T: 963200, Avg. loss: 0.919353\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 19.21, NNZs: 106, Bias: -16.363363, T: 974400, Avg. loss: 0.927859\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 19.03, NNZs: 106, Bias: -16.363388, T: 985600, Avg. loss: 0.934101\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 18.91, NNZs: 106, Bias: -16.393657, T: 996800, Avg. loss: 0.907949\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 18.76, NNZs: 106, Bias: -16.403615, T: 1008000, Avg. loss: 0.913192\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 18.70, NNZs: 106, Bias: -16.423303, T: 1019200, Avg. loss: 0.891198\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 18.67, NNZs: 106, Bias: -16.442732, T: 1030400, Avg. loss: 0.870702\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 18.56, NNZs: 106, Bias: -16.442719, T: 1041600, Avg. loss: 0.897082\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 18.49, NNZs: 106, Bias: -16.461771, T: 1052800, Avg. loss: 0.878773\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 18.39, NNZs: 106, Bias: -16.471129, T: 1064000, Avg. loss: 0.863503\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 18.34, NNZs: 106, Bias: -16.480436, T: 1075200, Avg. loss: 0.867734\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 18.23, NNZs: 106, Bias: -16.498861, T: 1086400, Avg. loss: 0.840728\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 18.12, NNZs: 106, Bias: -16.498841, T: 1097600, Avg. loss: 0.885281\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 18.07, NNZs: 106, Bias: -16.526027, T: 1108800, Avg. loss: 0.837666\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 17.93, NNZs: 106, Bias: -16.526049, T: 1120000, Avg. loss: 0.853038\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 17.86, NNZs: 106, Bias: -16.552684, T: 1131200, Avg. loss: 0.834025\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 17.81, NNZs: 106, Bias: -16.561425, T: 1142400, Avg. loss: 0.826249\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 17.76, NNZs: 106, Bias: -16.570130, T: 1153600, Avg. loss: 0.826758\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 17.63, NNZs: 106, Bias: -16.587360, T: 1164800, Avg. loss: 0.842909\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 17.55, NNZs: 106, Bias: -16.595850, T: 1176000, Avg. loss: 0.817585\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 17.51, NNZs: 106, Bias: -16.604270, T: 1187200, Avg. loss: 0.813865\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 17.43, NNZs: 106, Bias: -16.604327, T: 1198400, Avg. loss: 0.815081\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 17.36, NNZs: 106, Bias: -16.620946, T: 1209600, Avg. loss: 0.798837\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 17.26, NNZs: 106, Bias: -16.629192, T: 1220800, Avg. loss: 0.799215\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 17.16, NNZs: 106, Bias: -16.629282, T: 1232000, Avg. loss: 0.782373\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 17.08, NNZs: 106, Bias: -16.653559, T: 1243200, Avg. loss: 0.788221\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 17.05, NNZs: 106, Bias: -16.661570, T: 1254400, Avg. loss: 0.774446\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 17.01, NNZs: 106, Bias: -16.677404, T: 1265600, Avg. loss: 0.786388\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 16.89, NNZs: 106, Bias: -16.669533, T: 1276800, Avg. loss: 0.786756\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 16.83, NNZs: 106, Bias: -16.685141, T: 1288000, Avg. loss: 0.771326\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 16.76, NNZs: 106, Bias: -16.692868, T: 1299200, Avg. loss: 0.773284\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 16.69, NNZs: 106, Bias: -16.708192, T: 1310400, Avg. loss: 0.767741\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 16.63, NNZs: 106, Bias: -16.715818, T: 1321600, Avg. loss: 0.768211\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 16.53, NNZs: 106, Bias: -16.723407, T: 1332800, Avg. loss: 0.782641\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 16.47, NNZs: 106, Bias: -16.738376, T: 1344000, Avg. loss: 0.745073\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 16.43, NNZs: 106, Bias: -16.760574, T: 1355200, Avg. loss: 0.745948\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 16.33, NNZs: 106, Bias: -16.767875, T: 1366400, Avg. loss: 0.755352\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 16.29, NNZs: 106, Bias: -16.767863, T: 1377600, Avg. loss: 0.743842\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 16.22, NNZs: 106, Bias: -16.782313, T: 1388800, Avg. loss: 0.748352\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 16.19, NNZs: 106, Bias: -16.789475, T: 1400000, Avg. loss: 0.730337\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 16.12, NNZs: 106, Bias: -16.803692, T: 1411200, Avg. loss: 0.732410\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 16.06, NNZs: 106, Bias: -16.803710, T: 1422400, Avg. loss: 0.739425\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 16.02, NNZs: 106, Bias: -16.824695, T: 1433600, Avg. loss: 0.717860\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 15.97, NNZs: 106, Bias: -16.831597, T: 1444800, Avg. loss: 0.719995\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 15.90, NNZs: 106, Bias: -16.831591, T: 1456000, Avg. loss: 0.726012\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 15.85, NNZs: 106, Bias: -16.845284, T: 1467200, Avg. loss: 0.724127\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 15.78, NNZs: 106, Bias: -16.852087, T: 1478400, Avg. loss: 0.702155\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 15.72, NNZs: 106, Bias: -16.858823, T: 1489600, Avg. loss: 0.712357\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 15.67, NNZs: 106, Bias: -16.872168, T: 1500800, Avg. loss: 0.715047\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 15.61, NNZs: 106, Bias: -16.878770, T: 1512000, Avg. loss: 0.707805\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 15.58, NNZs: 106, Bias: -16.878740, T: 1523200, Avg. loss: 0.696918\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 15.53, NNZs: 106, Bias: -16.891800, T: 1534400, Avg. loss: 0.690718\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 15.50, NNZs: 106, Bias: -16.898270, T: 1545600, Avg. loss: 0.682707\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 15.44, NNZs: 106, Bias: -16.898276, T: 1556800, Avg. loss: 0.709103\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 15.38, NNZs: 106, Bias: -16.917463, T: 1568000, Avg. loss: 0.685812\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 15.35, NNZs: 106, Bias: -16.923786, T: 1579200, Avg. loss: 0.684467\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 15.28, NNZs: 106, Bias: -16.930080, T: 1590400, Avg. loss: 0.688491\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 15.20, NNZs: 106, Bias: -16.936327, T: 1601600, Avg. loss: 0.693207\n",
      "Total training time: 0.58 seconds.\n",
      "Convergence after 143 epochs took 0.58 seconds\n",
      "-- Epoch 1\n",
      "Norm: 244.12, NNZs: 106, Bias: -14.376622, T: 11200, Avg. loss: 126.765846\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 155.32, NNZs: 106, Bias: -14.987774, T: 22400, Avg. loss: 31.836861\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 120.34, NNZs: 106, Bias: -15.634335, T: 33600, Avg. loss: 19.584587\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 86.48, NNZs: 106, Bias: -15.582668, T: 44800, Avg. loss: 14.335947\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 70.52, NNZs: 106, Bias: -15.378627, T: 56000, Avg. loss: 11.069460\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 63.53, NNZs: 106, Bias: -15.557923, T: 67200, Avg. loss: 8.913359\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 51.00, NNZs: 106, Bias: -15.689918, T: 78400, Avg. loss: 7.896244\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 48.80, NNZs: 106, Bias: -15.802091, T: 89600, Avg. loss: 6.709779\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 46.68, NNZs: 106, Bias: -15.799836, T: 100800, Avg. loss: 6.166986\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 43.92, NNZs: 106, Bias: -15.984042, T: 112000, Avg. loss: 5.391312\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 40.14, NNZs: 106, Bias: -15.983739, T: 123200, Avg. loss: 4.960019\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 38.48, NNZs: 106, Bias: -16.061633, T: 134400, Avg. loss: 4.522366\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 37.56, NNZs: 106, Bias: -16.131543, T: 145600, Avg. loss: 4.215361\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 36.64, NNZs: 106, Bias: -16.131268, T: 156800, Avg. loss: 3.843691\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 34.98, NNZs: 106, Bias: -16.192192, T: 168000, Avg. loss: 3.677349\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 33.48, NNZs: 106, Bias: -16.248043, T: 179200, Avg. loss: 3.417420\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 33.06, NNZs: 106, Bias: -16.299314, T: 190400, Avg. loss: 3.253204\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 31.19, NNZs: 106, Bias: -16.296616, T: 201600, Avg. loss: 3.063879\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 28.76, NNZs: 106, Bias: -16.247951, T: 212800, Avg. loss: 2.989566\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 28.59, NNZs: 106, Bias: -16.294298, T: 224000, Avg. loss: 2.818070\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 27.75, NNZs: 106, Bias: -16.338701, T: 235200, Avg. loss: 2.684232\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 27.37, NNZs: 106, Bias: -16.380650, T: 246400, Avg. loss: 2.618119\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 25.65, NNZs: 106, Bias: -16.419925, T: 257600, Avg. loss: 2.543949\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 25.58, NNZs: 106, Bias: -16.420361, T: 268800, Avg. loss: 2.381449\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 25.67, NNZs: 106, Bias: -16.492516, T: 280000, Avg. loss: 2.270888\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 24.46, NNZs: 106, Bias: -16.457387, T: 291200, Avg. loss: 2.242347\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 24.03, NNZs: 106, Bias: -16.490725, T: 302400, Avg. loss: 2.145699\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 24.30, NNZs: 106, Bias: -16.490633, T: 313600, Avg. loss: 2.094879\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 23.64, NNZs: 106, Bias: -16.553032, T: 324800, Avg. loss: 2.013986\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 23.59, NNZs: 106, Bias: -16.553079, T: 336000, Avg. loss: 1.943586\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 23.01, NNZs: 106, Bias: -16.582145, T: 347200, Avg. loss: 1.940939\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 23.06, NNZs: 106, Bias: -16.582034, T: 358400, Avg. loss: 1.828338\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 22.37, NNZs: 106, Bias: -16.582278, T: 369600, Avg. loss: 1.839910\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 22.16, NNZs: 106, Bias: -16.635487, T: 380800, Avg. loss: 1.756881\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 21.84, NNZs: 106, Bias: -16.635315, T: 392000, Avg. loss: 1.690298\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 21.69, NNZs: 106, Bias: -16.660818, T: 403200, Avg. loss: 1.661660\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 21.61, NNZs: 106, Bias: -16.685516, T: 414400, Avg. loss: 1.627937\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 20.97, NNZs: 106, Bias: -16.709165, T: 425600, Avg. loss: 1.621306\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 20.84, NNZs: 106, Bias: -16.709342, T: 436800, Avg. loss: 1.546813\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 20.51, NNZs: 106, Bias: -16.754581, T: 448000, Avg. loss: 1.562856\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 20.28, NNZs: 106, Bias: -16.754674, T: 459200, Avg. loss: 1.514956\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 19.86, NNZs: 106, Bias: -16.776020, T: 470400, Avg. loss: 1.455935\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 19.60, NNZs: 106, Bias: -16.775854, T: 481600, Avg. loss: 1.419372\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 19.63, NNZs: 106, Bias: -16.816849, T: 492800, Avg. loss: 1.426148\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 19.53, NNZs: 106, Bias: -16.836802, T: 504000, Avg. loss: 1.386234\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 19.50, NNZs: 106, Bias: -16.856421, T: 515200, Avg. loss: 1.362246\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 19.27, NNZs: 106, Bias: -16.875571, T: 526400, Avg. loss: 1.368260\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 19.15, NNZs: 106, Bias: -16.875638, T: 537600, Avg. loss: 1.328736\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 18.89, NNZs: 106, Bias: -16.894212, T: 548800, Avg. loss: 1.294439\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 18.67, NNZs: 106, Bias: -16.912337, T: 560000, Avg. loss: 1.267892\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 18.51, NNZs: 106, Bias: -16.947541, T: 571200, Avg. loss: 1.291905\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 18.32, NNZs: 106, Bias: -16.930214, T: 582400, Avg. loss: 1.274330\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 18.13, NNZs: 106, Bias: -16.964269, T: 593600, Avg. loss: 1.213857\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 17.93, NNZs: 106, Bias: -16.964265, T: 604800, Avg. loss: 1.202839\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 17.71, NNZs: 106, Bias: -16.964346, T: 616000, Avg. loss: 1.212296\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 17.76, NNZs: 106, Bias: -16.980445, T: 627200, Avg. loss: 1.149368\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 17.54, NNZs: 106, Bias: -16.980522, T: 638400, Avg. loss: 1.162778\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 17.28, NNZs: 106, Bias: -16.996139, T: 649600, Avg. loss: 1.172260\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 17.40, NNZs: 106, Bias: -17.011457, T: 660800, Avg. loss: 1.123051\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 17.17, NNZs: 106, Bias: -17.026513, T: 672000, Avg. loss: 1.122614\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 16.99, NNZs: 106, Bias: -17.041244, T: 683200, Avg. loss: 1.099829\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 16.90, NNZs: 106, Bias: -17.041253, T: 694400, Avg. loss: 1.086807\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 16.85, NNZs: 106, Bias: -17.055618, T: 705600, Avg. loss: 1.065035\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 16.67, NNZs: 106, Bias: -17.055677, T: 716800, Avg. loss: 1.067316\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 16.54, NNZs: 106, Bias: -17.083331, T: 728000, Avg. loss: 1.073814\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 16.35, NNZs: 106, Bias: -17.083204, T: 739200, Avg. loss: 1.059491\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 16.31, NNZs: 106, Bias: -17.083180, T: 750400, Avg. loss: 1.025788\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 16.19, NNZs: 106, Bias: -17.096474, T: 761600, Avg. loss: 1.024991\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 16.09, NNZs: 106, Bias: -17.109474, T: 772800, Avg. loss: 1.019553\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 16.02, NNZs: 106, Bias: -17.122321, T: 784000, Avg. loss: 1.008754\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 15.95, NNZs: 106, Bias: -17.122298, T: 795200, Avg. loss: 0.996325\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 15.71, NNZs: 106, Bias: -17.134724, T: 806400, Avg. loss: 0.987882\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 15.62, NNZs: 106, Bias: -17.134723, T: 817600, Avg. loss: 0.985467\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 15.50, NNZs: 106, Bias: -17.146913, T: 828800, Avg. loss: 0.963662\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 15.42, NNZs: 106, Bias: -17.158843, T: 840000, Avg. loss: 0.955863\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 15.28, NNZs: 106, Bias: -17.170518, T: 851200, Avg. loss: 0.938699\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 15.21, NNZs: 106, Bias: -17.158847, T: 862400, Avg. loss: 0.925787\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 15.15, NNZs: 106, Bias: -17.193380, T: 873600, Avg. loss: 0.939293\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 15.04, NNZs: 106, Bias: -17.182001, T: 884800, Avg. loss: 0.933327\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 15.01, NNZs: 106, Bias: -17.204447, T: 896000, Avg. loss: 0.903762\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 14.88, NNZs: 106, Bias: -17.215368, T: 907200, Avg. loss: 0.890944\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 14.83, NNZs: 106, Bias: -17.215259, T: 918400, Avg. loss: 0.900676\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 14.69, NNZs: 106, Bias: -17.226010, T: 929600, Avg. loss: 0.890541\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 14.64, NNZs: 106, Bias: -17.225956, T: 940800, Avg. loss: 0.892502\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 14.58, NNZs: 106, Bias: -17.225975, T: 952000, Avg. loss: 0.871105\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 14.57, NNZs: 106, Bias: -17.236442, T: 963200, Avg. loss: 0.869778\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 14.33, NNZs: 106, Bias: -17.236499, T: 974400, Avg. loss: 0.873525\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 14.22, NNZs: 106, Bias: -17.256885, T: 985600, Avg. loss: 0.857284\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 14.22, NNZs: 106, Bias: -17.246810, T: 996800, Avg. loss: 0.859630\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 14.14, NNZs: 106, Bias: -17.246950, T: 1008000, Avg. loss: 0.846730\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 14.05, NNZs: 106, Bias: -17.266718, T: 1019200, Avg. loss: 0.823309\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 13.99, NNZs: 106, Bias: -17.276443, T: 1030400, Avg. loss: 0.830558\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 13.93, NNZs: 106, Bias: -17.276373, T: 1041600, Avg. loss: 0.832792\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 13.89, NNZs: 106, Bias: -17.276372, T: 1052800, Avg. loss: 0.820976\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 13.84, NNZs: 106, Bias: -17.285828, T: 1064000, Avg. loss: 0.813236\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 13.81, NNZs: 106, Bias: -17.295124, T: 1075200, Avg. loss: 0.816554\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 13.73, NNZs: 106, Bias: -17.304304, T: 1086400, Avg. loss: 0.802112\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 13.58, NNZs: 106, Bias: -17.295133, T: 1097600, Avg. loss: 0.811959\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 13.51, NNZs: 106, Bias: -17.313254, T: 1108800, Avg. loss: 0.793837\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 13.49, NNZs: 106, Bias: -17.313232, T: 1120000, Avg. loss: 0.795141\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 13.46, NNZs: 106, Bias: -17.322085, T: 1131200, Avg. loss: 0.795168\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 13.37, NNZs: 106, Bias: -17.330799, T: 1142400, Avg. loss: 0.792793\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 13.28, NNZs: 106, Bias: -17.330759, T: 1153600, Avg. loss: 0.779430\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 13.19, NNZs: 106, Bias: -17.339346, T: 1164800, Avg. loss: 0.773256\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 13.19, NNZs: 106, Bias: -17.330809, T: 1176000, Avg. loss: 0.769583\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 13.13, NNZs: 106, Bias: -17.347752, T: 1187200, Avg. loss: 0.756847\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 13.04, NNZs: 106, Bias: -17.347767, T: 1198400, Avg. loss: 0.754858\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 13.02, NNZs: 106, Bias: -17.364363, T: 1209600, Avg. loss: 0.753347\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 12.97, NNZs: 106, Bias: -17.356132, T: 1220800, Avg. loss: 0.751508\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 12.88, NNZs: 106, Bias: -17.364330, T: 1232000, Avg. loss: 0.756790\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 12.86, NNZs: 106, Bias: -17.364351, T: 1243200, Avg. loss: 0.738219\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 12.79, NNZs: 106, Bias: -17.372369, T: 1254400, Avg. loss: 0.731712\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 12.74, NNZs: 106, Bias: -17.372415, T: 1265600, Avg. loss: 0.725582\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 12.71, NNZs: 106, Bias: -17.388142, T: 1276800, Avg. loss: 0.735004\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 12.59, NNZs: 106, Bias: -17.380345, T: 1288000, Avg. loss: 0.731237\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 12.56, NNZs: 106, Bias: -17.395788, T: 1299200, Avg. loss: 0.716734\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 12.48, NNZs: 106, Bias: -17.388179, T: 1310400, Avg. loss: 0.723865\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 12.40, NNZs: 106, Bias: -17.403404, T: 1321600, Avg. loss: 0.706577\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 12.35, NNZs: 106, Bias: -17.410911, T: 1332800, Avg. loss: 0.719476\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 12.33, NNZs: 106, Bias: -17.418322, T: 1344000, Avg. loss: 0.708091\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 12.26, NNZs: 106, Bias: -17.410930, T: 1355200, Avg. loss: 0.703676\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 12.25, NNZs: 106, Bias: -17.418307, T: 1366400, Avg. loss: 0.699142\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 12.14, NNZs: 106, Bias: -17.418338, T: 1377600, Avg. loss: 0.702216\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 12.07, NNZs: 106, Bias: -17.432784, T: 1388800, Avg. loss: 0.699899\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 12.09, NNZs: 106, Bias: -17.432748, T: 1400000, Avg. loss: 0.670477\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 11.98, NNZs: 106, Bias: -17.439815, T: 1411200, Avg. loss: 0.681181\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 11.93, NNZs: 106, Bias: -17.432744, T: 1422400, Avg. loss: 0.703894\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 11.95, NNZs: 106, Bias: -17.446716, T: 1433600, Avg. loss: 0.684633\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 11.90, NNZs: 106, Bias: -17.439752, T: 1444800, Avg. loss: 0.683418\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 11.89, NNZs: 106, Bias: -17.446663, T: 1456000, Avg. loss: 0.677877\n",
      "Total training time: 0.54 seconds.\n",
      "Convergence after 130 epochs took 0.54 seconds\n",
      "-- Epoch 1\n",
      "Norm: 351.03, NNZs: 106, Bias: -0.869669, T: 11200, Avg. loss: 124.098105\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 224.50, NNZs: 106, Bias: -1.407775, T: 22400, Avg. loss: 29.796210\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 161.60, NNZs: 106, Bias: -1.780606, T: 33600, Avg. loss: 17.977235\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 126.87, NNZs: 106, Bias: -2.003394, T: 44800, Avg. loss: 13.087283\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 107.89, NNZs: 106, Bias: -2.023696, T: 56000, Avg. loss: 10.204711\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 91.70, NNZs: 106, Bias: -2.347726, T: 67200, Avg. loss: 8.539190\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 83.02, NNZs: 106, Bias: -2.214184, T: 78400, Avg. loss: 7.079189\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 74.81, NNZs: 106, Bias: -2.462850, T: 89600, Avg. loss: 6.347074\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 68.99, NNZs: 106, Bias: -2.675640, T: 100800, Avg. loss: 5.487124\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 64.17, NNZs: 106, Bias: -2.770031, T: 112000, Avg. loss: 5.121066\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 60.63, NNZs: 106, Bias: -2.856356, T: 123200, Avg. loss: 4.521482\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 56.99, NNZs: 106, Bias: -2.934270, T: 134400, Avg. loss: 4.202336\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 53.35, NNZs: 106, Bias: -2.934471, T: 145600, Avg. loss: 3.912912\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 51.70, NNZs: 106, Bias: -3.065534, T: 156800, Avg. loss: 3.649142\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 49.41, NNZs: 106, Bias: -3.065505, T: 168000, Avg. loss: 3.353971\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 47.56, NNZs: 106, Bias: -3.180943, T: 179200, Avg. loss: 3.150599\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 45.69, NNZs: 106, Bias: -3.234225, T: 190400, Avg. loss: 2.930036\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 44.50, NNZs: 106, Bias: -3.284936, T: 201600, Avg. loss: 2.851680\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 42.88, NNZs: 106, Bias: -3.333990, T: 212800, Avg. loss: 2.671458\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 42.23, NNZs: 106, Bias: -3.380518, T: 224000, Avg. loss: 2.532772\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 41.27, NNZs: 106, Bias: -3.467882, T: 235200, Avg. loss: 2.403468\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 40.50, NNZs: 106, Bias: -3.509763, T: 246400, Avg. loss: 2.333097\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 39.69, NNZs: 106, Bias: -3.588999, T: 257600, Avg. loss: 2.260758\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 38.91, NNZs: 106, Bias: -3.589408, T: 268800, Avg. loss: 2.146370\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 38.18, NNZs: 106, Bias: -3.698324, T: 280000, Avg. loss: 2.085765\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 37.41, NNZs: 106, Bias: -3.731990, T: 291200, Avg. loss: 2.026402\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 36.54, NNZs: 106, Bias: -3.765021, T: 302400, Avg. loss: 1.964979\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 36.46, NNZs: 106, Bias: -3.797014, T: 313600, Avg. loss: 1.836850\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 36.10, NNZs: 106, Bias: -3.828271, T: 324800, Avg. loss: 1.814463\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 35.43, NNZs: 106, Bias: -3.888147, T: 336000, Avg. loss: 1.745213\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 35.18, NNZs: 106, Bias: -3.917207, T: 347200, Avg. loss: 1.680115\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 34.83, NNZs: 106, Bias: -3.945349, T: 358400, Avg. loss: 1.709786\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 34.53, NNZs: 106, Bias: -3.972822, T: 369600, Avg. loss: 1.648594\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 34.18, NNZs: 106, Bias: -3.999823, T: 380800, Avg. loss: 1.580931\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 33.76, NNZs: 106, Bias: -4.026252, T: 392000, Avg. loss: 1.546187\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 33.37, NNZs: 106, Bias: -4.101641, T: 403200, Avg. loss: 1.537163\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 33.15, NNZs: 106, Bias: -4.149784, T: 414400, Avg. loss: 1.448020\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 32.66, NNZs: 106, Bias: -4.149425, T: 425600, Avg. loss: 1.460381\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 32.29, NNZs: 106, Bias: -4.172713, T: 436800, Avg. loss: 1.389404\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 31.98, NNZs: 106, Bias: -4.218114, T: 448000, Avg. loss: 1.372978\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 31.84, NNZs: 106, Bias: -4.262145, T: 459200, Avg. loss: 1.323655\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 31.55, NNZs: 106, Bias: -4.304782, T: 470400, Avg. loss: 1.323823\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 31.46, NNZs: 106, Bias: -4.304487, T: 481600, Avg. loss: 1.267270\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 31.08, NNZs: 106, Bias: -4.345612, T: 492800, Avg. loss: 1.292720\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 30.82, NNZs: 106, Bias: -4.385894, T: 504000, Avg. loss: 1.233170\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 30.50, NNZs: 106, Bias: -4.444463, T: 515200, Avg. loss: 1.217254\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 30.24, NNZs: 106, Bias: -4.463748, T: 526400, Avg. loss: 1.231042\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 30.04, NNZs: 106, Bias: -4.482881, T: 537600, Avg. loss: 1.180218\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 29.84, NNZs: 106, Bias: -4.538186, T: 548800, Avg. loss: 1.187586\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 29.69, NNZs: 106, Bias: -4.556086, T: 560000, Avg. loss: 1.152199\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 29.54, NNZs: 106, Bias: -4.591361, T: 571200, Avg. loss: 1.116171\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 29.29, NNZs: 106, Bias: -4.625905, T: 582400, Avg. loss: 1.094273\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 29.18, NNZs: 106, Bias: -4.642798, T: 593600, Avg. loss: 1.110720\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 29.02, NNZs: 106, Bias: -4.676120, T: 604800, Avg. loss: 1.060593\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 28.79, NNZs: 106, Bias: -4.676456, T: 616000, Avg. loss: 1.087605\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 28.68, NNZs: 106, Bias: -4.740835, T: 627200, Avg. loss: 1.042536\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 28.46, NNZs: 106, Bias: -4.756542, T: 638400, Avg. loss: 1.052727\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 28.33, NNZs: 106, Bias: -4.787543, T: 649600, Avg. loss: 1.017905\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 28.16, NNZs: 106, Bias: -4.802727, T: 660800, Avg. loss: 0.991056\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 28.01, NNZs: 106, Bias: -4.832825, T: 672000, Avg. loss: 0.998990\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 27.88, NNZs: 106, Bias: -4.862371, T: 683200, Avg. loss: 0.978760\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 27.74, NNZs: 106, Bias: -4.905843, T: 694400, Avg. loss: 0.966940\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 27.61, NNZs: 106, Bias: -4.934333, T: 705600, Avg. loss: 0.955203\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 27.49, NNZs: 106, Bias: -4.962417, T: 716800, Avg. loss: 0.930917\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 27.29, NNZs: 106, Bias: -4.976208, T: 728000, Avg. loss: 0.944602\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 27.23, NNZs: 106, Bias: -5.003467, T: 739200, Avg. loss: 0.903664\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 27.09, NNZs: 106, Bias: -5.030280, T: 750400, Avg. loss: 0.928207\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 26.92, NNZs: 106, Bias: -5.056743, T: 761600, Avg. loss: 0.903574\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 26.79, NNZs: 106, Bias: -5.082846, T: 772800, Avg. loss: 0.902143\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 26.69, NNZs: 106, Bias: -5.121322, T: 784000, Avg. loss: 0.877858\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 26.59, NNZs: 106, Bias: -5.133937, T: 795200, Avg. loss: 0.868380\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 26.43, NNZs: 106, Bias: -5.158918, T: 806400, Avg. loss: 0.885345\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 26.27, NNZs: 106, Bias: -5.195757, T: 817600, Avg. loss: 0.867880\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 26.17, NNZs: 106, Bias: -5.207776, T: 828800, Avg. loss: 0.857271\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 26.05, NNZs: 106, Bias: -5.243627, T: 840000, Avg. loss: 0.852151\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 25.93, NNZs: 106, Bias: -5.267075, T: 851200, Avg. loss: 0.846523\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 25.73, NNZs: 106, Bias: -5.278669, T: 862400, Avg. loss: 0.833654\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 25.63, NNZs: 106, Bias: -5.301632, T: 873600, Avg. loss: 0.820302\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 25.58, NNZs: 106, Bias: -5.324318, T: 884800, Avg. loss: 0.810387\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 25.51, NNZs: 106, Bias: -5.346685, T: 896000, Avg. loss: 0.783002\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 25.41, NNZs: 106, Bias: -5.368773, T: 907200, Avg. loss: 0.804223\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 25.28, NNZs: 106, Bias: -5.390586, T: 918400, Avg. loss: 0.789525\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 25.22, NNZs: 106, Bias: -5.401378, T: 929600, Avg. loss: 0.782015\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 25.14, NNZs: 106, Bias: -5.422760, T: 940800, Avg. loss: 0.791380\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 25.01, NNZs: 106, Bias: -5.443862, T: 952000, Avg. loss: 0.779530\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 24.93, NNZs: 106, Bias: -5.475121, T: 963200, Avg. loss: 0.752856\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 24.85, NNZs: 106, Bias: -5.495624, T: 974400, Avg. loss: 0.762339\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 24.76, NNZs: 106, Bias: -5.495635, T: 985600, Avg. loss: 0.738917\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 24.67, NNZs: 106, Bias: -5.525888, T: 996800, Avg. loss: 0.742183\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 24.54, NNZs: 106, Bias: -5.545843, T: 1008000, Avg. loss: 0.747220\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 24.46, NNZs: 106, Bias: -5.575411, T: 1019200, Avg. loss: 0.740394\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 24.35, NNZs: 106, Bias: -5.594893, T: 1030400, Avg. loss: 0.737800\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 24.24, NNZs: 106, Bias: -5.614160, T: 1041600, Avg. loss: 0.737670\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 24.16, NNZs: 106, Bias: -5.623701, T: 1052800, Avg. loss: 0.723071\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 24.11, NNZs: 106, Bias: -5.652025, T: 1064000, Avg. loss: 0.713054\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 24.04, NNZs: 106, Bias: -5.670655, T: 1075200, Avg. loss: 0.719624\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 23.93, NNZs: 106, Bias: -5.689122, T: 1086400, Avg. loss: 0.720125\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 23.86, NNZs: 106, Bias: -5.707405, T: 1097600, Avg. loss: 0.709043\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 23.76, NNZs: 106, Bias: -5.716476, T: 1108800, Avg. loss: 0.698100\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 23.67, NNZs: 106, Bias: -5.743455, T: 1120000, Avg. loss: 0.701645\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 23.58, NNZs: 106, Bias: -5.761286, T: 1131200, Avg. loss: 0.691369\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 23.51, NNZs: 106, Bias: -5.796466, T: 1142400, Avg. loss: 0.682741\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 23.42, NNZs: 106, Bias: -5.805186, T: 1153600, Avg. loss: 0.697310\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 23.36, NNZs: 106, Bias: -5.822465, T: 1164800, Avg. loss: 0.678113\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 23.28, NNZs: 106, Bias: -5.848135, T: 1176000, Avg. loss: 0.685987\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 23.22, NNZs: 106, Bias: -5.881916, T: 1187200, Avg. loss: 0.665053\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 23.13, NNZs: 106, Bias: -5.898613, T: 1198400, Avg. loss: 0.682772\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 23.06, NNZs: 106, Bias: -5.915185, T: 1209600, Avg. loss: 0.671873\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 22.95, NNZs: 106, Bias: -5.931634, T: 1220800, Avg. loss: 0.658882\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 22.89, NNZs: 106, Bias: -5.947927, T: 1232000, Avg. loss: 0.662195\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 22.80, NNZs: 106, Bias: -5.964064, T: 1243200, Avg. loss: 0.660126\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 22.74, NNZs: 106, Bias: -5.988034, T: 1254400, Avg. loss: 0.655614\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 22.66, NNZs: 106, Bias: -5.995923, T: 1265600, Avg. loss: 0.668937\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 22.57, NNZs: 106, Bias: -6.011685, T: 1276800, Avg. loss: 0.646822\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 22.51, NNZs: 106, Bias: -6.027332, T: 1288000, Avg. loss: 0.645647\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 22.44, NNZs: 106, Bias: -6.058259, T: 1299200, Avg. loss: 0.651921\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 22.40, NNZs: 106, Bias: -6.073543, T: 1310400, Avg. loss: 0.641004\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 22.31, NNZs: 106, Bias: -6.088665, T: 1321600, Avg. loss: 0.625095\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 22.25, NNZs: 106, Bias: -6.103677, T: 1332800, Avg. loss: 0.632493\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 22.16, NNZs: 106, Bias: -6.126036, T: 1344000, Avg. loss: 0.619965\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 22.08, NNZs: 106, Bias: -6.133399, T: 1355200, Avg. loss: 0.624260\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 22.02, NNZs: 106, Bias: -6.148091, T: 1366400, Avg. loss: 0.625650\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 21.97, NNZs: 106, Bias: -6.169918, T: 1377600, Avg. loss: 0.613904\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 21.89, NNZs: 106, Bias: -6.184340, T: 1388800, Avg. loss: 0.617718\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 21.82, NNZs: 106, Bias: -6.205806, T: 1400000, Avg. loss: 0.619642\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 21.74, NNZs: 106, Bias: -6.212887, T: 1411200, Avg. loss: 0.617021\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 21.70, NNZs: 106, Bias: -6.219965, T: 1422400, Avg. loss: 0.603266\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 21.64, NNZs: 106, Bias: -6.240991, T: 1433600, Avg. loss: 0.609238\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 21.59, NNZs: 106, Bias: -6.254915, T: 1444800, Avg. loss: 0.601804\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 21.52, NNZs: 106, Bias: -6.275610, T: 1456000, Avg. loss: 0.596155\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 21.46, NNZs: 106, Bias: -6.275675, T: 1467200, Avg. loss: 0.595688\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 21.41, NNZs: 106, Bias: -6.309649, T: 1478400, Avg. loss: 0.581915\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 21.35, NNZs: 106, Bias: -6.323097, T: 1489600, Avg. loss: 0.590638\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 21.30, NNZs: 106, Bias: -6.336474, T: 1500800, Avg. loss: 0.595406\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 21.26, NNZs: 106, Bias: -6.349780, T: 1512000, Avg. loss: 0.590533\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 21.21, NNZs: 106, Bias: -6.369555, T: 1523200, Avg. loss: 0.588170\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 21.17, NNZs: 106, Bias: -6.382626, T: 1534400, Avg. loss: 0.580801\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 21.10, NNZs: 106, Bias: -6.408578, T: 1545600, Avg. loss: 0.586315\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 21.01, NNZs: 106, Bias: -6.415015, T: 1556800, Avg. loss: 0.587040\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 20.97, NNZs: 106, Bias: -6.434212, T: 1568000, Avg. loss: 0.572479\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 20.93, NNZs: 106, Bias: -6.446913, T: 1579200, Avg. loss: 0.570392\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 20.86, NNZs: 106, Bias: -6.465836, T: 1590400, Avg. loss: 0.572585\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 20.79, NNZs: 106, Bias: -6.478345, T: 1601600, Avg. loss: 0.573431\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 20.73, NNZs: 106, Bias: -6.490780, T: 1612800, Avg. loss: 0.575767\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 20.69, NNZs: 106, Bias: -6.496976, T: 1624000, Avg. loss: 0.572535\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 20.66, NNZs: 106, Bias: -6.509292, T: 1635200, Avg. loss: 0.564704\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 20.60, NNZs: 106, Bias: -6.533670, T: 1646400, Avg. loss: 0.563650\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 20.56, NNZs: 106, Bias: -6.551789, T: 1657600, Avg. loss: 0.566373\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 20.49, NNZs: 106, Bias: -6.557769, T: 1668800, Avg. loss: 0.562744\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 20.45, NNZs: 106, Bias: -6.569700, T: 1680000, Avg. loss: 0.554991\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 20.40, NNZs: 106, Bias: -6.581580, T: 1691200, Avg. loss: 0.559935\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 20.35, NNZs: 106, Bias: -6.605137, T: 1702400, Avg. loss: 0.553640\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 20.33, NNZs: 106, Bias: -6.622669, T: 1713600, Avg. loss: 0.557128\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 20.28, NNZs: 106, Bias: -6.628476, T: 1724800, Avg. loss: 0.545010\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 20.25, NNZs: 106, Bias: -6.645823, T: 1736000, Avg. loss: 0.546638\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 20.22, NNZs: 106, Bias: -6.668760, T: 1747200, Avg. loss: 0.536654\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 20.18, NNZs: 106, Bias: -6.674424, T: 1758400, Avg. loss: 0.544420\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 20.14, NNZs: 106, Bias: -6.685774, T: 1769600, Avg. loss: 0.536180\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 20.06, NNZs: 106, Bias: -6.702676, T: 1780800, Avg. loss: 0.542846\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 20.03, NNZs: 106, Bias: -6.719445, T: 1792000, Avg. loss: 0.533037\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 19.98, NNZs: 106, Bias: -6.724991, T: 1803200, Avg. loss: 0.536330\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 19.92, NNZs: 106, Bias: -6.736047, T: 1814400, Avg. loss: 0.534762\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 19.89, NNZs: 106, Bias: -6.752525, T: 1825600, Avg. loss: 0.535507\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 19.88, NNZs: 106, Bias: -6.757991, T: 1836800, Avg. loss: 0.525636\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 19.82, NNZs: 106, Bias: -6.779694, T: 1848000, Avg. loss: 0.527667\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 19.80, NNZs: 106, Bias: -6.790481, T: 1859200, Avg. loss: 0.533656\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 19.76, NNZs: 106, Bias: -6.806554, T: 1870400, Avg. loss: 0.524750\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 19.72, NNZs: 106, Bias: -6.811903, T: 1881600, Avg. loss: 0.523599\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 19.68, NNZs: 106, Bias: -6.827807, T: 1892800, Avg. loss: 0.521441\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 19.66, NNZs: 106, Bias: -6.848857, T: 1904000, Avg. loss: 0.513761\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 19.61, NNZs: 106, Bias: -6.854064, T: 1915200, Avg. loss: 0.514273\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 19.56, NNZs: 106, Bias: -6.869661, T: 1926400, Avg. loss: 0.514752\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 19.56, NNZs: 106, Bias: -6.879971, T: 1937600, Avg. loss: 0.505775\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 19.52, NNZs: 106, Bias: -6.885109, T: 1948800, Avg. loss: 0.520772\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 19.47, NNZs: 106, Bias: -6.900455, T: 1960000, Avg. loss: 0.517911\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 19.43, NNZs: 106, Bias: -6.905547, T: 1971200, Avg. loss: 0.516135\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 19.40, NNZs: 106, Bias: -6.920730, T: 1982400, Avg. loss: 0.502175\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 19.35, NNZs: 106, Bias: -6.935808, T: 1993600, Avg. loss: 0.510185\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 19.30, NNZs: 106, Bias: -6.945807, T: 2004800, Avg. loss: 0.504250\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 19.27, NNZs: 106, Bias: -6.960742, T: 2016000, Avg. loss: 0.510921\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 19.22, NNZs: 106, Bias: -6.975586, T: 2027200, Avg. loss: 0.503202\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 19.17, NNZs: 106, Bias: -6.980524, T: 2038400, Avg. loss: 0.512559\n",
      "Total training time: 0.74 seconds.\n",
      "Convergence after 182 epochs took 0.74 seconds\n",
      "-- Epoch 1\n",
      "Norm: 309.82, NNZs: 106, Bias: 14.479962, T: 11200, Avg. loss: 262.023956\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 196.16, NNZs: 106, Bias: 15.899426, T: 22400, Avg. loss: 62.637228\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 162.34, NNZs: 106, Bias: 17.653841, T: 33600, Avg. loss: 37.861800\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 136.66, NNZs: 106, Bias: 17.625549, T: 44800, Avg. loss: 26.807789\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 116.22, NNZs: 106, Bias: 18.226485, T: 56000, Avg. loss: 20.964977\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 105.51, NNZs: 105, Bias: 18.718812, T: 67200, Avg. loss: 17.485964\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 96.51, NNZs: 106, Bias: 18.998376, T: 78400, Avg. loss: 14.745241\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 89.70, NNZs: 106, Bias: 19.469849, T: 89600, Avg. loss: 12.876964\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 84.83, NNZs: 106, Bias: 19.676907, T: 100800, Avg. loss: 11.448108\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 80.01, NNZs: 106, Bias: 19.862420, T: 112000, Avg. loss: 10.194661\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 76.93, NNZs: 106, Bias: 20.033587, T: 123200, Avg. loss: 9.064992\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 73.72, NNZs: 106, Bias: 20.268999, T: 134400, Avg. loss: 8.499536\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 70.62, NNZs: 106, Bias: 20.479511, T: 145600, Avg. loss: 7.927285\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 68.06, NNZs: 106, Bias: 20.608974, T: 156800, Avg. loss: 7.161860\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 66.71, NNZs: 106, Bias: 20.731585, T: 168000, Avg. loss: 6.696888\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 65.28, NNZs: 106, Bias: 20.850183, T: 179200, Avg. loss: 6.387953\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 63.59, NNZs: 106, Bias: 21.067249, T: 190400, Avg. loss: 5.972041\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 62.11, NNZs: 106, Bias: 21.219230, T: 201600, Avg. loss: 5.661118\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 60.54, NNZs: 106, Bias: 21.361624, T: 212800, Avg. loss: 5.513760\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 59.77, NNZs: 106, Bias: 21.450599, T: 224000, Avg. loss: 5.229561\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 58.37, NNZs: 106, Bias: 21.493473, T: 235200, Avg. loss: 4.911492\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 57.44, NNZs: 106, Bias: 21.618962, T: 246400, Avg. loss: 4.656167\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 56.49, NNZs: 106, Bias: 21.777927, T: 257600, Avg. loss: 4.553474\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 55.66, NNZs: 106, Bias: 21.890371, T: 268800, Avg. loss: 4.359947\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 54.84, NNZs: 106, Bias: 21.926497, T: 280000, Avg. loss: 4.206987\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 54.11, NNZs: 106, Bias: 22.031623, T: 291200, Avg. loss: 4.077021\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 53.30, NNZs: 106, Bias: 22.132180, T: 302400, Avg. loss: 3.988079\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 52.75, NNZs: 106, Bias: 22.228512, T: 313600, Avg. loss: 3.760181\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 52.11, NNZs: 106, Bias: 22.290897, T: 324800, Avg. loss: 3.632083\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 51.41, NNZs: 106, Bias: 22.381837, T: 336000, Avg. loss: 3.568264\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 50.99, NNZs: 106, Bias: 22.498336, T: 347200, Avg. loss: 3.461815\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 50.24, NNZs: 106, Bias: 22.610594, T: 358400, Avg. loss: 3.394201\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 49.73, NNZs: 106, Bias: 22.664665, T: 369600, Avg. loss: 3.265378\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 48.99, NNZs: 106, Bias: 22.744141, T: 380800, Avg. loss: 3.234162\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 48.46, NNZs: 106, Bias: 22.795733, T: 392000, Avg. loss: 3.096491\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 48.07, NNZs: 106, Bias: 22.896022, T: 403200, Avg. loss: 3.024809\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 47.66, NNZs: 106, Bias: 22.920825, T: 414400, Avg. loss: 2.989328\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 47.08, NNZs: 106, Bias: 23.039911, T: 425600, Avg. loss: 2.914548\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 46.43, NNZs: 106, Bias: 23.086057, T: 436800, Avg. loss: 2.853629\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 46.09, NNZs: 106, Bias: 23.154013, T: 448000, Avg. loss: 2.770000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 45.49, NNZs: 106, Bias: 23.264021, T: 459200, Avg. loss: 2.739491\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 45.15, NNZs: 106, Bias: 23.328075, T: 470400, Avg. loss: 2.670037\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 44.79, NNZs: 106, Bias: 23.369979, T: 481600, Avg. loss: 2.605332\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 44.34, NNZs: 106, Bias: 23.431512, T: 492800, Avg. loss: 2.559369\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 43.96, NNZs: 106, Bias: 23.511425, T: 504000, Avg. loss: 2.522907\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 43.50, NNZs: 106, Bias: 23.589311, T: 515200, Avg. loss: 2.488967\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 43.09, NNZs: 106, Bias: 23.608316, T: 526400, Avg. loss: 2.429269\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 42.78, NNZs: 106, Bias: 23.664719, T: 537600, Avg. loss: 2.399804\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 42.27, NNZs: 106, Bias: 23.720155, T: 548800, Avg. loss: 2.377310\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 41.98, NNZs: 106, Bias: 23.774374, T: 560000, Avg. loss: 2.344979\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 41.66, NNZs: 106, Bias: 23.844981, T: 571200, Avg. loss: 2.262151\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 41.39, NNZs: 106, Bias: 23.914065, T: 582400, Avg. loss: 2.282631\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 41.04, NNZs: 106, Bias: 23.964837, T: 593600, Avg. loss: 2.225671\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 40.77, NNZs: 106, Bias: 24.014621, T: 604800, Avg. loss: 2.199279\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 40.53, NNZs: 106, Bias: 24.063558, T: 616000, Avg. loss: 2.193347\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 40.29, NNZs: 106, Bias: 24.127716, T: 627200, Avg. loss: 2.183738\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 39.91, NNZs: 106, Bias: 24.159405, T: 638400, Avg. loss: 2.092718\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 39.52, NNZs: 106, Bias: 24.221532, T: 649600, Avg. loss: 2.090521\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 39.13, NNZs: 106, Bias: 24.252005, T: 660800, Avg. loss: 2.045851\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 38.87, NNZs: 106, Bias: 24.311983, T: 672000, Avg. loss: 2.019485\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 38.60, NNZs: 106, Bias: 24.341537, T: 683200, Avg. loss: 2.013455\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 38.30, NNZs: 106, Bias: 24.385249, T: 694400, Avg. loss: 1.949234\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 38.06, NNZs: 106, Bias: 24.442364, T: 705600, Avg. loss: 1.966223\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 37.81, NNZs: 106, Bias: 24.484324, T: 716800, Avg. loss: 1.939000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 37.59, NNZs: 106, Bias: 24.525745, T: 728000, Avg. loss: 1.895186\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 37.32, NNZs: 106, Bias: 24.593682, T: 739200, Avg. loss: 1.868636\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 37.06, NNZs: 106, Bias: 24.633694, T: 750400, Avg. loss: 1.870090\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 36.85, NNZs: 106, Bias: 24.659905, T: 761600, Avg. loss: 1.855927\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 36.56, NNZs: 106, Bias: 24.698959, T: 772800, Avg. loss: 1.826252\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 36.39, NNZs: 106, Bias: 24.750333, T: 784000, Avg. loss: 1.815736\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 36.11, NNZs: 106, Bias: 24.788151, T: 795200, Avg. loss: 1.811589\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 35.95, NNZs: 106, Bias: 24.825435, T: 806400, Avg. loss: 1.784506\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 35.76, NNZs: 106, Bias: 24.850040, T: 817600, Avg. loss: 1.783854\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 35.61, NNZs: 106, Bias: 24.886554, T: 828800, Avg. loss: 1.725750\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 35.37, NNZs: 106, Bias: 24.934442, T: 840000, Avg. loss: 1.741164\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 35.18, NNZs: 106, Bias: 24.957973, T: 851200, Avg. loss: 1.719677\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 34.92, NNZs: 106, Bias: 24.981366, T: 862400, Avg. loss: 1.714370\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 34.72, NNZs: 106, Bias: 25.038942, T: 873600, Avg. loss: 1.679396\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 34.53, NNZs: 106, Bias: 25.061630, T: 884800, Avg. loss: 1.657246\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 34.35, NNZs: 106, Bias: 25.106528, T: 896000, Avg. loss: 1.636296\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 34.09, NNZs: 106, Bias: 25.150769, T: 907200, Avg. loss: 1.652811\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 33.96, NNZs: 106, Bias: 25.172570, T: 918400, Avg. loss: 1.631516\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 33.73, NNZs: 106, Bias: 25.204899, T: 929600, Avg. loss: 1.639426\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 33.56, NNZs: 106, Bias: 25.226215, T: 940800, Avg. loss: 1.586692\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 33.40, NNZs: 106, Bias: 25.268311, T: 952000, Avg. loss: 1.588915\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 33.25, NNZs: 106, Bias: 25.268337, T: 963200, Avg. loss: 1.583058\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 33.07, NNZs: 106, Bias: 25.319949, T: 974400, Avg. loss: 1.573093\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 32.90, NNZs: 106, Bias: 25.360650, T: 985600, Avg. loss: 1.548067\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 32.72, NNZs: 106, Bias: 25.390791, T: 996800, Avg. loss: 1.537769\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 32.56, NNZs: 106, Bias: 25.400766, T: 1008000, Avg. loss: 1.545749\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 32.43, NNZs: 106, Bias: 25.440217, T: 1019200, Avg. loss: 1.535919\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 32.28, NNZs: 106, Bias: 25.479153, T: 1030400, Avg. loss: 1.509974\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 32.15, NNZs: 106, Bias: 25.498424, T: 1041600, Avg. loss: 1.497179\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 32.00, NNZs: 106, Bias: 25.527056, T: 1052800, Avg. loss: 1.494674\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 31.86, NNZs: 106, Bias: 25.545942, T: 1064000, Avg. loss: 1.500771\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 31.70, NNZs: 106, Bias: 25.583318, T: 1075200, Avg. loss: 1.486716\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 31.59, NNZs: 106, Bias: 25.592569, T: 1086400, Avg. loss: 1.461690\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 31.46, NNZs: 106, Bias: 25.620051, T: 1097600, Avg. loss: 1.477541\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 31.29, NNZs: 106, Bias: 25.656294, T: 1108800, Avg. loss: 1.451064\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 31.13, NNZs: 106, Bias: 25.692098, T: 1120000, Avg. loss: 1.436499\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 31.01, NNZs: 106, Bias: 25.709815, T: 1131200, Avg. loss: 1.462816\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 30.88, NNZs: 106, Bias: 25.727421, T: 1142400, Avg. loss: 1.400319\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 30.72, NNZs: 106, Bias: 25.753601, T: 1153600, Avg. loss: 1.405017\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 30.63, NNZs: 106, Bias: 25.779507, T: 1164800, Avg. loss: 1.390587\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 30.52, NNZs: 106, Bias: 25.796657, T: 1176000, Avg. loss: 1.400641\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 30.40, NNZs: 106, Bias: 25.822121, T: 1187200, Avg. loss: 1.389567\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 30.26, NNZs: 106, Bias: 25.855650, T: 1198400, Avg. loss: 1.390176\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 30.19, NNZs: 106, Bias: 25.863994, T: 1209600, Avg. loss: 1.373715\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 30.09, NNZs: 106, Bias: 25.896950, T: 1220800, Avg. loss: 1.361642\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 29.96, NNZs: 106, Bias: 25.929525, T: 1232000, Avg. loss: 1.356676\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 29.84, NNZs: 106, Bias: 25.945679, T: 1243200, Avg. loss: 1.343838\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 29.77, NNZs: 106, Bias: 25.969690, T: 1254400, Avg. loss: 1.333393\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 29.71, NNZs: 106, Bias: 25.993451, T: 1265600, Avg. loss: 1.328739\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 29.61, NNZs: 106, Bias: 26.017000, T: 1276800, Avg. loss: 1.324795\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 29.50, NNZs: 106, Bias: 26.032548, T: 1288000, Avg. loss: 1.312950\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 29.41, NNZs: 106, Bias: 26.048052, T: 1299200, Avg. loss: 1.325773\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 29.29, NNZs: 106, Bias: 26.086352, T: 1310400, Avg. loss: 1.319256\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 29.19, NNZs: 106, Bias: 26.101496, T: 1321600, Avg. loss: 1.315295\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 29.07, NNZs: 106, Bias: 26.124091, T: 1332800, Avg. loss: 1.298717\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 28.97, NNZs: 106, Bias: 26.139059, T: 1344000, Avg. loss: 1.318262\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 28.89, NNZs: 106, Bias: 26.161334, T: 1355200, Avg. loss: 1.281831\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 28.80, NNZs: 106, Bias: 26.190745, T: 1366400, Avg. loss: 1.273092\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 28.70, NNZs: 106, Bias: 26.212623, T: 1377600, Avg. loss: 1.280090\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 28.61, NNZs: 106, Bias: 26.234292, T: 1388800, Avg. loss: 1.268996\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 28.52, NNZs: 106, Bias: 26.255776, T: 1400000, Avg. loss: 1.251223\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 28.45, NNZs: 106, Bias: 26.262899, T: 1411200, Avg. loss: 1.242351\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 28.35, NNZs: 106, Bias: 26.284104, T: 1422400, Avg. loss: 1.243363\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 28.27, NNZs: 106, Bias: 26.298104, T: 1433600, Avg. loss: 1.241111\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 28.21, NNZs: 106, Bias: 26.318958, T: 1444800, Avg. loss: 1.236690\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 28.08, NNZs: 106, Bias: 26.346523, T: 1456000, Avg. loss: 1.228851\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 28.02, NNZs: 106, Bias: 26.373823, T: 1467200, Avg. loss: 1.228796\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 27.98, NNZs: 106, Bias: 26.380584, T: 1478400, Avg. loss: 1.248999\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 27.89, NNZs: 106, Bias: 26.407549, T: 1489600, Avg. loss: 1.224576\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 27.80, NNZs: 106, Bias: 26.427578, T: 1500800, Avg. loss: 1.228107\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 27.71, NNZs: 106, Bias: 26.454056, T: 1512000, Avg. loss: 1.206065\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 27.65, NNZs: 106, Bias: 26.454035, T: 1523200, Avg. loss: 1.212541\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 27.54, NNZs: 106, Bias: 26.480187, T: 1534400, Avg. loss: 1.191775\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 27.45, NNZs: 106, Bias: 26.493160, T: 1545600, Avg. loss: 1.199021\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 27.39, NNZs: 106, Bias: 26.506044, T: 1556800, Avg. loss: 1.202231\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 27.33, NNZs: 106, Bias: 26.518862, T: 1568000, Avg. loss: 1.181129\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 27.27, NNZs: 106, Bias: 26.537928, T: 1579200, Avg. loss: 1.181690\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 27.19, NNZs: 106, Bias: 26.556866, T: 1590400, Avg. loss: 1.166413\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 27.11, NNZs: 106, Bias: 26.575661, T: 1601600, Avg. loss: 1.167324\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 27.03, NNZs: 106, Bias: 26.594315, T: 1612800, Avg. loss: 1.177819\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 26.97, NNZs: 106, Bias: 26.606673, T: 1624000, Avg. loss: 1.161819\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 26.90, NNZs: 106, Bias: 26.631187, T: 1635200, Avg. loss: 1.165509\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 26.85, NNZs: 106, Bias: 26.643341, T: 1646400, Avg. loss: 1.157716\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 26.79, NNZs: 106, Bias: 26.649397, T: 1657600, Avg. loss: 1.149181\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 26.69, NNZs: 106, Bias: 26.667452, T: 1668800, Avg. loss: 1.155199\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 26.63, NNZs: 106, Bias: 26.691320, T: 1680000, Avg. loss: 1.139241\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 26.57, NNZs: 106, Bias: 26.697250, T: 1691200, Avg. loss: 1.146640\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 26.53, NNZs: 106, Bias: 26.720804, T: 1702400, Avg. loss: 1.131546\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 26.47, NNZs: 106, Bias: 26.744163, T: 1713600, Avg. loss: 1.137533\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 26.39, NNZs: 106, Bias: 26.749949, T: 1724800, Avg. loss: 1.129589\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 26.34, NNZs: 106, Bias: 26.761505, T: 1736000, Avg. loss: 1.109337\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 26.26, NNZs: 106, Bias: 26.784447, T: 1747200, Avg. loss: 1.119740\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 26.20, NNZs: 106, Bias: 26.790138, T: 1758400, Avg. loss: 1.119683\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 26.14, NNZs: 106, Bias: 26.801494, T: 1769600, Avg. loss: 1.118582\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 26.09, NNZs: 106, Bias: 26.818425, T: 1780800, Avg. loss: 1.108875\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 26.03, NNZs: 106, Bias: 26.846373, T: 1792000, Avg. loss: 1.121889\n",
      "Total training time: 0.74 seconds.\n",
      "Convergence after 160 epochs took 0.74 seconds\n",
      "Accuracy: 0.595\n",
      "CPU times: user 2.71 s, sys: 83.6 ms, total: 2.8 s\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_model = SGDClassifier(n_jobs=jobs, random_state=42, verbose=1)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred = sgd_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonparallelized SVC that scales quadratically with samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]iter  1 act 5.717e+03 pre 5.717e+03 delta 3.125e-02 f 1.120e+04 |g| 3.659e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 7.514e+01 pre 7.514e+01 delta 1.250e-01 f 5.483e+03 |g| 2.533e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 2.280e+02 pre 2.280e+02 delta 3.015e-01 f 5.407e+03 |g| 3.406e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 1.890e+02 pre 1.812e+02 delta 3.188e-01 f 5.179e+03 |g| 1.842e+03 CG   4\n",
      "iter  5 act 3.599e-01 pre 3.596e-01 delta 3.188e-01 f 4.990e+03 |g| 2.782e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 3.358e+01 pre 3.222e+01 delta 4.016e-01 f 4.990e+03 |g| 2.519e+02 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter  7 act 3.143e+01 pre 3.193e+01 delta 4.504e-01 f 4.957e+03 |g| 9.499e+02 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 3.334e+01 pre 3.115e+01 delta 5.509e-01 f 4.925e+03 |g| 6.184e+02 CG   5\n",
      "iter  9 act 5.752e+00 pre 5.731e+00 delta 5.509e-01 f 4.892e+03 |g| 1.244e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 2.143e+01 pre 2.114e+01 delta 6.645e-01 f 4.886e+03 |g| 1.996e+02 CG   5\n",
      "iter 11 act 6.503e+00 pre 6.364e+00 delta 6.645e-01 f 4.865e+03 |g| 5.494e+02 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter 12 act 1.231e+01 pre 1.186e+01 delta 7.407e-01 f 4.858e+03 |g| 2.721e+02 CG   7\n",
      "iter 13 act 1.127e+00 pre 1.127e+00 delta 7.407e-01 f 4.846e+03 |g| 4.013e+02 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 5.243e+00 pre 5.151e+00 delta 8.206e-01 f 4.845e+03 |g| 4.688e+01 CG   7\n",
      "iter 15 act 4.431e+00 pre 4.566e+00 delta 8.206e-01 f 4.839e+03 |g| 1.393e+02 CG  12\n",
      "iter 16 act 1.985e-03 pre 1.985e-03 delta 8.206e-01 f 4.835e+03 |g| 1.936e+02 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 17 act 4.367e+00 pre 4.323e+00 delta 1.051e+00 f 4.835e+03 |g| 1.476e+01 CG  13\n",
      "iter 18 act 2.909e-01 pre 2.906e-01 delta 1.051e+00 f 4.831e+03 |g| 9.944e+01 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 19 act 2.322e+00 pre 2.327e+00 delta 1.105e+00 f 4.830e+03 |g| 1.168e+01 CG  13\n",
      "iter 20 act 5.387e-01 pre 5.394e-01 delta 1.105e+00 f 4.828e+03 |g| 7.377e+01 CG   3\n",
      "iter 21 act 8.284e-01 pre 8.217e-01 delta 1.105e+00 f 4.827e+03 |g| 9.908e+00 CG  17\n",
      "iter 22 act 3.065e-05 pre 3.065e-05 delta 1.105e+00 f 4.827e+03 |g| 2.390e+01 CG   1\n",
      "iter  1 act 5.683e+03 pre 5.683e+03 delta 3.115e-02 f 1.120e+04 |g| 3.648e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 1.107e+02 pre 1.107e+02 delta 1.246e-01 f 5.517e+03 |g| 3.699e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 3.663e+02 pre 3.663e+02 delta 4.144e-01 f 5.406e+03 |g| 5.029e+03 CG   2\n",
      "iter  4 act 4.469e+02 pre 4.180e+02 delta 4.144e-01 f 5.040e+03 |g| 3.335e+03 CG   3\n",
      "iter  5 act 2.922e+00 pre 2.902e+00 delta 4.144e-01 f 4.593e+03 |g| 7.613e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 4.363e+01 pre 3.916e+01 delta 4.847e-01 f 4.590e+03 |g| 4.153e+02 CG   7\n",
      "iter  7 act 5.751e+00 pre 5.615e+00 delta 4.847e-01 f 4.547e+03 |g| 1.523e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 1.858e+01 pre 1.864e+01 delta 5.994e-01 f 4.541e+03 |g| 2.483e+02 CG   4\n",
      "iter  9 act 5.614e+00 pre 5.522e+00 delta 5.994e-01 f 4.522e+03 |g| 5.232e+02 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 1.142e+01 pre 1.101e+01 delta 6.236e-01 f 4.517e+03 |g| 2.282e+02 CG   7\n",
      "iter 11 act 1.723e+00 pre 1.709e+00 delta 6.236e-01 f 4.505e+03 |g| 5.992e+02 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 12 act 5.032e+00 pre 4.928e+00 delta 6.413e-01 f 4.504e+03 |g| 8.138e+01 CG  10\n",
      "iter 13 act 2.603e-03 pre 2.602e-03 delta 6.413e-01 f 4.499e+03 |g| 2.067e+02 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 3.354e+00 pre 3.424e+00 delta 6.515e-01 f 4.499e+03 |g| 1.739e+01 CG  10\n",
      "cg reaches trust region boundary\n",
      "iter 15 act 3.250e+00 pre 3.190e+00 delta 6.884e-01 f 4.495e+03 |g| 7.740e+01 CG  14\n",
      "iter 16 act 6.234e-01 pre 6.230e-01 delta 6.884e-01 f 4.492e+03 |g| 1.203e+02 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 17 act 2.342e+00 pre 2.362e+00 delta 7.411e-01 f 4.491e+03 |g| 1.980e+01 CG  13\n",
      "iter 18 act 1.429e+00 pre 1.410e+00 delta 7.411e-01 f 4.489e+03 |g| 8.456e+01 CG  12\n",
      "cg reaches trust region boundary\n",
      "iter 19 act 2.036e+00 pre 2.045e+00 delta 9.846e-01 f 4.488e+03 |g| 5.226e+01 CG  13\n",
      "iter 20 act 6.921e-01 pre 6.830e-01 delta 9.846e-01 f 4.486e+03 |g| 5.226e+01 CG  12\n",
      "cg reaches trust region boundary\n",
      "iter 21 act 1.351e+00 pre 1.356e+00 delta 9.846e-01 f 4.485e+03 |g| 3.010e+01 CG  14\n",
      "iter 22 act 4.893e-01 pre 4.878e-01 delta 9.846e-01 f 4.483e+03 |g| 6.473e+01 CG   4\n",
      "iter 23 act 5.753e-01 pre 5.747e-01 delta 9.846e-01 f 4.483e+03 |g| 1.780e+01 CG  17\n",
      "iter 24 act 2.357e-04 pre 2.357e-04 delta 9.846e-01 f 4.482e+03 |g| 5.610e+00 CG   2\n",
      "iter  1 act 5.731e+03 pre 5.731e+03 delta 3.129e-02 f 1.120e+04 |g| 3.664e+05 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 9.148e+01 pre 9.148e+01 delta 1.251e-01 f 5.469e+03 |g| 3.034e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 3.080e+02 pre 3.080e+02 delta 4.583e-01 f 5.377e+03 |g| 4.139e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 5.529e+02 pre 5.012e+02 delta 5.539e-01 f 5.069e+03 |g| 2.869e+03 CG   3\n",
      "iter  5 act 5.378e+00 pre 5.331e+00 delta 5.539e-01 f 4.516e+03 |g| 1.030e+04 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 1.544e+02 pre 1.466e+02 delta 5.916e-01 f 4.511e+03 |g| 6.398e+02 CG   4\n",
      "iter  7 act 8.136e-01 pre 8.103e-01 delta 5.916e-01 f 4.356e+03 |g| 3.759e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 7.762e+01 pre 7.503e+01 delta 1.010e+00 f 4.356e+03 |g| 3.478e+02 CG   4\n",
      "iter  9 act 7.291e+00 pre 7.144e+00 delta 1.010e+00 f 4.278e+03 |g| 1.511e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 5.870e+01 pre 5.889e+01 delta 1.079e+00 f 4.271e+03 |g| 3.179e+02 CG   3\n",
      "iter 11 act 3.288e+01 pre 3.164e+01 delta 1.079e+00 f 4.212e+03 |g| 1.454e+03 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 12 act 5.531e+01 pre 5.366e+01 delta 1.784e+00 f 4.179e+03 |g| 9.251e+02 CG   4\n",
      "iter 13 act 7.487e+00 pre 7.433e+00 delta 1.784e+00 f 4.124e+03 |g| 9.703e+02 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 14 act 4.949e+01 pre 4.802e+01 delta 2.125e+00 f 4.116e+03 |g| 2.283e+02 CG   5\n",
      "iter 15 act 8.424e+00 pre 8.337e+00 delta 2.125e+00 f 4.067e+03 |g| 1.217e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 16 act 4.109e+01 pre 4.076e+01 delta 3.480e+00 f 4.058e+03 |g| 2.650e+02 CG   7\n",
      "iter 17 act 4.842e+00 pre 4.796e+00 delta 3.480e+00 f 4.017e+03 |g| 4.128e+02 CG   3\n",
      "iter 18 act 2.732e+01 pre 2.758e+01 delta 3.480e+00 f 4.012e+03 |g| 1.358e+02 CG  11\n",
      "iter 19 act 1.214e-02 pre 1.214e-02 delta 3.480e+00 f 3.985e+03 |g| 4.115e+02 CG   1\n",
      "iter 20 act 6.526e+00 pre 6.419e+00 delta 3.480e+00 f 3.985e+03 |g| 1.784e+01 CG  23\n",
      "iter 21 act 1.274e-03 pre 1.274e-03 delta 3.480e+00 f 3.979e+03 |g| 1.324e+02 CG   1\n",
      "iter  1 act 2.278e+02 pre 2.278e+02 delta 6.248e-03 f 1.120e+04 |g| 7.291e+04 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  2 act 2.612e+01 pre 2.612e+01 delta 2.499e-02 f 1.097e+04 |g| 4.329e+03 CG   1\n",
      "cg reaches trust region boundary\n",
      "iter  3 act 1.048e+02 pre 1.048e+02 delta 9.997e-02 f 1.095e+04 |g| 5.915e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  4 act 3.801e+02 pre 3.801e+02 delta 3.999e-01 f 1.084e+04 |g| 5.967e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  5 act 9.292e+02 pre 9.292e+02 delta 5.964e-01 f 1.046e+04 |g| 4.818e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  6 act 6.508e+02 pre 6.505e+02 delta 8.177e-01 f 9.532e+03 |g| 2.103e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter  7 act 2.372e+02 pre 2.366e+02 delta 9.855e-01 f 8.881e+03 |g| 8.963e+02 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter  8 act 1.851e+02 pre 1.861e+02 delta 1.132e+00 f 8.644e+03 |g| 5.166e+02 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter  9 act 1.812e+02 pre 1.777e+02 delta 1.332e+00 f 8.459e+03 |g| 5.740e+02 CG   5\n",
      "cg reaches trust region boundary\n",
      "iter 10 act 1.408e+02 pre 1.403e+02 delta 1.636e+00 f 8.278e+03 |g| 1.024e+03 CG   7\n",
      "cg reaches trust region boundary\n",
      "iter 11 act 1.001e+02 pre 9.828e+01 delta 2.373e+00 f 8.137e+03 |g| 4.331e+02 CG   7\n",
      "iter 12 act 6.500e+00 pre 6.496e+00 delta 2.373e+00 f 8.037e+03 |g| 1.410e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 13 act 8.322e+01 pre 8.357e+01 delta 3.581e+00 f 8.030e+03 |g| 1.073e+02 CG   7\n",
      "iter 14 act 1.300e+01 pre 1.297e+01 delta 3.581e+00 f 7.947e+03 |g| 3.185e+03 CG   2\n",
      "cg reaches trust region boundary\n",
      "iter 15 act 7.315e+01 pre 7.188e+01 delta 3.791e+00 f 7.934e+03 |g| 1.810e+02 CG   9\n",
      "iter 16 act 4.870e+00 pre 4.851e+00 delta 3.791e+00 f 7.861e+03 |g| 5.963e+02 CG   3\n",
      "cg reaches trust region boundary\n",
      "iter 17 act 4.047e+01 pre 4.076e+01 delta 3.791e+00 f 7.856e+03 |g| 6.563e+01 CG  14\n",
      "iter 18 act 6.229e+00 pre 6.229e+00 delta 3.791e+00 f 7.816e+03 |g| 2.816e+02 CG   4\n",
      "cg reaches trust region boundary\n",
      "iter 19 act 1.604e+01 pre 1.606e+01 delta 3.840e+00 f 7.809e+03 |g| 6.110e+01 CG  21\n",
      "iter 20 act 1.114e+00 pre 1.112e+00 delta 3.840e+00 f 7.793e+03 |g| 9.429e+02 CG   2\n",
      "iter 21 act 9.358e-01 pre 9.302e-01 delta 3.840e+00 f 7.792e+03 |g| 2.652e+01 CG  17\n",
      "iter 22 act 5.236e-01 pre 5.236e-01 delta 3.840e+00 f 7.791e+03 |g| 8.062e+00 CG  23\n",
      "Accuracy: 0.6378571428571429\n",
      "CPU times: user 2.67 s, sys: 97.8 ms, total: 2.77 s\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.svm import LinearSVC\n",
    "svc_model = LinearSVC(random_state=42, verbose=1)\n",
    "svc_model.fit(X_train, y_train)\n",
    "y_pred = svc_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "937q3D9VAzlM"
   },
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Perform hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMKS-cKqbkL0",
    "outputId": "f1e56f55-3340-423b-8fe5-ac06e95cb71c"
   },
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Perform hyperparameter tuning with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=5, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDFlld25Xze5"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-entropy was previously calculated, no need to recompute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6096428571428572\n",
      "CPU times: user 3.83 s, sys: 103 ms, total: 3.93 s\n",
      "Wall time: 4.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=jobs, verbose=1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFkkQCBxXlWh",
    "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "min_entropy_train = vectorized_entropy(X_train)\n",
    "min_entropy_test = vectorized_entropy(X_test)\n",
    "\n",
    "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
    "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
    "# Create the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_with_entropy, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
    "\n",
    "# Calculate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KULNfXpncJZC",
    "outputId": "f44dab80-2cb1-46c1-ab5f-a26725d94c9c"
   },
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['auto', 'sqrt'],       # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_rf = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DfNGSyaX3kV"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1081           54.22s\n",
      "         2           1.0732           46.32s\n",
      "         3           1.0457           43.20s\n",
      "         4           1.0232           40.03s\n",
      "         5           1.0045           38.48s\n",
      "         6           0.9889           36.93s\n",
      "         7           0.9760           35.85s\n",
      "         8           0.9650           34.81s\n",
      "         9           0.9558           33.92s\n",
      "        10           0.9479           33.16s\n",
      "        20           0.9029           28.39s\n",
      "        30           0.8810           24.52s\n",
      "        40           0.8637           20.64s\n",
      "        50           0.8486           17.10s\n",
      "        60           0.8348           13.67s\n",
      "        70           0.8215           10.23s\n",
      "        80           0.8098            6.82s\n",
      "        90           0.7992            3.40s\n",
      "       100           0.7890            0.00s\n",
      "Accuracy: 0.6303571428571428\n",
      "CPU times: user 33.5 s, sys: 107 ms, total: 33.6 s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42, verbose=1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred = gb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parallelized version is available as Histogram Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.009 GB of training data: 0.093 s\n",
      "Binning 0.001 GB of validation data: 0.003 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 1.07446, val loss: 1.08688, in 0.093s\n",
      "[2/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 1.01973, val loss: 1.04399, in 0.081s\n",
      "[3/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.97837, val loss: 1.01259, in 0.072s\n",
      "[4/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.94601, val loss: 0.98833, in 0.079s\n",
      "[5/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.91969, val loss: 0.97225, in 0.059s\n",
      "[6/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.89738, val loss: 0.95857, in 0.079s\n",
      "[7/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.87878, val loss: 0.94832, in 0.069s\n",
      "[8/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.86165, val loss: 0.93910, in 0.063s\n",
      "[9/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.84624, val loss: 0.93168, in 0.073s\n",
      "[10/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.83228, val loss: 0.92635, in 0.070s\n",
      "[11/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.81954, val loss: 0.92277, in 0.069s\n",
      "[12/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.80691, val loss: 0.91688, in 0.086s\n",
      "[13/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.79585, val loss: 0.91452, in 0.065s\n",
      "[14/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.78448, val loss: 0.91207, in 0.071s\n",
      "[15/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.77375, val loss: 0.90905, in 0.074s\n",
      "[16/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.76365, val loss: 0.90669, in 0.063s\n",
      "[17/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.75403, val loss: 0.90461, in 0.070s\n",
      "[18/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.74435, val loss: 0.90429, in 0.079s\n",
      "[19/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.73455, val loss: 0.90321, in 0.081s\n",
      "[20/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.72576, val loss: 0.90146, in 0.073s\n",
      "[21/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.71709, val loss: 0.90120, in 0.059s\n",
      "[22/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.70820, val loss: 0.90141, in 0.074s\n",
      "[23/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.69946, val loss: 0.89965, in 0.067s\n",
      "[24/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.69128, val loss: 0.89721, in 0.063s\n",
      "[25/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.68342, val loss: 0.89733, in 0.059s\n",
      "[26/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.67565, val loss: 0.89668, in 0.065s\n",
      "[27/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.66799, val loss: 0.89585, in 0.059s\n",
      "[28/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.66068, val loss: 0.89509, in 0.065s\n",
      "[29/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.65313, val loss: 0.89426, in 0.055s\n",
      "[30/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.64572, val loss: 0.89337, in 0.067s\n",
      "[31/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.63908, val loss: 0.89286, in 0.063s\n",
      "[32/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.63211, val loss: 0.89256, in 0.058s\n",
      "[33/100] 4 trees, 124 leaves (31 on avg), max depth = 14, train loss: 0.62479, val loss: 0.89203, in 0.067s\n",
      "[34/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.61808, val loss: 0.89148, in 0.078s\n",
      "[35/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.61134, val loss: 0.88995, in 0.076s\n",
      "[36/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.60458, val loss: 0.88994, in 0.080s\n",
      "[37/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.59832, val loss: 0.88956, in 0.073s\n",
      "[38/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.59218, val loss: 0.88990, in 0.061s\n",
      "[39/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.58563, val loss: 0.89116, in 0.070s\n",
      "[40/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.57961, val loss: 0.89035, in 0.055s\n",
      "[41/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.57376, val loss: 0.89015, in 0.063s\n",
      "[42/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.56833, val loss: 0.88974, in 0.071s\n",
      "[43/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.56249, val loss: 0.88917, in 0.065s\n",
      "[44/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.55685, val loss: 0.88909, in 0.070s\n",
      "[45/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.55153, val loss: 0.88953, in 0.065s\n",
      "[46/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.54602, val loss: 0.88879, in 0.124s\n",
      "[47/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.54036, val loss: 0.88742, in 0.078s\n",
      "[48/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.53532, val loss: 0.88756, in 0.061s\n",
      "[49/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.52979, val loss: 0.88663, in 0.062s\n",
      "[50/100] 4 trees, 124 leaves (31 on avg), max depth = 12, train loss: 0.52455, val loss: 0.88724, in 0.083s\n",
      "[51/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.51958, val loss: 0.88709, in 0.068s\n",
      "[52/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.51428, val loss: 0.88786, in 0.153s\n",
      "[53/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.50880, val loss: 0.88755, in 0.060s\n",
      "[54/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.50361, val loss: 0.88829, in 0.057s\n",
      "[55/100] 4 trees, 124 leaves (31 on avg), max depth = 13, train loss: 0.49868, val loss: 0.88949, in 0.058s\n",
      "[56/100] 4 trees, 124 leaves (31 on avg), max depth = 9, train loss: 0.49353, val loss: 0.88876, in 0.061s\n",
      "[57/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.48877, val loss: 0.88892, in 0.055s\n",
      "[58/100] 4 trees, 124 leaves (31 on avg), max depth = 11, train loss: 0.48421, val loss: 0.88987, in 0.051s\n",
      "[59/100] 4 trees, 124 leaves (31 on avg), max depth = 10, train loss: 0.47886, val loss: 0.88915, in 0.058s\n",
      "Fit 236 trees in 4.416 s, (7316 total leaves)\n",
      "Time spent computing histograms: 2.619s\n",
      "Time spent finding best splits:  0.378s\n",
      "Time spent applying splits:      0.419s\n",
      "Time spent predicting:           0.039s\n",
      "Accuracy: 0.6360714285714286\n",
      "CPU times: user 7.04 s, sys: 212 ms, total: 7.25 s\n",
      "Wall time: 4.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb_model = HistGradientBoostingClassifier(random_state=42, verbose=1)\n",
    "hgb_model.fit(X_train, y_train)\n",
    "y_pred = hgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBUNIc9wAzvF",
    "outputId": "91db7722-33bb-4ea7-d39d-e0d6f484d698"
   },
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "# Train the model with the new feature included\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set with the new feature included\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model with the new feature\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Accuracy with Min-Entropy Feature:\", accuracy_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1L945YBW8hx"
   },
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iD-fcbeW6NG",
    "outputId": "f25bbfd6-eb0e-4516-fbd3-ee1355c603f6"
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def calculate_min_entropy(sequence):\n",
    "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
    "    p = np.mean(sequence)  # Proportion of ones\n",
    "    max_prob = max(p, 1 - p)\n",
    "    if max_prob == 0:  # Handle the case where all bits are the same\n",
    "        return 0\n",
    "    min_entropy = -np.log2(max_prob)\n",
    "    return min_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
    "\n",
    "# Calculate min-entropy for each sequence in the training and testing datasets\n",
    "min_entropy_train = vectorized_entropy(X_train)\n",
    "min_entropy_test = vectorized_entropy(X_test)\n",
    "\n",
    "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
    "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
    "\n",
    "\n",
    "# Create the Gradient Boosting classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_with_entropy, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred_gb = best_model.predict(X_test_with_entropy)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting model with the best hyperparameters\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other gradient boosted tree methods may have different runtime/performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6rTZWuztjNr"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "def do_xgb(tree_method):\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state = 42,\n",
    "        verbosity = 1,\n",
    "        n_jobs = jobs,\n",
    "        tree_method = tree_method\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train - 1)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test - 1, y_pred)\n",
    "    return [accuracy, xgb_model, y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6321428571428571\n",
      "CPU times: user 23.2 s, sys: 292 ms, total: 23.5 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, _, _ = do_xgb(tree_method='exact')\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6357142857142857\n",
      "CPU times: user 15.5 s, sys: 118 ms, total: 15.6 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, _, _ = do_xgb(tree_method='approx')\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6267857142857143\n",
      "CPU times: user 3.06 s, sys: 40.1 ms, total: 3.1 s\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, _, _ = do_xgb(tree_method='hist')\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4qdLReQtmZ1",
    "outputId": "85c7db2a-2634-4203-b910-d940e2277d82"
   },
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Map classes to [0, 1, 2]\n",
    "y_train_mapped = y_train - 1  # This will change classes [1, 2, 3] to [0, 1, 2]\n",
    "\n",
    "# Continue with the Grid Search\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train_mapped)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best XGBoost model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the XGBoost model with the best hyperparameters\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
    "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVKGPCOT3x9q"
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def do_lgb(boosting_type):\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        random_state = 42,\n",
    "        n_jobs = jobs,\n",
    "        boosting_type = boosting_type,\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    y_pred = lgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return [accuracy, lgb_model, y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 997\n",
      "[LightGBM] [Info] Number of data points in the train set: 11200, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score -1.950295\n",
      "[LightGBM] [Info] Start training from score -1.944661\n",
      "[LightGBM] [Info] Start training from score -1.947161\n",
      "[LightGBM] [Info] Start training from score -0.558523\n",
      "Accuracy: 0.6410714285714286\n",
      "CPU times: user 4.41 s, sys: 51.8 ms, total: 4.46 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, _, _ = do_lgb(boosting_type='gbdt')\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 997\n",
      "[LightGBM] [Info] Number of data points in the train set: 11200, number of used features: 106\n",
      "[LightGBM] [Info] Start training from score -1.950295\n",
      "[LightGBM] [Info] Start training from score -1.944661\n",
      "[LightGBM] [Info] Start training from score -1.947161\n",
      "[LightGBM] [Info] Start training from score -0.558523\n",
      "Accuracy: 0.6285714285714286\n",
      "CPU times: user 3.27 s, sys: 31.8 ms, total: 3.3 s\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy, _, _ = do_lgb(boosting_type='dart')\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fails\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_jobs=jobs, boosting_type='rf')\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXVAMJrx3xLr",
    "outputId": "c38cd918-64d1-4d74-ede9-90354ac3b778"
   },
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Create the LightGBM classifier\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
    "grid_search = GridSearchCV(lgb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best LightGBM model\n",
    "y_pred_lgb = best_lgb_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the LightGBM model with the best hyperparameters\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"Best Hyperparameters for LightGBM:\", best_params)\n",
    "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3RxycX8X8wX"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 17:35:30.740829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 17:35:32.792960: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-04 17:35:34.243935: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3990 - loss: 1.7413 - val_accuracy: 0.5652 - val_loss: 1.1505\n",
      "Epoch 2/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5828 - loss: 1.1075 - val_accuracy: 0.5723 - val_loss: 1.0678\n",
      "Epoch 3/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5948 - loss: 1.0341 - val_accuracy: 0.5786 - val_loss: 1.0259\n",
      "Epoch 4/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 0.9992 - val_accuracy: 0.5763 - val_loss: 1.0097\n",
      "Epoch 5/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6057 - loss: 0.9840 - val_accuracy: 0.5750 - val_loss: 1.0014\n",
      "Epoch 6/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6057 - loss: 0.9742 - val_accuracy: 0.5790 - val_loss: 0.9935\n",
      "Epoch 7/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6089 - loss: 0.9647 - val_accuracy: 0.5835 - val_loss: 0.9863\n",
      "Epoch 8/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6122 - loss: 0.9543 - val_accuracy: 0.5906 - val_loss: 0.9763\n",
      "Epoch 9/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6149 - loss: 0.9421 - val_accuracy: 0.5906 - val_loss: 0.9668\n",
      "Epoch 10/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6173 - loss: 0.9308 - val_accuracy: 0.5920 - val_loss: 0.9597\n",
      "Epoch 11/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6217 - loss: 0.9215 - val_accuracy: 0.5982 - val_loss: 0.9558\n",
      "Epoch 12/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6270 - loss: 0.9153 - val_accuracy: 0.5991 - val_loss: 0.9548\n",
      "Epoch 13/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6289 - loss: 0.9115 - val_accuracy: 0.5973 - val_loss: 0.9542\n",
      "Epoch 14/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6307 - loss: 0.9091 - val_accuracy: 0.6036 - val_loss: 0.9505\n",
      "Epoch 15/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6290 - loss: 0.9067 - val_accuracy: 0.6031 - val_loss: 0.9484\n",
      "Epoch 16/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6301 - loss: 0.9045 - val_accuracy: 0.6027 - val_loss: 0.9460\n",
      "Epoch 17/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6306 - loss: 0.9019 - val_accuracy: 0.6049 - val_loss: 0.9454\n",
      "Epoch 18/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6304 - loss: 0.9001 - val_accuracy: 0.6040 - val_loss: 0.9457\n",
      "Epoch 19/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6324 - loss: 0.8984 - val_accuracy: 0.6045 - val_loss: 0.9453\n",
      "Epoch 20/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6335 - loss: 0.8966 - val_accuracy: 0.6027 - val_loss: 0.9447\n",
      "Epoch 21/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6341 - loss: 0.8948 - val_accuracy: 0.6027 - val_loss: 0.9460\n",
      "Epoch 22/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6335 - loss: 0.8933 - val_accuracy: 0.6040 - val_loss: 0.9457\n",
      "Epoch 23/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6345 - loss: 0.8914 - val_accuracy: 0.6022 - val_loss: 0.9475\n",
      "Epoch 24/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6346 - loss: 0.8901 - val_accuracy: 0.6022 - val_loss: 0.9479\n",
      "Epoch 25/25\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6342 - loss: 0.8885 - val_accuracy: 0.6022 - val_loss: 0.9475\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Neural Network Accuracy: 0.6192857142857143\n",
      "CPU times: user 14.1 s, sys: 1.76 s, total: 15.9 s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "y_train_integer = y_train-1 #.astype('float32')\n",
    "y_test_integer = y_test-1 #.astype('float32')\n",
    "\n",
    "X_train_small, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
    "X_train_small = X_train_small.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "# Create the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Input(shape=(X_train_small.shape[1],)))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train_small, y_train_integer, epochs=25, batch_size=64, validation_data=(X_val, y_val_integer)) #, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = nn_model.predict(X_test.astype('float32'))\n",
    "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the Neural Network model\n",
    "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htyjktKSX-cN",
    "outputId": "04255ea4-c379-4f5f-9b09-8ff8dd70ef1e"
   },
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming you have already defined X_train, X_test, y_train, and y_test\n",
    "\n",
    "# Convert binary numbers to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_integer = label_encoder.transform(y_train)\n",
    "y_test_integer = label_encoder.transform(y_test)\n",
    "\n",
    "# Check unique values in y_train_integer and y_test_integer\n",
    "print(\"Unique values in y_train:\", np.unique(y_train_integer))\n",
    "print(\"Unique values in y_test:\", np.unique(y_test_integer))\n",
    "\n",
    "print(\"Shape of y_train_integer:\", y_train_integer.shape)\n",
    "print(\"Shape of y_test_integer:\", y_test_integer.shape)\n",
    "\n",
    "# Manually split the data into training and validation sets\n",
    "X_train, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train, y_train_integer, epochs=2, batch_size=64, validation_data=(X_val, y_val_integer), verbose=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = nn_model.predict(X_test)\n",
    "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the Neural Network model\n",
    "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGXQrE8d9d0x"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5296 - loss: 1.1914\n",
      "Epoch 2/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5808 - loss: 1.0523\n",
      "Epoch 3/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6095 - loss: 0.9664\n",
      "Epoch 4/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6198 - loss: 0.9398\n",
      "Epoch 5/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6228 - loss: 0.9285\n",
      "Epoch 6/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6249 - loss: 0.9221\n",
      "Epoch 7/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6304 - loss: 0.9155\n",
      "Epoch 8/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6306 - loss: 0.9086\n",
      "Epoch 9/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6344 - loss: 0.9001\n",
      "Epoch 10/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6405 - loss: 0.8917\n",
      "Epoch 11/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6422 - loss: 0.8809\n",
      "Epoch 12/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6456 - loss: 0.8695\n",
      "Epoch 13/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 0.8569\n",
      "Epoch 14/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6550 - loss: 0.8431\n",
      "Epoch 15/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6611 - loss: 0.8288\n",
      "Epoch 16/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6679 - loss: 0.8142\n",
      "Epoch 17/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6745 - loss: 0.7994\n",
      "Epoch 18/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6828 - loss: 0.7843\n",
      "Epoch 19/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6895 - loss: 0.7702\n",
      "Epoch 20/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6949 - loss: 0.7567\n",
      "Epoch 21/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7002 - loss: 0.7432\n",
      "Epoch 22/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7083 - loss: 0.7302\n",
      "Epoch 23/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7134 - loss: 0.7179\n",
      "Epoch 24/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7166 - loss: 0.7060\n",
      "Epoch 25/25\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7220 - loss: 0.6938\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "LSTM Accuracy: 0.5853571428571429\n",
      "CPU times: user 20.7 s, sys: 1.86 s, total: 22.5 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "y_train_integer = y_train-1 #.astype('float32')\n",
    "y_test_integer = y_test-1 #.astype('float32')\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "# TODO: LSTM with a single time step is not a sequence!\n",
    "time_steps = 1  # Each sample is treated as a single time step\n",
    "X_train_lstm = X_train.astype('float32').reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
    "X_test_lstm = X_test.astype('float32').reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
    "# y_train_lstm = y_train_integer.reshape(y_train_integer.shape[0], time_steps, y_train_integer.shape[1])\n",
    "\n",
    "# Create the Neural Network model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Input(shape=(time_steps, X_train_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(32))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_lstm, y_train_integer, epochs=25, batch_size=64)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probabilities = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm = np.argmax(y_pred_probabilities, axis=-1)\n",
    "\n",
    "# Calculate the accuracy of the LSTM model\n",
    "accuracy_lstm = accuracy_score(y_test_integer, y_pred_lstm)\n",
    "print(\"LSTM Accuracy:\", accuracy_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2sE0n_hlbMAr",
    "outputId": "3cf01564-e8f0-431b-e490-48ab0299a741"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ... (Previous code for reading and preprocessing the data)\n",
    "\n",
    "# Convert the data into numerical format\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# Reshape the input data for LSTM\n",
    "time_steps = 1  # Each sample is treated as a single time step\n",
    "X_train_lstm = X_train.reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
    "X_test_lstm = X_test.reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
    "\n",
    "# Create the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(32, input_shape=(time_steps, X_train.shape[1])))\n",
    "lstm_model.add(Dense(16, activation='relu'))\n",
    "lstm_model.add(Dense(3, activation='sigmoid'))  # Assuming binary classification\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "y_pred_lstm = np.round(y_pred_lstm).astype(int).flatten()  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate the accuracy of the LSTM model\n",
    "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "print(\"LSTM Accuracy:\", accuracy_lstm)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
