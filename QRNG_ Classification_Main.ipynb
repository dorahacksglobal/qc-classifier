{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wgVYr5-TXm5t"
      },
      "outputs": [],
      "source": [
        "# Importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import log2, sqrt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import fft, ifft\n",
        "from scipy.special import erfc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxSPTrNXsp6",
        "outputId": "0e8f768e-776e-480f-f290-0725176cabe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    label  spectral_test  shannon_entropy  frequency_test  runs_test  \\\n",
            "0       1     -21.771553         1.935451        0.423711   0.120217   \n",
            "1       1     -22.230385         1.963615        0.841481   0.027240   \n",
            "2       1     -22.230385         1.939471        0.109599   0.498506   \n",
            "3       1     -22.230385         1.872164        0.071861   0.620874   \n",
            "4       1     -22.230385         1.976281        0.230139   0.725698   \n",
            "..    ...            ...              ...             ...        ...   \n",
            "94      3     -22.230385         1.994217        0.689157   0.987149   \n",
            "95      3     -22.230385         1.994217        0.548506   0.295989   \n",
            "96      3     -22.230385         1.950631        0.071861   0.897478   \n",
            "97      3     -22.230385         1.980269        0.841481   0.548989   \n",
            "98      3     -22.230385         1.970764        0.423711   0.501761   \n",
            "\n",
            "    autocorrelation    0    1    2    3  ...   90   91   92   93   94   95  \\\n",
            "0              0.33  0.0  1.0  0.0  0.0  ...  1.0  1.0  1.0  1.0  1.0  1.0   \n",
            "1              0.31  0.0  1.0  1.0  0.0  ...  0.0  1.0  1.0  0.0  0.0  0.0   \n",
            "2              0.32  1.0  1.0  1.0  0.0  ...  0.0  1.0  1.0  0.0  0.0  0.0   \n",
            "3              0.36  1.0  1.0  0.0  1.0  ...  1.0  1.0  0.0  1.0  1.0  1.0   \n",
            "4              0.18  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0  1.0  1.0  1.0   \n",
            "..              ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
            "94             0.27  0.0  0.0  1.0  0.0  ...  1.0  0.0  1.0  1.0  0.0  0.0   \n",
            "95             0.19  0.0  1.0  0.0  0.0  ...  1.0  1.0  1.0  0.0  1.0  0.0   \n",
            "96             0.16  0.0  0.0  1.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n",
            "97             0.27  0.0  1.0  1.0  0.0  ...  1.0  1.0  0.0  0.0  1.0  1.0   \n",
            "98             0.19  1.0  0.0  0.0  1.0  ...  1.0  1.0  1.0  1.0  0.0  0.0   \n",
            "\n",
            "     96   97   98   99  \n",
            "0   0.0  0.0  1.0  0.0  \n",
            "1   1.0  1.0  0.0  1.0  \n",
            "2   0.0  0.0  1.0  1.0  \n",
            "3   1.0  1.0  0.0  1.0  \n",
            "4   0.0  0.0  1.0  1.0  \n",
            "..  ...  ...  ...  ...  \n",
            "94  0.0  1.0  1.0  0.0  \n",
            "95  0.0  0.0  0.0  1.0  \n",
            "96  0.0  0.0  0.0  1.0  \n",
            "97  0.0  0.0  1.0  1.0  \n",
            "98  0.0  0.0  0.0  0.0  \n",
            "\n",
            "[99 rows x 106 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Concatenate data\n",
        "def concatenateData(df, num_concats):\n",
        "    new_df = pd.DataFrame({\n",
        "        'Concatenated_Data': [''] * (len(df) // num_concats), \n",
        "        'label': [0] * (len(df) // num_concats)\n",
        "    })\n",
        "\n",
        "    # Loop through each group of num_concats rows and concatenate their 'binary_number' strings\n",
        "    for i in range(0, len(df), num_concats):\n",
        "        new_df.iloc[i // num_concats, 0] = ''.join(df['binary_number'][i:i + num_concats])\n",
        "        new_df.iloc[i // num_concats, 1] = df['label'][i]\n",
        "\n",
        "    return new_df\n",
        "\n",
        "# Calculate Shannon entropy for each concatenated binary sequence\n",
        "def shannon_entropy(binary_string):\n",
        "    if len(binary_string) % 2 != 0:\n",
        "        raise ValueError(\"Binary string length must be a multiple of 2.\")\n",
        "    \n",
        "    patterns = ['00', '10', '11', '01']\n",
        "    frequency = {pattern: 0 for pattern in patterns}\n",
        "    \n",
        "    for i in range(0, len(binary_string), 2):\n",
        "        segment = binary_string[i:i+2]\n",
        "        if segment in patterns:\n",
        "            frequency[segment] += 1\n",
        "    \n",
        "    total_segments = sum(frequency.values())\n",
        "    \n",
        "    entropy = 0\n",
        "    for count in frequency.values():\n",
        "        if count > 0:\n",
        "            probability = count / total_segments\n",
        "            entropy -= probability * log2(probability)\n",
        "    \n",
        "    return entropy\n",
        "\n",
        "\n",
        "def classic_spectral_test(bit_string):\n",
        "    bit_array = 2 * np.array([int(bit) for bit in bit_string]) - 1\n",
        "    dft = fft(bit_array)\n",
        "    n_half = len(bit_string) // 2 + 1\n",
        "    mod_dft = np.abs(dft[:n_half])\n",
        "    threshold = np.sqrt(np.log(1 / 0.05) / len(bit_string))\n",
        "    peaks_below_threshold = np.sum(mod_dft < threshold)\n",
        "    expected_peaks = 0.95 * n_half\n",
        "    d = (peaks_below_threshold - expected_peaks) / np.sqrt(len(bit_string) * 0.95 * 0.05)\n",
        "    p_value = erfc(np.abs(d) / np.sqrt(2)) / 2\n",
        "    return d\n",
        "\n",
        "def frequency_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    count_ones = bit_string.count('1')\n",
        "    count_zeros = bit_string.count('0')\n",
        "    \n",
        "    # The test statistic\n",
        "    s = (count_ones - count_zeros) / sqrt(n)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(s) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def runs_test(bit_string):\n",
        "    n = len(bit_string)\n",
        "    runs = 1  # Start with the first run\n",
        "    for i in range(1, n):\n",
        "        if bit_string[i] != bit_string[i - 1]:\n",
        "            runs += 1\n",
        "    \n",
        "    n0 = bit_string.count('0')\n",
        "    n1 = bit_string.count('1')\n",
        "    \n",
        "    # Expected number of runs\n",
        "    expected_runs = (2 * n0 * n1 / n) + 1\n",
        "    variance_runs = (2 * n0 * n1 * (2 * n0 * n1 - n)) / (n ** 2 * (n - 1))\n",
        "    \n",
        "    # The test statistic\n",
        "    z = (runs - expected_runs) / sqrt(variance_runs)\n",
        "    \n",
        "    # The p-value\n",
        "    p_value = erfc(abs(z) / sqrt(2))\n",
        "    \n",
        "    return p_value\n",
        "\n",
        "def linear_complexity(bit_string, M=500):\n",
        "    # Perform linear complexity test with block size M\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    lc = 0  # Initialize linear complexity\n",
        "    \n",
        "    # Process blocks of size M\n",
        "    for i in range(0, n, M):\n",
        "        block = bit_array[i:i+M]\n",
        "        if len(block) < M:\n",
        "            continue\n",
        "        \n",
        "        lc_block = 0\n",
        "        for j in range(M):\n",
        "            if block[j] == 1:\n",
        "                lc_block = j + 1\n",
        "        \n",
        "        lc += lc_block\n",
        "    \n",
        "    lc = lc / (n / M)\n",
        "    return lc\n",
        "\n",
        "def autocorrelation_test(bit_string, lag=1):\n",
        "    n = len(bit_string)\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    autocorrelation = np.correlate(bit_array, np.roll(bit_array, lag), mode='valid')[0]\n",
        "    return autocorrelation / n\n",
        "\n",
        "def maurer_universal_test(bit_string):\n",
        "    k = 6\n",
        "    l = 5\n",
        "    q = 20\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    max_val = 2**k\n",
        "    init_subseq = bit_array[:q]\n",
        "    rest_subseq = bit_array[q:]\n",
        "    d = {}\n",
        "    for i in range(len(init_subseq) - k + 1):\n",
        "        d[tuple(init_subseq[i:i+k])] = i\n",
        "    t = []\n",
        "    for i in range(len(rest_subseq) - k + 1):\n",
        "        subseq = tuple(rest_subseq[i:i+k])\n",
        "        if subseq in d:\n",
        "            t.append(i - d[subseq])\n",
        "            d[subseq] = i\n",
        "    if not t:\n",
        "        return 0\n",
        "    t = np.array(t)\n",
        "    log_avg = np.mean(np.log2(t))\n",
        "    return log_avg - np.log2(q)\n",
        "\n",
        "def binary_matrix_rank_test(bit_string, M=32, Q=32):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_matrices = len(bit_array) // (M * Q)\n",
        "    ranks = []\n",
        "    for i in range(num_matrices):\n",
        "        matrix = bit_array[i*M*Q:(i+1)*M*Q].reshape((M, Q))\n",
        "        rank = np.linalg.matrix_rank(matrix)\n",
        "        ranks.append(rank)\n",
        "    return np.mean(ranks)\n",
        "\n",
        "def cumulative_sums_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    adjusted = 2 * bit_array - 1\n",
        "    cumulative_sum = np.cumsum(adjusted)\n",
        "    max_excursion = np.max(np.abs(cumulative_sum))\n",
        "    return max_excursion\n",
        "\n",
        "def longest_run_ones_test(bit_string, block_size=100):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    num_blocks = len(bit_array) // block_size\n",
        "    max_runs = []\n",
        "    for i in range(num_blocks):\n",
        "        block = bit_array[i*block_size:(i+1)*block_size]\n",
        "        max_run = max([len(list(g)) for k, g in itertools.groupby(block) if k == 1])\n",
        "        max_runs.append(max_run)\n",
        "    return np.mean(max_runs)\n",
        "\n",
        "def random_excursions_test(bit_string):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    bit_array = 2 * bit_array - 1  # Convert to ±1\n",
        "\n",
        "    cumulative_sum = np.cumsum(bit_array)\n",
        "    states = np.unique(cumulative_sum)\n",
        "\n",
        "    if 0 not in states:\n",
        "        states = np.append(states, 0)\n",
        "    state_counts = {state: 0 for state in states}\n",
        "    for state in cumulative_sum:\n",
        "        state_counts[state] += 1\n",
        "\n",
        "    state_counts[0] -= 1  # Adjust for zero state\n",
        "    pi = [0.5 * (1 - (1 / (2 * state + 1)**2)) for state in states]\n",
        "    x = np.sum([(state_counts[state] - len(bit_string) * pi[i])**2 / (len(bit_string) * pi[i]) for i, state in enumerate(states)])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def unique_subsequences(bit_string, length=4):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    subsequences = set()\n",
        "    \n",
        "    for i in range(n - length + 1):\n",
        "        subseq = tuple(bit_array[i:i+length])\n",
        "        subsequences.add(subseq)\n",
        "    \n",
        "    return len(subsequences)\n",
        "\n",
        "def sample_entropy(bit_string, m=2, r=0.2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= r, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(C) / (N - m + 1.0)\n",
        "    \n",
        "    return -np.log(_phi(m + 1) / _phi(m))\n",
        "\n",
        "def permutation_entropy(bit_string, order=3):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    \n",
        "    permutations = np.array(list(itertools.permutations(range(order))))\n",
        "    c = np.zeros(len(permutations))\n",
        "    \n",
        "    for i in range(n - order + 1):\n",
        "        sorted_index_array = tuple(np.argsort(bit_array[i:i+order]))\n",
        "        for j, p in enumerate(permutations):\n",
        "            if np.array_equal(p, sorted_index_array):\n",
        "                c[j] += 1\n",
        "    \n",
        "    c = c / (n - order + 1)\n",
        "    pe = -np.sum(c * np.log2(c + np.finfo(float).eps))\n",
        "    return pe\n",
        "\n",
        "def lyapunov_exponent(bit_string, m=2, t=1):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    N = len(bit_array)\n",
        "    \n",
        "    def _phi(m):\n",
        "        x = np.array([bit_array[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.all(np.abs(x[:, None] - x) <= t, axis=2), axis=0) / (N - m + 1.0)\n",
        "        return np.sum(np.log(C + np.finfo(float).eps)) / (N - m + 1.0)\n",
        "    \n",
        "    return abs(_phi(m) - _phi(m + 1))\n",
        "\n",
        "def entropy_rate(bit_string, k=2):\n",
        "    bit_array = np.array([int(bit) for bit in bit_string])\n",
        "    n = len(bit_array)\n",
        "    prob = {}\n",
        "    \n",
        "    for i in range(n - k + 1):\n",
        "        subseq = tuple(bit_array[i:i + k])\n",
        "        if subseq in prob:\n",
        "            prob[subseq] += 1\n",
        "        else:\n",
        "            prob[subseq] = 1\n",
        "    \n",
        "    for key in prob:\n",
        "        prob[key] /= (n - k + 1)\n",
        "    \n",
        "    entropy_rate = -sum(p * log2(p) for p in prob.values())\n",
        "    return entropy_rate\n",
        "\n",
        "# Apply randomness tests\n",
        "def apply_randomness_tests(df, tests):\n",
        "    if not tests:\n",
        "        raise ValueError(\"No randomness tests specified.\")\n",
        "\n",
        "    test_functions = {\n",
        "        'autocorrelation': autocorrelation_test,\n",
        "        'cumulative_sums': cumulative_sums_test,\n",
        "        'spectral_test': classic_spectral_test,\n",
        "        'frequency_test': frequency_test,\n",
        "        'runs_test': runs_test,\n",
        "        'shannon_entropy': shannon_entropy\n",
        "    }\n",
        "\n",
        "    for test in tests:\n",
        "        if test not in test_functions:\n",
        "            raise ValueError(f\"Invalid randomness test: {test}\")\n",
        "        df[test] = df['Concatenated_Data'].apply(test_functions[test])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_data(df, num_concats, tests):\n",
        "    df = concatenateData(df, num_concats)\n",
        "    processed_df = apply_randomness_tests(df, tests)\n",
        "    \n",
        "    # Convert concatenated binary strings into separate columns\n",
        "    df_features = pd.DataFrame(processed_df['Concatenated_Data'].apply(list).tolist(), dtype=float)\n",
        "    processed_df = pd.concat([processed_df.drop(columns='Concatenated_Data'), df_features], axis=1)\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "# Calculate min-entropy\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "# Main\n",
        "file_path = '/Users/sid/Code/qc-classifier/AI_2qubits_training_data copy.txt'\n",
        "\n",
        "# Read the data from the file\n",
        "data = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        if line.strip():\n",
        "            binary_number, label = line.strip().split()\n",
        "            data.append((binary_number, int(label)))\n",
        "\n",
        "# Convert the data into a DataFrame\n",
        "df = pd.DataFrame(data, columns=['binary_number', 'label'])\n",
        "\n",
        "tests_to_apply = ['spectral_test', 'shannon_entropy', 'frequency_test', 'runs_test', 'autocorrelation']\n",
        "\n",
        "# Preprocess data and apply randomness tests\n",
        "preprocessed_df = preprocess_data(df, num_concats=1, tests=tests_to_apply)\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = preprocessed_df.drop(columns='label').values\n",
        "y = preprocessed_df['label'].values.astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = np.apply_along_axis(calculate_min_entropy, 1, X_train)\n",
        "min_entropy_test = np.apply_along_axis(calculate_min_entropy, 1, X_test)\n",
        "\n",
        "# Add min-entropy as a feature\n",
        "X_train = np.column_stack((X_train, min_entropy_train))\n",
        "X_test = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "print(preprocessed_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Brik04cEFwK"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQm1VbnVEF3K"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "937q3D9VAzlM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'degree': [2, 3, 4],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMKS-cKqbkL0",
        "outputId": "f1e56f55-3340-423b-8fe5-ac06e95cb71c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'kernel': 'linear', 'gamma': 'auto', 'C': 0.1}\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Perform hyperparameter tuning with RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42)\n",
        "random_search = RandomizedSearchCV(svm_model, param_distributions=param_dist, n_iter=5, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDFlld25Xze5"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFkkQCBxXlWh",
        "outputId": "1c9794f0-a799-4de4-98e3-0dad406a3ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.5542857142857143\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_with_entropy, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KULNfXpncJZC",
        "outputId": "f44dab80-2cb1-46c1-ab5f-a26725d94c9c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],          # Number of trees in the forest\n",
        "    'max_depth': [None, 10, 20, 30],         # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],         # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],           # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt'],       # Number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_rf = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the Random Forest model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
        "print(\"Best Hyperparameters:\", best_params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DfNGSyaX3kV"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBUNIc9wAzvF",
        "outputId": "91db7722-33bb-4ea7-d39d-e0d6f484d698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting Accuracy with Min-Entropy Feature: 0.7842857142857143\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "# Create the Gradient Boosting classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "# Train the model with the new feature included\n",
        "gb_model.fit(X_train_with_entropy, y_train)\n",
        "\n",
        "# Make predictions on the test set with the new feature included\n",
        "y_pred_gb = gb_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Gradient Boosting model with the new feature\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting Accuracy with Min-Entropy Feature:\", accuracy_gb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1L945YBW8hx"
      },
      "source": [
        "GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iD-fcbeW6NG",
        "outputId": "f25bbfd6-eb0e-4516-fbd3-ee1355c603f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
            "Gradient Boosting Accuracy: 0.56\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "def calculate_min_entropy(sequence):\n",
        "    sequence = np.asarray(sequence, dtype=float)  # Convert sequence to float\n",
        "    p = np.mean(sequence)  # Proportion of ones\n",
        "    max_prob = max(p, 1 - p)\n",
        "    if max_prob == 0:  # Handle the case where all bits are the same\n",
        "        return 0\n",
        "    min_entropy = -np.log2(max_prob)\n",
        "    return min_entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectorized_entropy = np.vectorize(calculate_min_entropy, signature='(n)->()')\n",
        "\n",
        "# Calculate min-entropy for each sequence in the training and testing datasets\n",
        "min_entropy_train = vectorized_entropy(X_train)\n",
        "min_entropy_test = vectorized_entropy(X_test)\n",
        "\n",
        "X_train_with_entropy = np.column_stack((X_train, min_entropy_train))\n",
        "X_test_with_entropy = np.column_stack((X_test, min_entropy_test))\n",
        "\n",
        "\n",
        "# Create the Gradient Boosting classifier\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train_with_entropy, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred_gb = best_model.predict(X_test_with_entropy)\n",
        "\n",
        "# Calculate the accuracy of the Gradient Boosting model with the best hyperparameters\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_gb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6rTZWuztjNr"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4qdLReQtmZ1",
        "outputId": "85c7db2a-2634-4203-b910-d940e2277d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Hyperparameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 150}\n",
            "XGBoost Accuracy: 0.054285714285714284\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# Create the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Map classes to [0, 1, 2]\n",
        "y_train_mapped = y_train - 1  # This will change classes [1, 2, 3] to [0, 1, 2]\n",
        "\n",
        "# Continue with the Grid Search\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train_mapped)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best XGBoost model\n",
        "y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the XGBoost model with the best hyperparameters\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
        "print(\"XGBoost Accuracy:\", accuracy_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_stlSoN3x3s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVKGPCOT3x9q"
      },
      "source": [
        "## CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXVAMJrx3xLr",
        "outputId": "c38cd918-64d1-4d74-ede9-90354ac3b778"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Create the LightGBM classifier\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation (cv=5) to find the best hyperparameters\n",
        "grid_search = GridSearchCV(lgb_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_lgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best LightGBM model\n",
        "y_pred_lgb = best_lgb_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the LightGBM model with the best hyperparameters\n",
        "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
        "print(\"Best Hyperparameters for LightGBM:\", best_params)\n",
        "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3RxycX8X8wX"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htyjktKSX-cN",
        "outputId": "04255ea4-c379-4f5f-9b09-8ff8dd70ef1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming you have already defined X_train, X_test, y_train, and y_test\n",
        "\n",
        "# Convert binary numbers to integer labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_integer = label_encoder.transform(y_train)\n",
        "y_test_integer = label_encoder.transform(y_test)\n",
        "\n",
        "# Check unique values in y_train_integer and y_test_integer\n",
        "print(\"Unique values in y_train:\", np.unique(y_train_integer))\n",
        "print(\"Unique values in y_test:\", np.unique(y_test_integer))\n",
        "\n",
        "print(\"Shape of y_train_integer:\", y_train_integer.shape)\n",
        "print(\"Shape of y_test_integer:\", y_test_integer.shape)\n",
        "\n",
        "# Manually split the data into training and validation sets\n",
        "X_train, X_val, y_train_integer, y_val_integer = train_test_split(X_train, y_train_integer, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Neural Network model\n",
        "nn_model = Sequential()\n",
        "nn_model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "nn_model.add(Dense(16, activation='relu'))\n",
        "nn_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "nn_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "nn_model.fit(X_train, y_train_integer, epochs=2, batch_size=64, validation_data=(X_val, y_val_integer), verbose=0)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_probabilities = nn_model.predict(X_test)\n",
        "y_pred_nn = np.argmax(y_pred_probabilities, axis=-1)\n",
        "\n",
        "# Calculate the accuracy of the Neural Network model\n",
        "accuracy_nn = accuracy_score(y_test_integer, y_pred_nn)\n",
        "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGXQrE8d9d0x"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sE0n_hlbMAr",
        "outputId": "3cf01564-e8f0-431b-e490-48ab0299a741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 16s 6ms/step - loss: -44.6486 - accuracy: 0.3256\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -225.9178 - accuracy: 0.3256\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -537.3671 - accuracy: 0.3256\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -983.8442 - accuracy: 0.3256\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -1560.6449 - accuracy: 0.3256\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 1s 10ms/step - loss: -2261.8774 - accuracy: 0.3256\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -3084.3223 - accuracy: 0.3256\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -4022.4128 - accuracy: 0.3256\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -5070.7061 - accuracy: 0.3256\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -6225.6167 - accuracy: 0.3256\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -7479.5684 - accuracy: 0.3256\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -8828.9736 - accuracy: 0.3256\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -10271.5215 - accuracy: 0.3256\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -11804.6680 - accuracy: 0.3256\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -13424.7754 - accuracy: 0.3256\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -15129.1650 - accuracy: 0.3256\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -16913.8594 - accuracy: 0.3256\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -18778.6680 - accuracy: 0.3256\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 3s 20ms/step - loss: -20722.0684 - accuracy: 0.3256\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 1s 8ms/step - loss: -22743.0566 - accuracy: 0.3256\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -24838.2930 - accuracy: 0.3256\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -27007.6738 - accuracy: 0.3256\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -29245.7227 - accuracy: 0.3256\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -31556.3359 - accuracy: 0.3256\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -33938.5000 - accuracy: 0.3256\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -36388.2539 - accuracy: 0.3256\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 1s 8ms/step - loss: -38907.3516 - accuracy: 0.3256\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -41493.5781 - accuracy: 0.3256\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -44147.6484 - accuracy: 0.3256\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 1s 10ms/step - loss: -46865.2500 - accuracy: 0.3256\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -49648.5273 - accuracy: 0.3256\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 1s 10ms/step - loss: -52496.3164 - accuracy: 0.3256\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 2s 10ms/step - loss: -55408.7383 - accuracy: 0.3256\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -58387.0547 - accuracy: 0.3256\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -61428.6719 - accuracy: 0.3256\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -64535.5859 - accuracy: 0.3256\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 1s 8ms/step - loss: -67701.7031 - accuracy: 0.3256\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -70933.1250 - accuracy: 0.3256\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -74228.2344 - accuracy: 0.3256\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -77586.0312 - accuracy: 0.3256\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -81004.1406 - accuracy: 0.3256\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -84487.2578 - accuracy: 0.3256\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -88034.9297 - accuracy: 0.3256\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -91643.4219 - accuracy: 0.3256\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -95315.4141 - accuracy: 0.3256\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 2s 11ms/step - loss: -99047.5547 - accuracy: 0.3256\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 1s 9ms/step - loss: -102842.0391 - accuracy: 0.3256\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: -106694.8750 - accuracy: 0.3256\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -110608.2031 - accuracy: 0.3256\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 1s 7ms/step - loss: -114587.6016 - accuracy: 0.3256\n",
            "38/38 [==============================] - 1s 4ms/step\n",
            "LSTM Accuracy: 0.3641666666666667\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ... (Previous code for reading and preprocessing the data)\n",
        "\n",
        "# Convert the data into numerical format\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Reshape the input data for LSTM\n",
        "time_steps = 1  # Each sample is treated as a single time step\n",
        "X_train_lstm = X_train.reshape(X_train.shape[0], time_steps, X_train.shape[1])\n",
        "X_test_lstm = X_test.reshape(X_test.shape[0], time_steps, X_test.shape[1])\n",
        "\n",
        "# Create the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(32, input_shape=(time_steps, X_train.shape[1])))\n",
        "lstm_model.add(Dense(16, activation='relu'))\n",
        "lstm_model.add(Dense(3, activation='sigmoid'))  # Assuming binary classification\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = np.round(y_pred_lstm).astype(int).flatten()  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the accuracy of the LSTM model\n",
        "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
        "print(\"LSTM Accuracy:\", accuracy_lstm)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
